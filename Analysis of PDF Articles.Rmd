---
title: "Analysis of PDF Articles"
author: "Kristina Becvar"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "~/GitHub/NYT-Analysis/pdf_articles")
```

# Getting Started
## Pulling in the PDF docs

I have the PDF files in my working directory. Using the "list.files()" function from the "pdftools" package, I can create a vector of PDF file names, specifying only files that end in ".pdf".

```{r code_folding=TRUE}
#check the working directory
getwd()
#load libraries
library(pdftools)
library(readtext)
library(readr)
library(tm)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.dictionaries)
library(quanteda.sentiment)
library(stringr)
library(MASS)
library(tidyverse)
library(plyr); library(dplyr)
```

## Extracting PDF Titles

```{r echo=TRUE}

# Get the data directory from readtext
DATA_DIR <- system.file("extdata/", package = "readtext")
#read in the pdf files
(nyt_pdf <- readtext(paste0(DATA_DIR, "/pdf/./pdf_articles/*.pdf"), 
                    docvarsfrom = "filenames", 
                    docvarnames = c("document", "language")))
```

## Check Headlines

```{r echo=TRUE}
#check first headlines
head(nyt_files)
```

## Change Encoding

This definitely helped with the issue of representing apostrophes like this: "Congressâ€™s"

```{r echo=TRUE}
#encoding for UTF-8 to allow the characters to match up for the importing of PDF files
Encoding(nyt_files) <- "UTF-8"
#check first headlines after changing encoding
head(nyt_files)
```

## Pull Text

Now I can use "lapply()" to pull the text from each article by headline as specified from the "list.files()" function. 

```{r echo=TRUE}
#pull text and create object
nyt_text <- lapply(nyt_files, pdf_text)
```

## Return Results in a Data Frame

```{r echo=TRUE}
nyt_df <- ldply(nyt_text, data.frame)
colnames(nyt_df)
```
## Create Corpus

```{r echo=TRUE}
nyt_corpus <- corpus(nyt_df, text_field = "X..i..")
```

### Generate Document-Feature Matrix

```{r echo=TRUE}
# create a full DFM for comparison
nyt_dfm <- dfm(tokens(nyt_corpus))
dim(nyt_dfm)
nyt_dfm

# convert corpus to DFM using the dictionary
nyt_nrc <- dfm(tokens(nyt_corpus,
                      remove_punct = TRUE),
                      tolower = TRUE) %>%
  dfm_lookup(data_dictionary_NRC)

dim(nyt_nrc)
nyt_nrc

```

```{r echo=TRUE}
# use liwcalike() to estimate sentiment using NRC dictionary
nyt_nrc <- liwcalike(as.character(nyt_text), data_dictionary_NRC)
names(nyt_nrc)
output_lsd <- liwcalike(nyt_text, 
                        dictionary = data_dictionary_NRC)
```

```{r echo=TRUE}
ggplot(nyt_nrc) + 
  geom_histogram(aes(x=positive)) + 
  theme_bw()
```


```{r echo=TRUE}
ggplot(nyt_nrc) + 
  geom_histogram(aes(x=negative)) + 
  theme_bw()
```

### Check Length

```{r echo=TRUE}
#checking the length of the corpus
length(nyt_corpus)
```

## Create Term Document Matrix

```{r echo=TRUE}
#create term document matrix
nyt_tdm <- TermDocumentMatrix(nyt_corpus, control = list(removeNumbers = TRUE, 
                                                         removePunctuation = TRUE,
                                                         stopwords = TRUE,
                                                         stemming = FALSE))
```


### Inspect Term Document Matrix

```{r echo=TRUE}
#inspect tdm
inspect(nyt_tdm)
```

### Terms & Frequencies

I want to get a sense of my terms and frequencies, so I'll look at the terms using the "stringr" package. 

For example, it looks like the word "veteran" appears in 86 of the documents with a word count of 210.

```{r echo=TRUE}
sum(str_detect(nyt_text, "veteran"))
sum(str_count(nyt_corpus, "veteran"))
```

### Preprocessing Choices

Refining further using the "stemming" option, and running the tdm() function, the sparsity remains 98%, with  20,985 terms as compared to 30,003 in the un-stemmed version.

#### Alternative Processing

```{r echo=TRUE}
#create term document matrix without stopwords and with stemming
nyt_tdm2 <- TermDocumentMatrix(nyt_corpus, 
                              control = list(removeNumbers = TRUE,
                                             removePunctuation = TRUE,
                                             stopwords = TRUE,
                                             stemming = TRUE))
#inspect alternative tdm
inspect(nyt_tdm2)
```

The problems is that stemming removes too many of the words I'll want to examine for sentiment analysis, so I will not use that option at this time.

## Basic Analysis

I'll move on to some basic analysis using the "tm" package functions such as finding the frequently occurring terms, starting with a threshold of 100:

```{r echo=TRUE}
findFreqTerms(nyt_tdm, lowfreq = 100, highfreq = Inf)
```

I'll save the result and use it to subset the TDM as "ft", "frequent terms". 

Looking at the top frequency words in decreasing order, I can beging to get a feel for what questions I need to be ready to ask next.

```{r echo=TRUE}
ft <- findFreqTerms(nyt_tdm, lowfreq = 100, highfreq = Inf)
nyt_tdm_ft <- as.matrix(nyt_tdm[ft,]) 
sort(apply(nyt_tdm_ft, 1, sum), decreasing = TRUE)
```

## Tokenization

I finally realized how to remove the "�" symbol that has plagued me since starting working with this API by using "remove_symbols=TRUE" in addition to removing the punctuation when tokenizing:

```{r code_folding = TRUE}
nyt_tokens <- tokens(nyt_corpus,
                   remove_punct = TRUE,
                   remove_symbols = TRUE)
nyt_tokens
```

