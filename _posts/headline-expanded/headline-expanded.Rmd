---
title: "Analysis of Main vs. Print Headlines: Phase 2"
description: |
  Text as Data Project Headline Comparison Research Using API Query "Afghanistan"
preview: 000010.png
categories:
  - text as data
  - NYT text analysis project
author:
  - name: Kristina Becvar
    url: https://kbec19.github.io/NYT-Analysis/
    affiliation: UMass DACSS Program (My Academic Blog Link)
    affiliation_url: https://kristinabecvar.com
slug: headline-expanded
date: 2022-04-26
output:
  distill::distill_article:
    toc: true
    code_folding: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Comparing Main vs. Print Headlines in the New York Times

This is a continuation of my analysis started in my [last post.](https://kbec19.github.io/NYT-Analysis/posts/headline-analysis/)

## Making Different Choices on Inclusion of Observations

In my initial analysis of the headline data, I used the scope of the project from a prior term to set the parameters for the headline API search. I decided to expand the term to include just "Afghanistan" rather than "Afghanistan withdrawal" for a couple of reasons. First, to increase the volume of observations and increase reliability. Second, because I want to look at the comparison between the two search terms for any change.

## Gathering Data

### Previous Process

```{r include=FALSE}
# load libraries

library(cleanNLP)
library(tidytext)
library(tidyverse)
library(quanteda)
library(reticulate)
library(spacyr)
library(plyr); library(dplyr)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.dictionaries)
library(quanteda.sentiment)
library(jsonlite)
library(caret)
library(magrittr)
library(stringr)
library(tidyr)
library(widyr)
library(ggplot2)

suppressWarnings(expr)

```

The data was pulled via API using the same process as in my first phase of the comparison research, with the only change in the query term "Afghanistan" as opposed to "Afghanistan Withdrawal". This led to a significantly larger dataset for comparison, though most of the increase in count was filtered out due to their classification as not news-related. Still, the number of observed, relevant headlines increased from 346 to 936 (for each type; main and print headlines).

### Load Data

Now to the active review of the data. Loading the data from my collection phase:

```{r code_folding = TRUE}
#load data
main_headlines <- read.csv("afghanistan_headlines_main.csv")
main_headlines <- as.data.frame(main_headlines)
#turn into data frame
print_headlines <- read.csv("afghanistan_headlines_print.csv")
print_headlines <- as.data.frame(print_headlines)
#inspect data
head(main_headlines)
head(print_headlines)
all_results <- read.csv("all_results.csv")
```

### Create Corpus

```{r echo=TRUE}
main_corpus <- corpus(main_headlines, docid_field = "doc_id", text_field = "text")
print_corpus <- corpus(print_headlines, docid_field = "doc_id", text_field = "text")
```

### Assign Type to Docvars

```{r echo=TRUE}
main_corpus$type <- "Main Headline"
print_corpus$type <- "Print Headline"

docvars(main_corpus, field = "type") <- main_corpus$type
docvars(print_corpus, field = "type") <- print_corpus$type
```

### Tokenization

I want to optimize pre-processing by removing the "ï¿½" symbol that has plagued me since starting working with this API by using "remove_symbols=TRUE" in addition to removing the punctuation when tokenizing. I also want to remove stopwords. I do NOT want to use stemming at this point.

#### Main Headlines

```{r code_folding = TRUE}
main_tokens <- tokens(main_corpus) %>%
  tokens(main_corpus, remove_punct = TRUE) %>%
  tokens(main_corpus, remove_numbers = TRUE) %>%
  tokens(main_corpus, remove_symbols = TRUE) %>%
  tokens_remove(stopwords("english")) %>%
  tokens_remove(c("s"))

main_dfm <- dfm(main_tokens)

length(main_tokens)
print(main_tokens)
```

#### Print Headlines

```{r code_folding = TRUE}
print_tokens <- tokens(print_corpus) %>%
  tokens(print_corpus, remove_punct = TRUE) %>%
  tokens(print_corpus, remove_numbers = TRUE) %>%
  tokens(print_corpus, remove_symbols = TRUE) %>%
  tokens_remove(stopwords("english")) %>%
  tokens_remove(c("s"))

main_dfm <- dfm(print_tokens)

length(print_tokens)
print(print_tokens)
```

### Document Feature Matrix

Again, this will show me the occurrence of words within each 'doc' or headline observation.

```{r code_folding = TRUE}
#print dfm
print_dfm <- dfm(print_tokens)
#main dfm
main_dfm <- dfm(main_tokens)
#look at each dfm
print_dfm
main_dfm
```

### Word Frequency Ratings

Again, I can take a preliminary look at the data frame from each of the headlines to see the most frequent words after pre-processing.

The only significant change by removing "withdrawal" from my search term is that the term "exit" is not present any longer on the print headline frequency header. This is logical.

```{r echo=TRUE}
#create a word frequency variable and the rankings
#main headlines
main_counts <- as.data.frame(sort(colSums(main_dfm),dec=T))
colnames(main_counts) <- c("Frequency")
main_counts$Rank <- c(1:ncol(main_dfm))
head(main_counts)
#print headlines
print_counts <- as.data.frame(sort(colSums(print_dfm),dec=T))
colnames(print_counts) <- c("Frequency")
print_counts$Rank <- c(1:ncol(print_dfm))
head(print_counts)
```

## Feature Co-Occurrence Matrix

Now I can take a look at this network of feature co-occurrences again.

First, for the main headlines:

```{r code_folding = TRUE}
# create fcm from dfm
main_fcm <- fcm(main_dfm)
# check the dimensions (i.e., the number of rows and the number of columnns)
# of the matrix we created
dim(main_fcm)
# pull the top features
myFeatures <- names(topfeatures(main_fcm, 30))
# retain only those top features as part of our matrix
smaller_main_fcm <- fcm_select(main_fcm, pattern = myFeatures, selection = "keep")
# check dimensions
dim(smaller_main_fcm)
# compute size weight for vertices in network
size <- log(colSums(smaller_main_fcm))
# create plot
textplot_network(smaller_main_fcm, vertex_size = size / max(size) * 3)
```

and for the print headlines:

```{r code_folding = TRUE}
# create fcm from dfm
print_fcm <- fcm(print_dfm)
# check the dimensions (i.e., the number of rows and the number of columnns)
# of the matrix we created
dim(print_fcm)
# pull the top features
myFeatures <- names(topfeatures(print_fcm, 30))
# retain only those top features as part of our matrix
smaller_print_fcm <- fcm_select(print_fcm, pattern = myFeatures, selection = "keep")
# check dimensions
dim(smaller_print_fcm)
# compute size weight for vertices in network
size <- log(colSums(smaller_print_fcm))
# create plot
textplot_network(smaller_print_fcm, vertex_size = size / max(size) * 3)
```

The resulting matrices have definitely changed, at least slightly.

## Dictionary Analysis

To compare equally both this and my initial sentiment analysis, I am going to use the three dictionaries we used in the course tutorial, the NRC, LSD(2015) and General Inquiry dictionaries.

### NRC

I am first using the "liwcalike()" function from the quanteda.dictionaries package to apply the NRC dictionary. I can take a look at the head or tail and choose to look at a snapshot of the sentiments that have been applied to the corpus for each text group. Just at first glance, I can again see some differences in the scoring.

```{r code_folding = TRUE}
#use liwcalike() to estimate sentiment using NRC dictionary
#for main headlines
main_sentiment_nrc <- liwcalike(as.character(main_corpus), data_dictionary_NRC)
head(main_sentiment_nrc)[7:12]
#and print headlines
print_sentiment_nrc <- liwcalike(as.character(print_corpus), data_dictionary_NRC)
head(print_sentiment_nrc)[11:16]
```

### NRC as DFM 

I can also put the results into a document feature matrix for each text group:

```{r echo=TRUE}
#convert tokens from each headline data set to DFM using the dictionary "NRC"
main_nrc <- dfm(main_tokens) %>%
  dfm_lookup(data_dictionary_NRC)
print_nrc <- dfm(print_tokens) %>%
  dfm_lookup(data_dictionary_NRC)

dim(main_nrc)
main_nrc
dim(print_nrc)
print_nrc
```

### NRC Polarity Plot

And use the information in a data frame to plot the output as represented by a calculation for polarity:

```{r echo=TRUE}
library(cowplot)
#for the main headlines
df_main_nrc <- convert(main_nrc, to = "data.frame")
df_main_nrc$polarity <- (df_main_nrc$positive - df_main_nrc$negative)/(df_main_nrc$positive + df_main_nrc$negative)
df_main_nrc$polarity[which((df_main_nrc$positive + df_main_nrc$negative) == 0)] <- 0

ggplot(df_main_nrc) + 
  geom_histogram(aes(x=polarity), bins = 15) + 
  theme_minimal_hgrid()

#and the print headlines
df_print_nrc <- convert(print_nrc, to = "data.frame")
df_print_nrc$polarity <- (df_print_nrc$positive - df_print_nrc$negative)/(df_print_nrc$positive + df_print_nrc$negative)
df_print_nrc$polarity[which((df_print_nrc$positive + df_print_nrc$negative) == 0)] <- 0

ggplot(df_print_nrc) + 
  geom_histogram(aes(x=polarity), bins = 15) + 
  theme_minimal_hgrid()
```

### NRC Sample Results

Looking at the headlines that are indicated as "1", or positive in sentiment, this expanded corpus reflects more positivity than the top results from the smaller corpus.

```{r echo=TRUE}
head(main_corpus[which(df_main_nrc$polarity == 1)])
head(print_corpus[which(df_print_nrc$polarity == 1)])
```

### LSD 2015

I am going to want to look at multiple dictionaries to see if one can best apply to this data. Next, the LSD 2015 dictionary:

```{r echo=TRUE}
# convert main corpus to DFM using the LSD2015 dictionary
main_lsd2015 <- dfm(main_tokens) %>%
  dfm_lookup(data_dictionary_LSD2015)
# create main polarity measure for LSD2015
main_lsd2015 <- convert(main_lsd2015, to = "data.frame")
main_lsd2015$polarity <- (main_lsd2015$positive - main_lsd2015$negative)/(main_lsd2015$positive + main_lsd2015$negative)
main_lsd2015$polarity[which((main_lsd2015$positive + main_lsd2015$negative) == 0)] <- 0
# convert print corpus to DFM using the LSD2015 dictionary
print_lsd2015 <- dfm(print_tokens) %>%
  dfm_lookup(data_dictionary_LSD2015)
# create print polarity measure for LSD2015
print_lsd2015 <- convert(print_lsd2015, to = "data.frame")
print_lsd2015$polarity <- (print_lsd2015$positive - print_lsd2015$negative)/(print_lsd2015$positive + print_lsd2015$negative)
print_lsd2015$polarity[which((print_lsd2015$positive + print_lsd2015$negative) == 0)] <- 0
```

### LSD Sample Results

Looking at the headlines that are indicated as "1", or positive in sentiment, I can again see why these specific headlines are being evaluated as 'positive' despite more aberrations than in the NRC dictionary.

```{r echo=TRUE}
head(main_corpus[which(main_lsd2015$polarity == 1)])
head(print_corpus[which(print_lsd2015$polarity == 1)])
```

### LSD Polarity Plot

And use the information in a data frame to plot the output as represented by a calculation for polarity:

```{r echo=TRUE}
#for the main headlines
ggplot(main_lsd2015) + 
  geom_histogram(aes(x=polarity), bins = 15) + 
  theme_minimal_hgrid()

#and the print headlines
ggplot(print_lsd2015) + 
  geom_histogram(aes(x=polarity), bins = 15) + 
  theme_minimal_hgrid()
```

### General Inquirer

and the General Inquirer dictionary:

```{r echo=TRUE}
# convert main corpus to DFM using the General Inquirer dictionary
main_geninq <- dfm(main_tokens) %>%
                    dfm_lookup(data_dictionary_geninqposneg)
# create main polarity measure for GenInq
main_geninq <- convert(main_geninq, to = "data.frame")
main_geninq$polarity <- (main_geninq$positive - main_geninq$negative)/(main_geninq$positive + main_geninq$negative)
main_geninq$polarity[which((main_geninq$positive + main_geninq$negative) == 0)] <- 0
# convert print corpus to DFM using the General Inquirer dictionary
print_geninq <- dfm(print_tokens) %>%
                    dfm_lookup(data_dictionary_geninqposneg)
# create print polarity measure for GenInq
print_geninq <- convert(print_geninq, to = "data.frame")
print_geninq$polarity <- (print_geninq$positive - print_geninq$negative)/(print_geninq$positive + print_geninq $negative)
print_geninq$polarity[which((print_geninq$positive + print_geninq$negative) == 0)] <- 0
```

### General Inquirer Sample Results

Looking at the headlines that are indicated as "1", or positive in sentiment, again - this one is even more of a mixed bag, with the sentiment rationale clear. However, it is also clear why the rationale is being used at the expense of subtle subject matter knowledge.

```{r echo=TRUE}
head(main_corpus[which(main_geninq$polarity == 1)])
head(print_corpus[which(print_geninq$polarity == 1)])
```

### General Inquirer Polarity Plot

And use the information in a data frame to plot the output as represented by a calculation for polarity:

```{r echo=TRUE}
#for the main headlines
ggplot(main_geninq) + 
  geom_histogram(aes(x=polarity), bins = 15) + 
  theme_minimal_hgrid()

#and the print headlines
ggplot(print_geninq) + 
  geom_histogram(aes(x=polarity), bins = 15) + 
  theme_minimal_hgrid()
```

## Comparison Study

### Create Data Frame of All Results

Now I'm going to be able to compare the different dictionary scores in one data frame for each type of headline.

#### Main Headlines

```{r code_folding = TRUE}
# create unique names for each main headline dataframe
colnames(df_main_nrc) <- paste("nrc", colnames(df_main_nrc), sep = "_")
colnames(main_lsd2015) <- paste("lsd2015", colnames(main_lsd2015), sep = "_")
colnames(main_geninq) <- paste("geninq", colnames(main_geninq), sep = "_")
# now let's compare our estimates
main_sent <- merge(df_main_nrc, main_lsd2015, by.x = "nrc_doc_id", by.y = "lsd2015_doc_id")
main_sent <- merge(main_sent, main_geninq, by.x = "nrc_doc_id", by.y = "geninq_doc_id")
head(main_sent)[1:5]
```

#### Print Headlines

```{r code_folding = TRUE}
# create unique names for each print headline dataframe
colnames(df_print_nrc) <- paste("nrc", colnames(df_print_nrc), sep = "_")
colnames(print_lsd2015) <- paste("lsd2015", colnames(print_lsd2015), sep = "_")
colnames(print_geninq) <- paste("geninq", colnames(print_geninq), sep = "_")
# now let's compare our estimates
print_sent <- merge(df_print_nrc, print_lsd2015, by.x = "nrc_doc_id", by.y = "lsd2015_doc_id")
print_sent <- merge(print_sent, print_geninq, by.x = "nrc_doc_id", by.y = "geninq_doc_id")
head(print_sent)[1:5]

write.csv(main_sent, file="main_sent.csv")
write.csv(print_sent, file="print_sent.csv")
```


### Correlation

Now that I have them all in a single data frame, it's straightforward to figure out a bit about how well our different measures of polarity agree across the different approaches by looking at their correlation using the "cor()" function.

It seems like the polarity of the headlines are more similar in this expanded analysis.

#### For Main Headlines

```{r echo=TRUE}
cor(main_sent$nrc_polarity, main_sent$lsd2015_polarity)
cor(main_sent$nrc_polarity, main_sent$geninq_polarity)
cor(main_sent$lsd2015_polarity, main_sent$geninq_polarity)
```

#### For Print Headlines

```{r echo=TRUE}
cor(print_sent$nrc_polarity, print_sent$lsd2015_polarity)
cor(print_sent$nrc_polarity, print_sent$geninq_polarity)
cor(print_sent$lsd2015_polarity, print_sent$geninq_polarity)
```

### Correlation of NRC Sentiments

I can take a quick visual look at the correlation between sentiments detected in both sets of headlines using the "GGally" package. There seems to be very little difference in that regard.

#### Main Headlines

```{r echo=TRUE}
library(GGally)

main_nrc_only<- read.csv("main_sent_nrc_only.csv")
ggcorr(main_nrc_only, method = c("everything", "pearson"))
```

#### Print Headlines

```{r echo=TRUE}

print_nrc_only<- read.csv("print_sent_nrc_only.csv")
ggcorr(print_nrc_only, method = c("everything", "pearson"))
```

### Linear Model Testing

Finally, I want to visually look at the correlations or positive and negative sentiments as my starting point for understanding relationships between both my sentiment analyses and dictionaries. I'll start by dividing the sentiment scores for positive and negative from each text source into its own object and change column names to make them unique except for 'doc_id' for joining them into one data frame.

```{r code_folding=TRUE}
corr_main <- main_sent %>%
  select(nrc_doc_id, nrc_polarity, lsd2015_polarity, geninq_polarity )
colnames(corr_main) <- c("doc_id", "main_nrc", "main_lsd", "main_geninq")
corr_print <- print_sent %>%
  select(nrc_doc_id, nrc_polarity, lsd2015_polarity, geninq_polarity )
colnames(corr_print) <- c("doc_id", "print_nrc", "print_lsd", "print_geninq")

corr_matrix <- join(corr_main, corr_print, by = "doc_id")
head(corr_matrix)
```

Then I can look at the model for each relationship

#### NRC

```{r code_folding=TRUE}

#run the linear model of main vs. print correlation in the NRC dictionary
lm_nrc <- lm(main_nrc~print_nrc, data = corr_matrix)
summary(lm_nrc)
```

#### LSD

```{r code_folding=TRUE}
#run the linear model of main vs. print correlation in the LSD dictionary
lm_lsd <- lm(main_lsd~print_lsd, data = corr_matrix)
summary(lm_lsd)
```

#### General Inquiry

```{r code_folding=TRUE}
#run the linear model of main vs. print correlation in the General Inquiry dictionary
lm_geninq <- lm(main_geninq~print_geninq, data = corr_matrix)
summary(lm_geninq)
```

And try to look at if there is any meaningful difference between the models. Despite the differences in the expanded dataset from the primary one, there stil does not seem to be any meaningful difference between main and print headlines based on these analyses.

```{r code_folding=TRUE}
#create a data frame from the NRC model results
tidynrc <- tidy(lm_nrc, conf.int = FALSE) 
#round the results to 3 decimal points
tidynrc <- tidynrc %>%
  mutate_if(is.numeric, round, 3)
tidynrc$model <- c("nrc")

#create a data frame from the LSD model results
tidylsd <- tidy(lm_lsd, conf.int = FALSE) 
#round the results to 3 decimal points
tidylsd <- tidylsd %>%
  mutate_if(is.numeric, round, 3)
tidylsd$model <- c("lsd")

#create a data frame from the Gen Inq model results
tidygeninq <- tidy(lm_geninq, conf.int = FALSE) 
#round the results to 3 decimal points
tidygeninq <- tidygeninq %>%
  mutate_if(is.numeric, round, 3)
tidygeninq$model <- c("geninq")

tidy_all <- do.call("rbind", list(tidynrc, tidylsd, tidygeninq))

tidy_all
                 
```

## Visualizing NRC Sentiment

### Main Headlines

```{r code_folding=TRUE}
#main headlines
head(main_nrc)
#transpose
main_df <-data.frame(t(main_nrc))
#The function rowSums computes column sums across rows for each level of a grouping variable.
df_new <- data.frame(rowSums(main_df[2:937]))
#Transformation and cleaning
names(df_new)[1] <- "count"
df_new <- cbind("sentiment" = rownames(df_new), df_new)
rownames(df_new) <- NULL
df_new2<-df_new[1:10,]
df_new2 <- read.csv("df_new2.csv")
#Plot One - count of words associated with each sentiment
quickplot(sentiment, data=df_new2, weight=count, geom="bar", fill=sentiment, ylab="count")+ggtitle("Main Headline Sentiments")
```

### Print Headlines

```{r code_folding=TRUE}
#print headlines
head(print_nrc)
#transpose
main_df2 <-data.frame(t(print_nrc))
#The function rowSums computes column sums across rows for each level of a grouping variable.
df_new3 <- data.frame(rowSums(main_df2[2:937]))
#Transformation and cleaning
names(df_new3)[1] <- "count"
df_new3 <- cbind("sentiment" = rownames(df_new3), df_new3)
rownames(df_new3) <- NULL
df_new4<-df_new3[1:10,]
df_new4 <- read.csv("df_new4.csv")
#Plot One - count of words associated with each sentiment
quickplot(sentiment, data=df_new4, weight=count, geom="bar", fill=sentiment, ylab="count")+ggtitle("Print Headline Sentiments")
```

### Clustered Bar Chart

```{r code_folding=TRUE}

#load data frame with both headline sentiments from NRC
df_dual <- read.csv("df_dual.csv")

ggplot(df_dual,
       aes(x = sentiment,
           y = count,
           fill = headline)) +
  geom_bar(stat = "identity",
           position = "dodge") +
  scale_fill_manual(values=c("#993333", "#336699")) +
  theme_minimal()
```

# Summary

Although I was not able to find any statistically meaningful, measurable difference between the sentiments of print vs. main headlines analyzed in this project, it is still a valid observation that there is an overall pattern to be observed.

Specifically, there is a pattern that the print headlines carry a lower level of emotionally weighted words than the online headlines. I can't make a factual observation, but I would like to do further, expanded studies in articles in this research path to investigate the hypothesis.

# Citations

Citations:

* This research makes use of the [NRC Word-Emotion Association Lexicon](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm), created by Saif Mohammad and Peter Turney at the National Research Council Canada. 

* This research makes use of the [LSD Lexicoder Sentiment Dictionary](http://www.snsoroka.com/data-lexicoder/). This dataset was first published in Young, L. & Soroka, S. (2012). Affective News: The Automated Coding of Sentiment in Political Texts]. doi: 10.1080/10584609.2012.671234 . Political Communication, 29(2), 205--231.

* This research makes use of the [General Inquirer Lexicon](http://www.mariapinto.es/ciberabstracts/Articulos/Inquirer.htm), Stone, P. J. (1962). The general inquirer: A computer system for content analysis and retrieval based on the sentence as a unit of information. Harvard: Laboratory of Social Relations, Harvard University.
