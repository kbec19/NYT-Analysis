---
title: "Analysis of PDF Articles"
description: |
  Text as Data Project-Article Sentiment Research
categories:
  - text as data
  - NYT text analysis project
author:
  - name: Kristina Becvar
    url: https://kbec19.github.io/NYT-Analysis/
    affiliation: UMass DACSS Program (My Academic Blog Link)
    affiliation_url: https://kristinabecvar.com
slug: pdf-analysis
date: 04/17/2022
output:
  distill::distill_article:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

# Getting Started

The primary goal of this aspect of research is to refine the process for examining the content of the full articles for which the main vs. print headlines are the most different from each other in the primary project analysis.

## Pulling in the PDF docs

I have the PDF files in my working directory. Using the "list.files()" function from the "pdftools" package, I can create a vector of PDF file names, specifying only files that end in ".pdf".

```{r code_folding=TRUE}
#load libraries
library(pdftools)
library(readtext)
library(readr)
library(tm)
library(tidytext)
library(stringr)
library(MASS)
library(tidyverse)
library(plyr); library(dplyr)
library(quanteda)
library(purrr)
library(here)
```

## Extracting PDF Files being examined (random at this time - exploratory)

```{r echo=TRUE}
#create file names
files <- list.files(pattern = "pdf$")

#extract the pdf file data
nyt_articles <- lapply(files, pdf_text)

#apply length functions
lapply(nyt_articles, length)

#view the structure of the list
str(nyt_articles)
```

## Inspect the first article

```{r echo = TRUE}
head(nyt_articles[1])
```

## Inspecting Individual Articles

Now I'm going to use "purrr" to "pluck()" each of the articles as its' own vector and create a corpus of each article to examine.

```{r echo=TRUE}
article_111 <- nyt_articles %>% 
  pluck(1)
article_111 <- as_vector(article_111)

article_111_corpus <- corpus(article_111)
article_111_summary <- summary(article_111_corpus)
article_111_summary
```

I also found a very interesting way to pul the text and save them as individual .txt files, but for now I'm just going to note that as an alternative process. I've struggled quite a bit to get the PDF text read compared to the headlines.

```{r code_folding = TRUE}

convertpdf2txt <- function(dirpath){
  files <- list.files(dirpath, full.names = T)
  x <- sapply(files, function(x){
  x <- pdftools::pdf_text(x) %>%
  paste(sep = " ") %>%
  stringr::str_replace_all(fixed("\n"), " ") %>%
  stringr::str_replace_all(fixed("\r"), " ") %>%
  stringr::str_replace_all(fixed("\t"), " ") %>%
  stringr::str_replace_all(fixed("\""), " ") %>%
  paste(sep = " ", collapse = " ") %>%
  stringr::str_squish() %>%
  stringr::str_replace_all("- ", "") 
  return(x)
    })
}
# apply function
txts <- convertpdf2txt("./files")
# inspect the structure of the txts element
str(txts)
```
```{r code_folding = TRUE}
#apply length functions
lapply(txts, length)

#view the structure of the list
str(txts)

# add names to txt files
names(txts) <- paste("nyt", 1:length(txts), sep = "")
# save result to disc
lapply(seq_along(txts), function(i)writeLines(text = unlist(txts[i]),
    con = paste("./txts", names(txts)[i],".txt", sep = "")))

```

## Unlist

Documenting, for now, the ways I'm struggling with so I can find out why.

```{r echo=TRUE}
#convert list to vector
#nyt_vector <- unlist(nyt_articles, recursive = TRUE)
#put articles into data frame
#nyt_df <- as.data.frame(nyt_vector, row.names = NULL, stringsAsFactors = FALSE)
```

```{r echo=TRUE}
#create corpus
#nyt_corpus <- corpus(txts)
#confirming class of corpus
#class(nyt_corpus)
#confirm length of corpus
#length(nyt_corpus)
```

# Conclusion

I will not be able to fit this type of analysis into the scope of my current project. I will use this in further studies.