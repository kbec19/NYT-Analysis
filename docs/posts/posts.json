[
  {
    "path": "posts/headline-expanded/",
    "title": "Analysis of Main vs. Print Headlines: Phase 2",
    "description": "Text as Data Project Headline Comparison Research Using API Query \"Afghanistan\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/NYT-Analysis/"
      }
    ],
    "date": "2022-04-26",
    "categories": [
      "text as data",
      "NYT text analysis project"
    ],
    "contents": "\r\n\r\nContents\r\nComparing\r\nMain vs. Print Headlines in the New York Times\r\nMaking\r\nDifferent Choices on Inclusion of Observations\r\nGathering Data\r\nPrevious Process\r\nLoad Data\r\nCreate Corpus\r\nAssign Type to Docvars\r\nTokenization\r\nDocument Feature Matrix\r\n\r\nDictionary Analysis\r\nliwcalike()\r\nNRC\r\nLSD 2015\r\nGeneral Inquirer\r\nVisualization\r\n\r\n\r\nUtilizing the cleanNLP\r\npackage:\r\nExploratory Analysis\r\n\r\nComparing\r\nMain vs. Print Headlines in the New York Times\r\nMaking\r\nDifferent Choices on Inclusion of Observations\r\nIn my initial look at the headline data, it was clear that not all of\r\nthe articles had different headlines; some are the same entries, and\r\nsome have “N/A” in the “print” version only, indicating they were\r\nonline-only stories. Although I initially felt inclined to leave the\r\n“N/A” observations in the analysis, I removed those observations as they\r\nwould not be relevant to my new research questions comparing the framing\r\nfor different audiences.\r\nI also removed whole sections where the API returned an observation\r\nas there was apparently use of the term “Afghanistan” somewhere in the\r\narticle/entry, but the type of entry was clearly not being represented\r\nin the headline. For example, “Corrections” entries have headlines\r\nconsisting only of the term “Corrections” and the corresponding date.\r\nSimilar choices were made on the “Arts”, Books”, and “Podcasts” sections\r\nwhen entries are primarily the names of the things being reviewed that\r\nmay have a reference to the Afghanistan withdrawal somewhere in the\r\ntext, but it is not relevant specifically to the withdrawal time period\r\nbeing analyzed.\r\nWith few exceptions, this left the entirety of the “U.S.” and “World”\r\nnews sections, even if the content related to Afghanistan is not readily\r\nobservable. The count (~650) matched the number of articles pulled for\r\nthe hand coding research as well.\r\nGathering Data\r\nPrevious Process\r\nThe data was pulled via API using the same process as in my first\r\nphase of the comparison research, with the only change in the query term\r\n“Afghanistan” as opposed to “Afghanistan Withdrawal”. This led to a\r\nlarger dataset for comparison, though most of the increase in count was\r\nfiltered out due to their classification as not news-related.\r\nLoad Data\r\nNow to the active review of the data. Loading the data from my\r\ncollection phase:\r\n\r\n  doc_id       date\r\n1      1  7/17/2020\r\n2      2  8/30/2020\r\n3      3   6/2/2021\r\n4      4 12/20/2020\r\n5      5  9/11/2021\r\n6      6   9/1/2021\r\n                                                                           text\r\n1  $174 Million Afghan Drone Program Is Riddled With Problems, U.S. Report Says\r\n2          ‘A Hail Mary’: Psychedelic Therapy Draws Veterans to Jungle Retreats\r\n3   ‘Come On In, Boys’: A Wave of the Hand Sets Off Spain-Morocco Migrant Fight\r\n4 ‘Covid Can’t Compete.’ In a Place Mired in War, the Virus Is an Afterthought.\r\n5    ‘Everything Changed Overnight’: Afghan Reporters Face an Intolerant Regime\r\n6      ‘Finally, I Am Safe’: U.S. Air Base Becomes Temporary Refuge for Afghans\r\n  doc_id       date\r\n1      1  7/17/2020\r\n2      2  8/30/2020\r\n3      3   6/2/2021\r\n4      4 12/20/2020\r\n5      5  9/11/2021\r\n6      6   9/1/2021\r\n                                                                            text\r\n1 $174 Million Drone Program for Afghans Is Riddled With Problems, Pentagon Says\r\n2                Psychedelic Therapy In the Jungle Soothes The Pain for Veterans\r\n3                                 Morocco Sends Spanish Outpost a Migrant Influx\r\n4         ‘It’s a Lie’: Denial and Skepticism Permeate a Nation Embroiled in War\r\n5                                     ‘Everything Changed’: Media Face Crackdown\r\n6         ‘Finally, I Am Safe’: Thousands Find Temporary Refuge at U.S. Air Base\r\n\r\nCreate Corpus\r\n\r\n\r\nmain_corpus <- corpus(main_headlines, docid_field = \"doc_id\", text_field = \"text\")\r\nprint_corpus <- corpus(print_headlines, docid_field = \"doc_id\", text_field = \"text\")\r\n\r\n\r\n\r\nAssign Type to Docvars\r\n\r\n\r\nmain_corpus$type <- \"Main Headline\"\r\nprint_corpus$type <- \"Print Headline\"\r\ndocvars(main_corpus, field = \"type\") <- main_corpus$type\r\ndocvars(print_corpus, field = \"type\") <- print_corpus$type\r\n\r\n\r\n\r\nTokenization\r\nI want to optimize pre-processing by removing the “�” symbol that has\r\nplagued me since starting working with this API by using\r\n“remove_symbols=TRUE” in addition to removing the punctuation when\r\ntokenizing. I also want to remove stopwords. I do NOT want to use\r\nstemming at this point.\r\nMain Headlines\r\n\r\n[1] 936\r\nTokens consisting of 936 documents and 2 docvars.\r\n1 :\r\n [1] \"174\"      \"Million\"  \"Afghan\"   \"Drone\"    \"Program\"  \"Riddled\" \r\n [7] \"Problems\" \"U.S\"      \"Report\"   \"Says\"    \r\n\r\n2 :\r\n[1] \"Hail\"        \"Mary\"        \"Psychedelic\" \"Therapy\"    \r\n[5] \"Draws\"       \"Veterans\"    \"Jungle\"      \"Retreats\"   \r\n\r\n3 :\r\n[1] \"Come\"          \"Boys\"          \"Wave\"          \"Hand\"         \r\n[5] \"Sets\"          \"Spain-Morocco\" \"Migrant\"       \"Fight\"        \r\n\r\n4 :\r\n[1] \"Covid\"        \"Compete\"      \"Place\"        \"Mired\"       \r\n[5] \"War\"          \"Virus\"        \"Afterthought\"\r\n\r\n5 :\r\n[1] \"Everything\" \"Changed\"    \"Overnight\"  \"Afghan\"     \"Reporters\" \r\n[6] \"Face\"       \"Intolerant\" \"Regime\"    \r\n\r\n6 :\r\n[1] \"Finally\"   \"Safe\"      \"U.S\"       \"Air\"       \"Base\"     \r\n[6] \"Becomes\"   \"Temporary\" \"Refuge\"    \"Afghans\"  \r\n\r\n[ reached max_ndoc ... 930 more documents ]\r\n\r\nPrint Headlines\r\n\r\n[1] 936\r\nTokens consisting of 936 documents and 2 docvars.\r\n1 :\r\n[1] \"174\"      \"Million\"  \"Drone\"    \"Program\"  \"Afghans\"  \"Riddled\" \r\n[7] \"Problems\" \"Pentagon\" \"Says\"    \r\n\r\n2 :\r\n[1] \"Psychedelic\" \"Therapy\"     \"Jungle\"      \"Soothes\"    \r\n[5] \"Pain\"        \"Veterans\"   \r\n\r\n3 :\r\n[1] \"Morocco\" \"Sends\"   \"Spanish\" \"Outpost\" \"Migrant\" \"Influx\" \r\n\r\n4 :\r\n[1] \"Lie\"        \"Denial\"     \"Skepticism\" \"Permeate\"   \"Nation\"    \r\n[6] \"Embroiled\"  \"War\"       \r\n\r\n5 :\r\n[1] \"Everything\" \"Changed\"    \"Media\"      \"Face\"       \"Crackdown\" \r\n\r\n6 :\r\n[1] \"Finally\"   \"Safe\"      \"Thousands\" \"Find\"      \"Temporary\"\r\n[6] \"Refuge\"    \"U.S\"       \"Air\"       \"Base\"     \r\n\r\n[ reached max_ndoc ... 930 more documents ]\r\n\r\nDocument Feature Matrix\r\n\r\n\r\n\r\n\r\n\r\n#create a word frequency variable and the rankings\r\nmain_counts <- as.data.frame(sort(colSums(main_dfm),dec=T))\r\ncolnames(main_counts) <- c(\"Frequency\")\r\nmain_counts$Rank <- c(1:ncol(main_dfm))\r\nhead(main_counts)\r\n\r\n\r\n            Frequency Rank\r\nu.s               166    1\r\nafghan            151    2\r\nafghanistan       135    3\r\ntaliban           110    4\r\nbiden              95    5\r\nwar                64    6\r\n\r\nprint_counts <- as.data.frame(sort(colSums(print_dfm),dec=T))\r\ncolnames(print_counts) <- c(\"Frequency\")\r\nprint_counts$Rank <- c(1:ncol(print_dfm))\r\nhead(print_counts)\r\n\r\n\r\n            Frequency Rank\r\nu.s               171    1\r\nafghan            109    2\r\ntaliban           108    3\r\nafghanistan        84    4\r\nbiden              76    5\r\nwar                53    6\r\n\r\nNow I can take a look at this network of feature co-occurrences for\r\nthe main headlines:\r\n\r\n[1] 2804 2804\r\n[1] 20 20\r\n\r\n\r\nand for the print headlines:\r\n\r\n[1] 2717 2717\r\n[1] 20 20\r\n\r\n\r\nDictionary Analysis\r\nliwcalike()\r\n\r\n [1] \"docname\"      \"Segment\"      \"WPS\"          \"WC\"          \r\n [5] \"Sixltr\"       \"Dic\"          \"anger\"        \"anticipation\"\r\n [9] \"disgust\"      \"fear\"         \"joy\"          \"negative\"    \r\n[13] \"positive\"     \"sadness\"      \"surprise\"     \"trust\"       \r\n[17] \"AllPunc\"      \"Period\"       \"Comma\"        \"Colon\"       \r\n[21] \"SemiC\"        \"QMark\"        \"Exclam\"       \"Dash\"        \r\n[25] \"Quote\"        \"Apostro\"      \"Parenth\"      \"OtherP\"      \r\n\r\nNRC\r\n\r\n\r\n# convert tokens from each headline data set to DFM using the dictionary \"NRC\"\r\nmain_nrc <- dfm(main_tokens) %>%\r\n  dfm_lookup(data_dictionary_NRC)\r\nprint_nrc <- dfm(print_tokens) %>%\r\n  dfm_lookup(data_dictionary_NRC)\r\n\r\ndim(main_nrc)\r\n\r\n\r\n[1] 936  10\r\n\r\nmain_nrc\r\n\r\n\r\nDocument-feature matrix of: 936 documents, 10 features (67.61% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            0       0    0   0        2        0       0\r\n   2     0            0       0    1   0        1        1       0\r\n   3     1            0       0    1   0        1        0       0\r\n   4     0            0       0    1   0        2        0       0\r\n   5     1            0       1    1   0        1        0       1\r\n   6     0            1       1    0   2        0        2       0\r\n    features\r\ndocs surprise trust\r\n   1        0     0\r\n   2        0     1\r\n   3        0     0\r\n   4        0     0\r\n   5        0     0\r\n   6        1     3\r\n[ reached max_ndoc ... 930 more documents ]\r\n\r\ndim(print_nrc)\r\n\r\n\r\n[1] 936  10\r\n\r\nprint_nrc\r\n\r\n\r\nDocument-feature matrix of: 936 documents, 10 features (69.21% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            0       0    0   0        2        0       0\r\n   2     0            0       0    2   0        1        0       1\r\n   3     0            0       0    1   0        0        0       0\r\n   4     1            0       1    1   0        4        0       1\r\n   5     0            0       0    0   0        0        0       0\r\n   6     0            1       1    0   2        0        2       0\r\n    features\r\ndocs surprise trust\r\n   1        0     0\r\n   2        0     0\r\n   3        0     0\r\n   4        0     1\r\n   5        0     0\r\n   6        1     3\r\n[ reached max_ndoc ... 930 more documents ]\r\n\r\nAnd use the information in a data frame to plot the output:\r\n\r\n\r\n#for the main headlines\r\ndf_main_nrc <- convert(main_nrc, to = \"data.frame\")\r\ndf_main_nrc$polarity <- (df_main_nrc$positive - df_main_nrc$negative)/(df_main_nrc$positive + df_main_nrc$negative)\r\ndf_main_nrc$polarity[which((df_main_nrc$positive + df_main_nrc$negative) == 0)] <- 0\r\n\r\nggplot(df_main_nrc) + \r\n  geom_histogram(aes(x=polarity)) + \r\n  theme_bw()\r\n\r\n\r\n\r\n#and the print headlines\r\ndf_print_nrc <- convert(print_nrc, to = \"data.frame\")\r\ndf_print_nrc$polarity <- (df_print_nrc$positive - df_print_nrc$negative)/(df_print_nrc$positive + df_print_nrc$negative)\r\ndf_print_nrc$polarity[which((df_print_nrc$positive + df_print_nrc$negative) == 0)] <- 0\r\n\r\nggplot(df_print_nrc) + \r\n  geom_histogram(aes(x=polarity)) + \r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nLooking at the headlines that are indicated as “1”, or positive in\r\nsentiment, it’s clear that this dictionary is not capturing the\r\nsentiment accurately.\r\n\r\n\r\nhead(main_corpus[which(df_main_nrc$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n6 :\r\n\"‘Finally, I Am Safe’: U.S. Air Base Becomes Temporary Refuge...\"\r\n\r\n8 :\r\n\"‘Football Is Like Food’: Afghan Female Soccer Players Find a...\"\r\n\r\n18 :\r\n\"‘Is Austin on Your List?’: Biden’s Pentagon Pick Rose Despit...\"\r\n\r\n29 :\r\n\"‘We Have to Try’: Lawmakers Rush to Assist in Afghanistan Ev...\"\r\n\r\n40 :\r\n\"4 Takeaways From the U.S. Deal With the Taliban\"\r\n\r\n43 :\r\n\"98 Countries Pledge to Accept Afghans After U.S. Military De...\"\r\n\r\nhead(print_corpus[which(df_print_nrc$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n6 :\r\n\"‘Finally, I Am Safe’: Thousands Find Temporary Refuge at U.S...\"\r\n\r\n15 :\r\n\"Veterans Feel Urgency to Aid Afghan Allies\"\r\n\r\n18 :\r\n\"How Biden’s Defense Nominee Overcame Barriers to Diversity\"\r\n\r\n25 :\r\n\"How Biden, by Turns Genial and Blunt, Built Diplomatic Bridg...\"\r\n\r\n45 :\r\n\"Rescue Flight To Germany Inspires Name For Newborn\"\r\n\r\n50 :\r\n\"A Call for the Return of Civility, And Truth as a Guiding Li...\"\r\n\r\nLSD 2015\r\nI am going to want to look at multiple dictionaries to see if one can\r\nbest apply to this data. First, the LSD 2015 dictionary:\r\n\r\n\r\n# convert main corpus to DFM using the LSD2015 dictionary\r\nmain_lsd2015 <- dfm(tokens(main_corpus, remove_punct = TRUE),\r\n                              tolower = TRUE) %>%\r\n                          dfm_lookup(data_dictionary_LSD2015)\r\n# create main polarity measure for LSD2015\r\nmain_lsd2015 <- convert(main_lsd2015, to = \"data.frame\")\r\nmain_lsd2015$polarity <- (main_lsd2015$positive - main_lsd2015$negative)/(main_lsd2015$positive + main_lsd2015$negative)\r\nmain_lsd2015$polarity[which((main_lsd2015$positive + main_lsd2015$negative) == 0)] <- 0\r\n# convert print corpus to DFM using the LSD2015 dictionary\r\nprint_lsd2015 <- dfm(tokens(print_corpus, remove_punct = TRUE),\r\n                              tolower = TRUE) %>%\r\n                          dfm_lookup(data_dictionary_LSD2015)\r\n# create print polarity measure for LSD2015\r\nprint_lsd2015 <- convert(print_lsd2015, to = \"data.frame\")\r\nprint_lsd2015$polarity <- (print_lsd2015$positive - print_lsd2015$negative)/(print_lsd2015$positive + print_lsd2015$negative)\r\nprint_lsd2015$polarity[which((print_lsd2015$positive + print_lsd2015$negative) == 0)] <- 0\r\n\r\n\r\n\r\nGeneral Inquirer\r\nand the General Inquirer dictionary:\r\n\r\n\r\n# convert main corpus to DFM using the General Inquirer dictionary\r\nmain_geninq <- dfm(tokens(main_corpus, remove_punct = TRUE),\r\n                             tolower = TRUE) %>%\r\n                    dfm_lookup(data_dictionary_geninqposneg)\r\n# create main polarity measure for GenInq\r\nmain_geninq <- convert(main_geninq, to = \"data.frame\")\r\nmain_geninq$polarity <- (main_geninq$positive - main_geninq$negative)/(main_geninq$positive + main_geninq$negative)\r\nmain_geninq$polarity[which((main_geninq$positive + main_geninq$negative) == 0)] <- 0\r\n# convert print corpus to DFM using the General Inquirer dictionary\r\nprint_geninq <- dfm(tokens(print_corpus, remove_punct = TRUE),\r\n                             tolower = TRUE) %>%\r\n                    dfm_lookup(data_dictionary_geninqposneg)\r\n# create print polarity measure for GenInq\r\nprint_geninq <- convert(print_geninq, to = \"data.frame\")\r\nprint_geninq$polarity <- (print_geninq$positive - print_geninq$negative)/(print_geninq$positive + print_geninq $negative)\r\nprint_geninq$polarity[which((print_geninq$positive + print_geninq$negative) == 0)] <- 0\r\n\r\n\r\n\r\nNow I’m going to be able to compare the different dictionary scores\r\nin one data frame for each type of headline.\r\n\r\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy\r\n1          1         0                0           0        0       0\r\n2         10         0                0           0        2       0\r\n3        100         0                0           0        1       0\r\n4        101         0                2           0        0       1\r\n5        102         0                1           0        0       1\r\n6        103         1                1           1        1       1\r\n  nrc_negative nrc_positive nrc_sadness nrc_surprise nrc_trust\r\n1            2            0           0            0         0\r\n2            1            0           1            0         1\r\n3            1            0           0            0         0\r\n4            0            1           0            0         1\r\n5            0            1           0            0         1\r\n6            1            1           1            0         1\r\n  nrc_polarity lsd2015_negative lsd2015_positive lsd2015_neg_positive\r\n1           -1                1                0                    0\r\n2           -1                1                0                    0\r\n3           -1                2                0                    0\r\n4            1                1                1                    0\r\n5            1                0                1                    0\r\n6            0                1                1                    0\r\n  lsd2015_neg_negative lsd2015_polarity geninq_positive\r\n1                    0               -1               0\r\n2                    0               -1               0\r\n3                    0               -1               0\r\n4                    0                0               1\r\n5                    0                1               2\r\n6                    0                0               1\r\n  geninq_negative geninq_polarity\r\n1               0               0\r\n2               1              -1\r\n3               1              -1\r\n4               0               1\r\n5               0               1\r\n6               1               0\r\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy\r\n1          1         0                0           0        0       0\r\n2         10         0                0           0        2       0\r\n3        100         0                0           0        1       0\r\n4        101         0                2           0        0       2\r\n5        102         0                1           0        0       1\r\n6        103         1                0           1        1       0\r\n  nrc_negative nrc_positive nrc_sadness nrc_surprise nrc_trust\r\n1            2            0           0            0         0\r\n2            2            0           1            0         0\r\n3            1            0           0            0         0\r\n4            1            2           0            1         2\r\n5            0            1           0            0         1\r\n6            1            0           1            0         0\r\n  nrc_polarity lsd2015_negative lsd2015_positive lsd2015_neg_positive\r\n1   -1.0000000                1                0                    0\r\n2   -1.0000000                2                0                    0\r\n3   -1.0000000                3                0                    0\r\n4    0.3333333                2                1                    0\r\n5    1.0000000                0                1                    0\r\n6   -1.0000000                1                0                    0\r\n  lsd2015_neg_negative lsd2015_polarity geninq_positive\r\n1                    0       -1.0000000               0\r\n2                    0       -1.0000000               0\r\n3                    0       -1.0000000               0\r\n4                    0       -0.3333333               2\r\n5                    0        1.0000000               1\r\n6                    0       -1.0000000               0\r\n  geninq_negative geninq_polarity\r\n1               0       0.0000000\r\n2               1      -1.0000000\r\n3               1      -1.0000000\r\n4               1       0.3333333\r\n5               0       1.0000000\r\n6               1      -1.0000000\r\n\r\nNow that we have them all in a single data frame, it’s\r\nstraightforward to figure out a bit about how well our different\r\nmeasures of polarity agree across the different approaches by looking at\r\ntheir correlation using the “cor()” function.\r\n\r\n\r\ncor(main_sent$nrc_polarity, main_sent$lsd2015_polarity)\r\n\r\n\r\n[1] 0.4547764\r\n\r\ncor(main_sent$nrc_polarity, main_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4633912\r\n\r\ncor(main_sent$lsd2015_polarity, main_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.5265193\r\n\r\n\r\n\r\ncor(print_sent$nrc_polarity, print_sent$lsd2015_polarity)\r\n\r\n\r\n[1] 0.4651277\r\n\r\ncor(print_sent$nrc_polarity, print_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4747134\r\n\r\ncor(print_sent$lsd2015_polarity, print_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4985192\r\n\r\nVisualization\r\n\r\n\r\n\r\nset.seed(11)\r\n# draw the wordcloud\r\nlibrary(wordcloud)\r\n\r\npar(mfrow=c(1,1)) # 1 panel plot\r\npar(mar=c(1, 3, 1, 3)) # Set the plot margin\r\npar(bg=\"black\") # set background color as black\r\npar(col.main=\"white\") # set title color as white\r\nwordcloud(main_corpus, scale=c(4,.5),min.freq=5, max.words=Inf, random.order=F, random.color=F, \r\n          colors = brewer.pal(8, \"Set3\"))   \r\ntitle(\"Main Website Headlines\")\r\n\r\n\r\n\r\nwordcloud(print_corpus, scale=c(4,.5),min.freq=5, max.words=Inf, random.order=F, random.color=F, \r\n          colors = brewer.pal(8, \"Set3\"))   \r\ntitle(\"Print Headlines\")\r\n\r\n\r\n\r\n\r\nI clearly have not done enough pre-processing to the corpus.\r\nUtilizing the cleanNLP\r\npackage:\r\nAs usual, I am struggling to get this to work due to my inexperience\r\nwith the python backend. What worked for me last time I ran this in a\r\nprevious tutorial was no longer working, so I had to uninstall my\r\nminiconda installation and re-install it, but eventually it initialized\r\nso I could run the “cnlp_annotate” function.\r\n\r\n\r\ncnlp_init_udpipe()\r\n\r\n#first the main headlines\r\n\r\nmain_tibble <- as_tibble(main_headlines) %>%\r\n  select(c(\"doc_id\", \"text\"))\r\nmain_tibble <- utf8::as_utf8(main_tibble$text)\r\n\r\nannotated_main <- cnlp_annotate(main_tibble)\r\n\r\n\r\nProcessed document 10 of 936\r\nProcessed document 20 of 936\r\nProcessed document 30 of 936\r\nProcessed document 40 of 936\r\nProcessed document 50 of 936\r\nProcessed document 60 of 936\r\nProcessed document 70 of 936\r\nProcessed document 80 of 936\r\nProcessed document 90 of 936\r\nProcessed document 100 of 936\r\nProcessed document 110 of 936\r\nProcessed document 120 of 936\r\nProcessed document 130 of 936\r\nProcessed document 140 of 936\r\nProcessed document 150 of 936\r\nProcessed document 160 of 936\r\nProcessed document 170 of 936\r\nProcessed document 180 of 936\r\nProcessed document 190 of 936\r\nProcessed document 200 of 936\r\nProcessed document 210 of 936\r\nProcessed document 220 of 936\r\nProcessed document 230 of 936\r\nProcessed document 240 of 936\r\nProcessed document 250 of 936\r\nProcessed document 260 of 936\r\nProcessed document 270 of 936\r\nProcessed document 280 of 936\r\nProcessed document 290 of 936\r\nProcessed document 300 of 936\r\nProcessed document 310 of 936\r\nProcessed document 320 of 936\r\nProcessed document 330 of 936\r\nProcessed document 340 of 936\r\nProcessed document 350 of 936\r\nProcessed document 360 of 936\r\nProcessed document 370 of 936\r\nProcessed document 380 of 936\r\nProcessed document 390 of 936\r\nProcessed document 400 of 936\r\nProcessed document 410 of 936\r\nProcessed document 420 of 936\r\nProcessed document 430 of 936\r\nProcessed document 440 of 936\r\nProcessed document 450 of 936\r\nProcessed document 460 of 936\r\nProcessed document 470 of 936\r\nProcessed document 480 of 936\r\nProcessed document 490 of 936\r\nProcessed document 500 of 936\r\nProcessed document 510 of 936\r\nProcessed document 520 of 936\r\nProcessed document 530 of 936\r\nProcessed document 540 of 936\r\nProcessed document 550 of 936\r\nProcessed document 560 of 936\r\nProcessed document 570 of 936\r\nProcessed document 580 of 936\r\nProcessed document 590 of 936\r\nProcessed document 600 of 936\r\nProcessed document 610 of 936\r\nProcessed document 620 of 936\r\nProcessed document 630 of 936\r\nProcessed document 640 of 936\r\nProcessed document 650 of 936\r\nProcessed document 660 of 936\r\nProcessed document 670 of 936\r\nProcessed document 680 of 936\r\nProcessed document 690 of 936\r\nProcessed document 700 of 936\r\nProcessed document 710 of 936\r\nProcessed document 720 of 936\r\nProcessed document 730 of 936\r\nProcessed document 740 of 936\r\nProcessed document 750 of 936\r\nProcessed document 760 of 936\r\nProcessed document 770 of 936\r\nProcessed document 780 of 936\r\nProcessed document 790 of 936\r\nProcessed document 800 of 936\r\nProcessed document 810 of 936\r\nProcessed document 820 of 936\r\nProcessed document 830 of 936\r\nProcessed document 840 of 936\r\nProcessed document 850 of 936\r\nProcessed document 860 of 936\r\nProcessed document 870 of 936\r\nProcessed document 880 of 936\r\nProcessed document 890 of 936\r\nProcessed document 900 of 936\r\nProcessed document 910 of 936\r\nProcessed document 920 of 936\r\nProcessed document 930 of 936\r\n\r\nhead(annotated_main)\r\n\r\n\r\n$token\r\n# A tibble: 11,019 x 11\r\n   doc_id   sid tid   token    token_with_ws lemma   upos  xpos  feats\r\n *  <int> <int> <chr> <chr>    <chr>         <chr>   <chr> <chr> <chr>\r\n 1      1     1 1     $        \"$\"           $       SYM   $     <NA> \r\n 2      1     1 2     174      \"174 \"        174     NUM   CD    NumT~\r\n 3      1     1 3     Million  \"Million \"    million NUM   CD    NumT~\r\n 4      1     1 4     Afghan   \"Afghan \"     afghan  ADJ   JJ    Degr~\r\n 5      1     1 5     Drone    \"Drone \"      Drone   PROPN NNP   Numb~\r\n 6      1     1 6     Program  \"Program \"    program PROPN NNP   Numb~\r\n 7      1     1 7     Is       \"Is \"         be      AUX   VBZ   Mood~\r\n 8      1     1 8     Riddled  \"Riddled \"    riddle  VERB  VBN   Tens~\r\n 9      1     1 9     With     \"With \"       with    ADP   IN    <NA> \r\n10      1     1 10    Problems \"Problems\"    proble~ PROPN NNPS  Numb~\r\n# ... with 11,009 more rows, and 2 more variables: tid_source <chr>,\r\n#   relation <chr>\r\n\r\n$document\r\n    doc_id\r\n1        1\r\n2        2\r\n3        3\r\n4        4\r\n5        5\r\n6        6\r\n7        7\r\n8        8\r\n9        9\r\n10      10\r\n11      11\r\n12      12\r\n13      13\r\n14      14\r\n15      15\r\n16      16\r\n17      17\r\n18      18\r\n19      19\r\n20      20\r\n21      21\r\n22      22\r\n23      23\r\n24      24\r\n25      25\r\n26      26\r\n27      27\r\n28      28\r\n29      29\r\n30      30\r\n31      31\r\n32      32\r\n33      33\r\n34      34\r\n35      35\r\n36      36\r\n37      37\r\n38      38\r\n39      39\r\n40      40\r\n41      41\r\n42      42\r\n43      43\r\n44      44\r\n45      45\r\n46      46\r\n47      47\r\n48      48\r\n49      49\r\n50      50\r\n51      51\r\n52      52\r\n53      53\r\n54      54\r\n55      55\r\n56      56\r\n57      57\r\n58      58\r\n59      59\r\n60      60\r\n61      61\r\n62      62\r\n63      63\r\n64      64\r\n65      65\r\n66      66\r\n67      67\r\n68      68\r\n69      69\r\n70      70\r\n71      71\r\n72      72\r\n73      73\r\n74      74\r\n75      75\r\n76      76\r\n77      77\r\n78      78\r\n79      79\r\n80      80\r\n81      81\r\n82      82\r\n83      83\r\n84      84\r\n85      85\r\n86      86\r\n87      87\r\n88      88\r\n89      89\r\n90      90\r\n91      91\r\n92      92\r\n93      93\r\n94      94\r\n95      95\r\n96      96\r\n97      97\r\n98      98\r\n99      99\r\n100    100\r\n101    101\r\n102    102\r\n103    103\r\n104    104\r\n105    105\r\n106    106\r\n107    107\r\n108    108\r\n109    109\r\n110    110\r\n111    111\r\n112    112\r\n113    113\r\n114    114\r\n115    115\r\n116    116\r\n117    117\r\n118    118\r\n119    119\r\n120    120\r\n121    121\r\n122    122\r\n123    123\r\n124    124\r\n125    125\r\n126    126\r\n127    127\r\n128    128\r\n129    129\r\n130    130\r\n131    131\r\n132    132\r\n133    133\r\n134    134\r\n135    135\r\n136    136\r\n137    137\r\n138    138\r\n139    139\r\n140    140\r\n141    141\r\n142    142\r\n143    143\r\n144    144\r\n145    145\r\n146    146\r\n147    147\r\n148    148\r\n149    149\r\n150    150\r\n151    151\r\n152    152\r\n153    153\r\n154    154\r\n155    155\r\n156    156\r\n157    157\r\n158    158\r\n159    159\r\n160    160\r\n161    161\r\n162    162\r\n163    163\r\n164    164\r\n165    165\r\n166    166\r\n167    167\r\n168    168\r\n169    169\r\n170    170\r\n171    171\r\n172    172\r\n173    173\r\n174    174\r\n175    175\r\n176    176\r\n177    177\r\n178    178\r\n179    179\r\n180    180\r\n181    181\r\n182    182\r\n183    183\r\n184    184\r\n185    185\r\n186    186\r\n187    187\r\n188    188\r\n189    189\r\n190    190\r\n191    191\r\n192    192\r\n193    193\r\n194    194\r\n195    195\r\n196    196\r\n197    197\r\n198    198\r\n199    199\r\n200    200\r\n201    201\r\n202    202\r\n203    203\r\n204    204\r\n205    205\r\n206    206\r\n207    207\r\n208    208\r\n209    209\r\n210    210\r\n211    211\r\n212    212\r\n213    213\r\n214    214\r\n215    215\r\n216    216\r\n217    217\r\n218    218\r\n219    219\r\n220    220\r\n221    221\r\n222    222\r\n223    223\r\n224    224\r\n225    225\r\n226    226\r\n227    227\r\n228    228\r\n229    229\r\n230    230\r\n231    231\r\n232    232\r\n233    233\r\n234    234\r\n235    235\r\n236    236\r\n237    237\r\n238    238\r\n239    239\r\n240    240\r\n241    241\r\n242    242\r\n243    243\r\n244    244\r\n245    245\r\n246    246\r\n247    247\r\n248    248\r\n249    249\r\n250    250\r\n251    251\r\n252    252\r\n253    253\r\n254    254\r\n255    255\r\n256    256\r\n257    257\r\n258    258\r\n259    259\r\n260    260\r\n261    261\r\n262    262\r\n263    263\r\n264    264\r\n265    265\r\n266    266\r\n267    267\r\n268    268\r\n269    269\r\n270    270\r\n271    271\r\n272    272\r\n273    273\r\n274    274\r\n275    275\r\n276    276\r\n277    277\r\n278    278\r\n279    279\r\n280    280\r\n281    281\r\n282    282\r\n283    283\r\n284    284\r\n285    285\r\n286    286\r\n287    287\r\n288    288\r\n289    289\r\n290    290\r\n291    291\r\n292    292\r\n293    293\r\n294    294\r\n295    295\r\n296    296\r\n297    297\r\n298    298\r\n299    299\r\n300    300\r\n301    301\r\n302    302\r\n303    303\r\n304    304\r\n305    305\r\n306    306\r\n307    307\r\n308    308\r\n309    309\r\n310    310\r\n311    311\r\n312    312\r\n313    313\r\n314    314\r\n315    315\r\n316    316\r\n317    317\r\n318    318\r\n319    319\r\n320    320\r\n321    321\r\n322    322\r\n323    323\r\n324    324\r\n325    325\r\n326    326\r\n327    327\r\n328    328\r\n329    329\r\n330    330\r\n331    331\r\n332    332\r\n333    333\r\n334    334\r\n335    335\r\n336    336\r\n337    337\r\n338    338\r\n339    339\r\n340    340\r\n341    341\r\n342    342\r\n343    343\r\n344    344\r\n345    345\r\n346    346\r\n347    347\r\n348    348\r\n349    349\r\n350    350\r\n351    351\r\n352    352\r\n353    353\r\n354    354\r\n355    355\r\n356    356\r\n357    357\r\n358    358\r\n359    359\r\n360    360\r\n361    361\r\n362    362\r\n363    363\r\n364    364\r\n365    365\r\n366    366\r\n367    367\r\n368    368\r\n369    369\r\n370    370\r\n371    371\r\n372    372\r\n373    373\r\n374    374\r\n375    375\r\n376    376\r\n377    377\r\n378    378\r\n379    379\r\n380    380\r\n381    381\r\n382    382\r\n383    383\r\n384    384\r\n385    385\r\n386    386\r\n387    387\r\n388    388\r\n389    389\r\n390    390\r\n391    391\r\n392    392\r\n393    393\r\n394    394\r\n395    395\r\n396    396\r\n397    397\r\n398    398\r\n399    399\r\n400    400\r\n401    401\r\n402    402\r\n403    403\r\n404    404\r\n405    405\r\n406    406\r\n407    407\r\n408    408\r\n409    409\r\n410    410\r\n411    411\r\n412    412\r\n413    413\r\n414    414\r\n415    415\r\n416    416\r\n417    417\r\n418    418\r\n419    419\r\n420    420\r\n421    421\r\n422    422\r\n423    423\r\n424    424\r\n425    425\r\n426    426\r\n427    427\r\n428    428\r\n429    429\r\n430    430\r\n431    431\r\n432    432\r\n433    433\r\n434    434\r\n435    435\r\n436    436\r\n437    437\r\n438    438\r\n439    439\r\n440    440\r\n441    441\r\n442    442\r\n443    443\r\n444    444\r\n445    445\r\n446    446\r\n447    447\r\n448    448\r\n449    449\r\n450    450\r\n451    451\r\n452    452\r\n453    453\r\n454    454\r\n455    455\r\n456    456\r\n457    457\r\n458    458\r\n459    459\r\n460    460\r\n461    461\r\n462    462\r\n463    463\r\n464    464\r\n465    465\r\n466    466\r\n467    467\r\n468    468\r\n469    469\r\n470    470\r\n471    471\r\n472    472\r\n473    473\r\n474    474\r\n475    475\r\n476    476\r\n477    477\r\n478    478\r\n479    479\r\n480    480\r\n481    481\r\n482    482\r\n483    483\r\n484    484\r\n485    485\r\n486    486\r\n487    487\r\n488    488\r\n489    489\r\n490    490\r\n491    491\r\n492    492\r\n493    493\r\n494    494\r\n495    495\r\n496    496\r\n497    497\r\n498    498\r\n499    499\r\n500    500\r\n501    501\r\n502    502\r\n503    503\r\n504    504\r\n505    505\r\n506    506\r\n507    507\r\n508    508\r\n509    509\r\n510    510\r\n511    511\r\n512    512\r\n513    513\r\n514    514\r\n515    515\r\n516    516\r\n517    517\r\n518    518\r\n519    519\r\n520    520\r\n521    521\r\n522    522\r\n523    523\r\n524    524\r\n525    525\r\n526    526\r\n527    527\r\n528    528\r\n529    529\r\n530    530\r\n531    531\r\n532    532\r\n533    533\r\n534    534\r\n535    535\r\n536    536\r\n537    537\r\n538    538\r\n539    539\r\n540    540\r\n541    541\r\n542    542\r\n543    543\r\n544    544\r\n545    545\r\n546    546\r\n547    547\r\n548    548\r\n549    549\r\n550    550\r\n551    551\r\n552    552\r\n553    553\r\n554    554\r\n555    555\r\n556    556\r\n557    557\r\n558    558\r\n559    559\r\n560    560\r\n561    561\r\n562    562\r\n563    563\r\n564    564\r\n565    565\r\n566    566\r\n567    567\r\n568    568\r\n569    569\r\n570    570\r\n571    571\r\n572    572\r\n573    573\r\n574    574\r\n575    575\r\n576    576\r\n577    577\r\n578    578\r\n579    579\r\n580    580\r\n581    581\r\n582    582\r\n583    583\r\n584    584\r\n585    585\r\n586    586\r\n587    587\r\n588    588\r\n589    589\r\n590    590\r\n591    591\r\n592    592\r\n593    593\r\n594    594\r\n595    595\r\n596    596\r\n597    597\r\n598    598\r\n599    599\r\n600    600\r\n601    601\r\n602    602\r\n603    603\r\n604    604\r\n605    605\r\n606    606\r\n607    607\r\n608    608\r\n609    609\r\n610    610\r\n611    611\r\n612    612\r\n613    613\r\n614    614\r\n615    615\r\n616    616\r\n617    617\r\n618    618\r\n619    619\r\n620    620\r\n621    621\r\n622    622\r\n623    623\r\n624    624\r\n625    625\r\n626    626\r\n627    627\r\n628    628\r\n629    629\r\n630    630\r\n631    631\r\n632    632\r\n633    633\r\n634    634\r\n635    635\r\n636    636\r\n637    637\r\n638    638\r\n639    639\r\n640    640\r\n641    641\r\n642    642\r\n643    643\r\n644    644\r\n645    645\r\n646    646\r\n647    647\r\n648    648\r\n649    649\r\n650    650\r\n651    651\r\n652    652\r\n653    653\r\n654    654\r\n655    655\r\n656    656\r\n657    657\r\n658    658\r\n659    659\r\n660    660\r\n661    661\r\n662    662\r\n663    663\r\n664    664\r\n665    665\r\n666    666\r\n667    667\r\n668    668\r\n669    669\r\n670    670\r\n671    671\r\n672    672\r\n673    673\r\n674    674\r\n675    675\r\n676    676\r\n677    677\r\n678    678\r\n679    679\r\n680    680\r\n681    681\r\n682    682\r\n683    683\r\n684    684\r\n685    685\r\n686    686\r\n687    687\r\n688    688\r\n689    689\r\n690    690\r\n691    691\r\n692    692\r\n693    693\r\n694    694\r\n695    695\r\n696    696\r\n697    697\r\n698    698\r\n699    699\r\n700    700\r\n701    701\r\n702    702\r\n703    703\r\n704    704\r\n705    705\r\n706    706\r\n707    707\r\n708    708\r\n709    709\r\n710    710\r\n711    711\r\n712    712\r\n713    713\r\n714    714\r\n715    715\r\n716    716\r\n717    717\r\n718    718\r\n719    719\r\n720    720\r\n721    721\r\n722    722\r\n723    723\r\n724    724\r\n725    725\r\n726    726\r\n727    727\r\n728    728\r\n729    729\r\n730    730\r\n731    731\r\n732    732\r\n733    733\r\n734    734\r\n735    735\r\n736    736\r\n737    737\r\n738    738\r\n739    739\r\n740    740\r\n741    741\r\n742    742\r\n743    743\r\n744    744\r\n745    745\r\n746    746\r\n747    747\r\n748    748\r\n749    749\r\n750    750\r\n751    751\r\n752    752\r\n753    753\r\n754    754\r\n755    755\r\n756    756\r\n757    757\r\n758    758\r\n759    759\r\n760    760\r\n761    761\r\n762    762\r\n763    763\r\n764    764\r\n765    765\r\n766    766\r\n767    767\r\n768    768\r\n769    769\r\n770    770\r\n771    771\r\n772    772\r\n773    773\r\n774    774\r\n775    775\r\n776    776\r\n777    777\r\n778    778\r\n779    779\r\n780    780\r\n781    781\r\n782    782\r\n783    783\r\n784    784\r\n785    785\r\n786    786\r\n787    787\r\n788    788\r\n789    789\r\n790    790\r\n791    791\r\n792    792\r\n793    793\r\n794    794\r\n795    795\r\n796    796\r\n797    797\r\n798    798\r\n799    799\r\n800    800\r\n801    801\r\n802    802\r\n803    803\r\n804    804\r\n805    805\r\n806    806\r\n807    807\r\n808    808\r\n809    809\r\n810    810\r\n811    811\r\n812    812\r\n813    813\r\n814    814\r\n815    815\r\n816    816\r\n817    817\r\n818    818\r\n819    819\r\n820    820\r\n821    821\r\n822    822\r\n823    823\r\n824    824\r\n825    825\r\n826    826\r\n827    827\r\n828    828\r\n829    829\r\n830    830\r\n831    831\r\n832    832\r\n833    833\r\n834    834\r\n835    835\r\n836    836\r\n837    837\r\n838    838\r\n839    839\r\n840    840\r\n841    841\r\n842    842\r\n843    843\r\n844    844\r\n845    845\r\n846    846\r\n847    847\r\n848    848\r\n849    849\r\n850    850\r\n851    851\r\n852    852\r\n853    853\r\n854    854\r\n855    855\r\n856    856\r\n857    857\r\n858    858\r\n859    859\r\n860    860\r\n861    861\r\n862    862\r\n863    863\r\n864    864\r\n865    865\r\n866    866\r\n867    867\r\n868    868\r\n869    869\r\n870    870\r\n871    871\r\n872    872\r\n873    873\r\n874    874\r\n875    875\r\n876    876\r\n877    877\r\n878    878\r\n879    879\r\n880    880\r\n881    881\r\n882    882\r\n883    883\r\n884    884\r\n885    885\r\n886    886\r\n887    887\r\n888    888\r\n889    889\r\n890    890\r\n891    891\r\n892    892\r\n893    893\r\n894    894\r\n895    895\r\n896    896\r\n897    897\r\n898    898\r\n899    899\r\n900    900\r\n901    901\r\n902    902\r\n903    903\r\n904    904\r\n905    905\r\n906    906\r\n907    907\r\n908    908\r\n909    909\r\n910    910\r\n911    911\r\n912    912\r\n913    913\r\n914    914\r\n915    915\r\n916    916\r\n917    917\r\n918    918\r\n919    919\r\n920    920\r\n921    921\r\n922    922\r\n923    923\r\n924    924\r\n925    925\r\n926    926\r\n927    927\r\n928    928\r\n929    929\r\n930    930\r\n931    931\r\n932    932\r\n933    933\r\n934    934\r\n935    935\r\n936    936\r\n\r\n#then the print headlines\r\n\r\nprint_tibble <- as_tibble(print_headlines) %>%\r\n  select(c(\"doc_id\", \"text\"))\r\nprint_tibble <- utf8::as_utf8(print_tibble$text)\r\n\r\nannotated_print <- cnlp_annotate(print_tibble)\r\n\r\n\r\nProcessed document 10 of 936\r\nProcessed document 20 of 936\r\nProcessed document 30 of 936\r\nProcessed document 40 of 936\r\nProcessed document 50 of 936\r\nProcessed document 60 of 936\r\nProcessed document 70 of 936\r\nProcessed document 80 of 936\r\nProcessed document 90 of 936\r\nProcessed document 100 of 936\r\nProcessed document 110 of 936\r\nProcessed document 120 of 936\r\nProcessed document 130 of 936\r\nProcessed document 140 of 936\r\nProcessed document 150 of 936\r\nProcessed document 160 of 936\r\nProcessed document 170 of 936\r\nProcessed document 180 of 936\r\nProcessed document 190 of 936\r\nProcessed document 200 of 936\r\nProcessed document 210 of 936\r\nProcessed document 220 of 936\r\nProcessed document 230 of 936\r\nProcessed document 240 of 936\r\nProcessed document 250 of 936\r\nProcessed document 260 of 936\r\nProcessed document 270 of 936\r\nProcessed document 280 of 936\r\nProcessed document 290 of 936\r\nProcessed document 300 of 936\r\nProcessed document 310 of 936\r\nProcessed document 320 of 936\r\nProcessed document 330 of 936\r\nProcessed document 340 of 936\r\nProcessed document 350 of 936\r\nProcessed document 360 of 936\r\nProcessed document 370 of 936\r\nProcessed document 380 of 936\r\nProcessed document 390 of 936\r\nProcessed document 400 of 936\r\nProcessed document 410 of 936\r\nProcessed document 420 of 936\r\nProcessed document 430 of 936\r\nProcessed document 440 of 936\r\nProcessed document 450 of 936\r\nProcessed document 460 of 936\r\nProcessed document 470 of 936\r\nProcessed document 480 of 936\r\nProcessed document 490 of 936\r\nProcessed document 500 of 936\r\nProcessed document 510 of 936\r\nProcessed document 520 of 936\r\nProcessed document 530 of 936\r\nProcessed document 540 of 936\r\nProcessed document 550 of 936\r\nProcessed document 560 of 936\r\nProcessed document 570 of 936\r\nProcessed document 580 of 936\r\nProcessed document 590 of 936\r\nProcessed document 600 of 936\r\nProcessed document 610 of 936\r\nProcessed document 620 of 936\r\nProcessed document 630 of 936\r\nProcessed document 640 of 936\r\nProcessed document 650 of 936\r\nProcessed document 660 of 936\r\nProcessed document 670 of 936\r\nProcessed document 680 of 936\r\nProcessed document 690 of 936\r\nProcessed document 700 of 936\r\nProcessed document 710 of 936\r\nProcessed document 720 of 936\r\nProcessed document 730 of 936\r\nProcessed document 740 of 936\r\nProcessed document 750 of 936\r\nProcessed document 760 of 936\r\nProcessed document 770 of 936\r\nProcessed document 780 of 936\r\nProcessed document 790 of 936\r\nProcessed document 800 of 936\r\nProcessed document 810 of 936\r\nProcessed document 820 of 936\r\nProcessed document 830 of 936\r\nProcessed document 840 of 936\r\nProcessed document 850 of 936\r\nProcessed document 860 of 936\r\nProcessed document 870 of 936\r\nProcessed document 880 of 936\r\nProcessed document 890 of 936\r\nProcessed document 900 of 936\r\nProcessed document 910 of 936\r\nProcessed document 920 of 936\r\nProcessed document 930 of 936\r\n\r\nhead(annotated_print)\r\n\r\n\r\n$token\r\n# A tibble: 9,625 x 11\r\n   doc_id   sid tid   token   token_with_ws lemma   upos  xpos  feats \r\n *  <int> <int> <chr> <chr>   <chr>         <chr>   <chr> <chr> <chr> \r\n 1      1     1 1     $       \"$\"           $       SYM   $     <NA>  \r\n 2      1     1 2     174     \"174 \"        174     NUM   CD    NumTy~\r\n 3      1     1 3     Million \"Million \"    million PROPN NNP   Numbe~\r\n 4      1     1 4     Drone   \"Drone \"      Drone   PROPN NNP   Numbe~\r\n 5      1     1 5     Program \"Program \"    Program PROPN NNP   Numbe~\r\n 6      1     1 6     for     \"for \"        for     ADP   IN    <NA>  \r\n 7      1     1 7     Afghans \"Afghans \"    Afghans PROPN NNPS  Numbe~\r\n 8      1     1 8     Is      \"Is \"         be      AUX   VBZ   Mood=~\r\n 9      1     1 9     Riddled \"Riddled \"    riddle  VERB  VBN   Tense~\r\n10      1     1 10    With    \"With \"       with    ADP   IN    <NA>  \r\n# ... with 9,615 more rows, and 2 more variables: tid_source <chr>,\r\n#   relation <chr>\r\n\r\n$document\r\n    doc_id\r\n1        1\r\n2        2\r\n3        3\r\n4        4\r\n5        5\r\n6        6\r\n7        7\r\n8        8\r\n9        9\r\n10      10\r\n11      11\r\n12      12\r\n13      13\r\n14      14\r\n15      15\r\n16      16\r\n17      17\r\n18      18\r\n19      19\r\n20      20\r\n21      21\r\n22      22\r\n23      23\r\n24      24\r\n25      25\r\n26      26\r\n27      27\r\n28      28\r\n29      29\r\n30      30\r\n31      31\r\n32      32\r\n33      33\r\n34      34\r\n35      35\r\n36      36\r\n37      37\r\n38      38\r\n39      39\r\n40      40\r\n41      41\r\n42      42\r\n43      43\r\n44      44\r\n45      45\r\n46      46\r\n47      47\r\n48      48\r\n49      49\r\n50      50\r\n51      51\r\n52      52\r\n53      53\r\n54      54\r\n55      55\r\n56      56\r\n57      57\r\n58      58\r\n59      59\r\n60      60\r\n61      61\r\n62      62\r\n63      63\r\n64      64\r\n65      65\r\n66      66\r\n67      67\r\n68      68\r\n69      69\r\n70      70\r\n71      71\r\n72      72\r\n73      73\r\n74      74\r\n75      75\r\n76      76\r\n77      77\r\n78      78\r\n79      79\r\n80      80\r\n81      81\r\n82      82\r\n83      83\r\n84      84\r\n85      85\r\n86      86\r\n87      87\r\n88      88\r\n89      89\r\n90      90\r\n91      91\r\n92      92\r\n93      93\r\n94      94\r\n95      95\r\n96      96\r\n97      97\r\n98      98\r\n99      99\r\n100    100\r\n101    101\r\n102    102\r\n103    103\r\n104    104\r\n105    105\r\n106    106\r\n107    107\r\n108    108\r\n109    109\r\n110    110\r\n111    111\r\n112    112\r\n113    113\r\n114    114\r\n115    115\r\n116    116\r\n117    117\r\n118    118\r\n119    119\r\n120    120\r\n121    121\r\n122    122\r\n123    123\r\n124    124\r\n125    125\r\n126    126\r\n127    127\r\n128    128\r\n129    129\r\n130    130\r\n131    131\r\n132    132\r\n133    133\r\n134    134\r\n135    135\r\n136    136\r\n137    137\r\n138    138\r\n139    139\r\n140    140\r\n141    141\r\n142    142\r\n143    143\r\n144    144\r\n145    145\r\n146    146\r\n147    147\r\n148    148\r\n149    149\r\n150    150\r\n151    151\r\n152    152\r\n153    153\r\n154    154\r\n155    155\r\n156    156\r\n157    157\r\n158    158\r\n159    159\r\n160    160\r\n161    161\r\n162    162\r\n163    163\r\n164    164\r\n165    165\r\n166    166\r\n167    167\r\n168    168\r\n169    169\r\n170    170\r\n171    171\r\n172    172\r\n173    173\r\n174    174\r\n175    175\r\n176    176\r\n177    177\r\n178    178\r\n179    179\r\n180    180\r\n181    181\r\n182    182\r\n183    183\r\n184    184\r\n185    185\r\n186    186\r\n187    187\r\n188    188\r\n189    189\r\n190    190\r\n191    191\r\n192    192\r\n193    193\r\n194    194\r\n195    195\r\n196    196\r\n197    197\r\n198    198\r\n199    199\r\n200    200\r\n201    201\r\n202    202\r\n203    203\r\n204    204\r\n205    205\r\n206    206\r\n207    207\r\n208    208\r\n209    209\r\n210    210\r\n211    211\r\n212    212\r\n213    213\r\n214    214\r\n215    215\r\n216    216\r\n217    217\r\n218    218\r\n219    219\r\n220    220\r\n221    221\r\n222    222\r\n223    223\r\n224    224\r\n225    225\r\n226    226\r\n227    227\r\n228    228\r\n229    229\r\n230    230\r\n231    231\r\n232    232\r\n233    233\r\n234    234\r\n235    235\r\n236    236\r\n237    237\r\n238    238\r\n239    239\r\n240    240\r\n241    241\r\n242    242\r\n243    243\r\n244    244\r\n245    245\r\n246    246\r\n247    247\r\n248    248\r\n249    249\r\n250    250\r\n251    251\r\n252    252\r\n253    253\r\n254    254\r\n255    255\r\n256    256\r\n257    257\r\n258    258\r\n259    259\r\n260    260\r\n261    261\r\n262    262\r\n263    263\r\n264    264\r\n265    265\r\n266    266\r\n267    267\r\n268    268\r\n269    269\r\n270    270\r\n271    271\r\n272    272\r\n273    273\r\n274    274\r\n275    275\r\n276    276\r\n277    277\r\n278    278\r\n279    279\r\n280    280\r\n281    281\r\n282    282\r\n283    283\r\n284    284\r\n285    285\r\n286    286\r\n287    287\r\n288    288\r\n289    289\r\n290    290\r\n291    291\r\n292    292\r\n293    293\r\n294    294\r\n295    295\r\n296    296\r\n297    297\r\n298    298\r\n299    299\r\n300    300\r\n301    301\r\n302    302\r\n303    303\r\n304    304\r\n305    305\r\n306    306\r\n307    307\r\n308    308\r\n309    309\r\n310    310\r\n311    311\r\n312    312\r\n313    313\r\n314    314\r\n315    315\r\n316    316\r\n317    317\r\n318    318\r\n319    319\r\n320    320\r\n321    321\r\n322    322\r\n323    323\r\n324    324\r\n325    325\r\n326    326\r\n327    327\r\n328    328\r\n329    329\r\n330    330\r\n331    331\r\n332    332\r\n333    333\r\n334    334\r\n335    335\r\n336    336\r\n337    337\r\n338    338\r\n339    339\r\n340    340\r\n341    341\r\n342    342\r\n343    343\r\n344    344\r\n345    345\r\n346    346\r\n347    347\r\n348    348\r\n349    349\r\n350    350\r\n351    351\r\n352    352\r\n353    353\r\n354    354\r\n355    355\r\n356    356\r\n357    357\r\n358    358\r\n359    359\r\n360    360\r\n361    361\r\n362    362\r\n363    363\r\n364    364\r\n365    365\r\n366    366\r\n367    367\r\n368    368\r\n369    369\r\n370    370\r\n371    371\r\n372    372\r\n373    373\r\n374    374\r\n375    375\r\n376    376\r\n377    377\r\n378    378\r\n379    379\r\n380    380\r\n381    381\r\n382    382\r\n383    383\r\n384    384\r\n385    385\r\n386    386\r\n387    387\r\n388    388\r\n389    389\r\n390    390\r\n391    391\r\n392    392\r\n393    393\r\n394    394\r\n395    395\r\n396    396\r\n397    397\r\n398    398\r\n399    399\r\n400    400\r\n401    401\r\n402    402\r\n403    403\r\n404    404\r\n405    405\r\n406    406\r\n407    407\r\n408    408\r\n409    409\r\n410    410\r\n411    411\r\n412    412\r\n413    413\r\n414    414\r\n415    415\r\n416    416\r\n417    417\r\n418    418\r\n419    419\r\n420    420\r\n421    421\r\n422    422\r\n423    423\r\n424    424\r\n425    425\r\n426    426\r\n427    427\r\n428    428\r\n429    429\r\n430    430\r\n431    431\r\n432    432\r\n433    433\r\n434    434\r\n435    435\r\n436    436\r\n437    437\r\n438    438\r\n439    439\r\n440    440\r\n441    441\r\n442    442\r\n443    443\r\n444    444\r\n445    445\r\n446    446\r\n447    447\r\n448    448\r\n449    449\r\n450    450\r\n451    451\r\n452    452\r\n453    453\r\n454    454\r\n455    455\r\n456    456\r\n457    457\r\n458    458\r\n459    459\r\n460    460\r\n461    461\r\n462    462\r\n463    463\r\n464    464\r\n465    465\r\n466    466\r\n467    467\r\n468    468\r\n469    469\r\n470    470\r\n471    471\r\n472    472\r\n473    473\r\n474    474\r\n475    475\r\n476    476\r\n477    477\r\n478    478\r\n479    479\r\n480    480\r\n481    481\r\n482    482\r\n483    483\r\n484    484\r\n485    485\r\n486    486\r\n487    487\r\n488    488\r\n489    489\r\n490    490\r\n491    491\r\n492    492\r\n493    493\r\n494    494\r\n495    495\r\n496    496\r\n497    497\r\n498    498\r\n499    499\r\n500    500\r\n501    501\r\n502    502\r\n503    503\r\n504    504\r\n505    505\r\n506    506\r\n507    507\r\n508    508\r\n509    509\r\n510    510\r\n511    511\r\n512    512\r\n513    513\r\n514    514\r\n515    515\r\n516    516\r\n517    517\r\n518    518\r\n519    519\r\n520    520\r\n521    521\r\n522    522\r\n523    523\r\n524    524\r\n525    525\r\n526    526\r\n527    527\r\n528    528\r\n529    529\r\n530    530\r\n531    531\r\n532    532\r\n533    533\r\n534    534\r\n535    535\r\n536    536\r\n537    537\r\n538    538\r\n539    539\r\n540    540\r\n541    541\r\n542    542\r\n543    543\r\n544    544\r\n545    545\r\n546    546\r\n547    547\r\n548    548\r\n549    549\r\n550    550\r\n551    551\r\n552    552\r\n553    553\r\n554    554\r\n555    555\r\n556    556\r\n557    557\r\n558    558\r\n559    559\r\n560    560\r\n561    561\r\n562    562\r\n563    563\r\n564    564\r\n565    565\r\n566    566\r\n567    567\r\n568    568\r\n569    569\r\n570    570\r\n571    571\r\n572    572\r\n573    573\r\n574    574\r\n575    575\r\n576    576\r\n577    577\r\n578    578\r\n579    579\r\n580    580\r\n581    581\r\n582    582\r\n583    583\r\n584    584\r\n585    585\r\n586    586\r\n587    587\r\n588    588\r\n589    589\r\n590    590\r\n591    591\r\n592    592\r\n593    593\r\n594    594\r\n595    595\r\n596    596\r\n597    597\r\n598    598\r\n599    599\r\n600    600\r\n601    601\r\n602    602\r\n603    603\r\n604    604\r\n605    605\r\n606    606\r\n607    607\r\n608    608\r\n609    609\r\n610    610\r\n611    611\r\n612    612\r\n613    613\r\n614    614\r\n615    615\r\n616    616\r\n617    617\r\n618    618\r\n619    619\r\n620    620\r\n621    621\r\n622    622\r\n623    623\r\n624    624\r\n625    625\r\n626    626\r\n627    627\r\n628    628\r\n629    629\r\n630    630\r\n631    631\r\n632    632\r\n633    633\r\n634    634\r\n635    635\r\n636    636\r\n637    637\r\n638    638\r\n639    639\r\n640    640\r\n641    641\r\n642    642\r\n643    643\r\n644    644\r\n645    645\r\n646    646\r\n647    647\r\n648    648\r\n649    649\r\n650    650\r\n651    651\r\n652    652\r\n653    653\r\n654    654\r\n655    655\r\n656    656\r\n657    657\r\n658    658\r\n659    659\r\n660    660\r\n661    661\r\n662    662\r\n663    663\r\n664    664\r\n665    665\r\n666    666\r\n667    667\r\n668    668\r\n669    669\r\n670    670\r\n671    671\r\n672    672\r\n673    673\r\n674    674\r\n675    675\r\n676    676\r\n677    677\r\n678    678\r\n679    679\r\n680    680\r\n681    681\r\n682    682\r\n683    683\r\n684    684\r\n685    685\r\n686    686\r\n687    687\r\n688    688\r\n689    689\r\n690    690\r\n691    691\r\n692    692\r\n693    693\r\n694    694\r\n695    695\r\n696    696\r\n697    697\r\n698    698\r\n699    699\r\n700    700\r\n701    701\r\n702    702\r\n703    703\r\n704    704\r\n705    705\r\n706    706\r\n707    707\r\n708    708\r\n709    709\r\n710    710\r\n711    711\r\n712    712\r\n713    713\r\n714    714\r\n715    715\r\n716    716\r\n717    717\r\n718    718\r\n719    719\r\n720    720\r\n721    721\r\n722    722\r\n723    723\r\n724    724\r\n725    725\r\n726    726\r\n727    727\r\n728    728\r\n729    729\r\n730    730\r\n731    731\r\n732    732\r\n733    733\r\n734    734\r\n735    735\r\n736    736\r\n737    737\r\n738    738\r\n739    739\r\n740    740\r\n741    741\r\n742    742\r\n743    743\r\n744    744\r\n745    745\r\n746    746\r\n747    747\r\n748    748\r\n749    749\r\n750    750\r\n751    751\r\n752    752\r\n753    753\r\n754    754\r\n755    755\r\n756    756\r\n757    757\r\n758    758\r\n759    759\r\n760    760\r\n761    761\r\n762    762\r\n763    763\r\n764    764\r\n765    765\r\n766    766\r\n767    767\r\n768    768\r\n769    769\r\n770    770\r\n771    771\r\n772    772\r\n773    773\r\n774    774\r\n775    775\r\n776    776\r\n777    777\r\n778    778\r\n779    779\r\n780    780\r\n781    781\r\n782    782\r\n783    783\r\n784    784\r\n785    785\r\n786    786\r\n787    787\r\n788    788\r\n789    789\r\n790    790\r\n791    791\r\n792    792\r\n793    793\r\n794    794\r\n795    795\r\n796    796\r\n797    797\r\n798    798\r\n799    799\r\n800    800\r\n801    801\r\n802    802\r\n803    803\r\n804    804\r\n805    805\r\n806    806\r\n807    807\r\n808    808\r\n809    809\r\n810    810\r\n811    811\r\n812    812\r\n813    813\r\n814    814\r\n815    815\r\n816    816\r\n817    817\r\n818    818\r\n819    819\r\n820    820\r\n821    821\r\n822    822\r\n823    823\r\n824    824\r\n825    825\r\n826    826\r\n827    827\r\n828    828\r\n829    829\r\n830    830\r\n831    831\r\n832    832\r\n833    833\r\n834    834\r\n835    835\r\n836    836\r\n837    837\r\n838    838\r\n839    839\r\n840    840\r\n841    841\r\n842    842\r\n843    843\r\n844    844\r\n845    845\r\n846    846\r\n847    847\r\n848    848\r\n849    849\r\n850    850\r\n851    851\r\n852    852\r\n853    853\r\n854    854\r\n855    855\r\n856    856\r\n857    857\r\n858    858\r\n859    859\r\n860    860\r\n861    861\r\n862    862\r\n863    863\r\n864    864\r\n865    865\r\n866    866\r\n867    867\r\n868    868\r\n869    869\r\n870    870\r\n871    871\r\n872    872\r\n873    873\r\n874    874\r\n875    875\r\n876    876\r\n877    877\r\n878    878\r\n879    879\r\n880    880\r\n881    881\r\n882    882\r\n883    883\r\n884    884\r\n885    885\r\n886    886\r\n887    887\r\n888    888\r\n889    889\r\n890    890\r\n891    891\r\n892    892\r\n893    893\r\n894    894\r\n895    895\r\n896    896\r\n897    897\r\n898    898\r\n899    899\r\n900    900\r\n901    901\r\n902    902\r\n903    903\r\n904    904\r\n905    905\r\n906    906\r\n907    907\r\n908    908\r\n909    909\r\n910    910\r\n911    911\r\n912    912\r\n913    913\r\n914    914\r\n915    915\r\n916    916\r\n917    917\r\n918    918\r\n919    919\r\n920    920\r\n921    921\r\n922    922\r\n923    923\r\n924    924\r\n925    925\r\n926    926\r\n927    927\r\n928    928\r\n929    929\r\n930    930\r\n931    931\r\n932    932\r\n933    933\r\n934    934\r\n935    935\r\n936    936\r\n\r\nExploratory Analysis\r\n\r\n\r\nparsed_main <- spacy_parse(main_headlines,\r\n                           lemma = TRUE,\r\n                           entity = TRUE,\r\n                           nounphrase = TRUE,\r\n                           additional_attributes = c(\"is_punct\",\r\n                                                     \"is_stop\")) %>%\r\n  as_tibble %>%\r\n  select(-sentence_id) %>%\r\n  rename(sentence_nr = doc_id) \r\n\r\nhead(parsed_main)\r\n\r\n\r\n# A tibble: 6 x 10\r\n  sentence_nr token_id token  lemma pos   entity nounphrase whitespace\r\n  <chr>          <int> <chr>  <chr> <chr> <chr>  <chr>      <lgl>     \r\n1 1                  1 $      $     SYM   \"MONE~ beg        FALSE     \r\n2 1                  2 174    174   NUM   \"MONE~ mid        TRUE      \r\n3 1                  3 Milli~ mill~ NUM   \"MONE~ mid        TRUE      \r\n4 1                  4 Afghan Afgh~ PROPN \"\"     mid        TRUE      \r\n5 1                  5 Drone  Drone PROPN \"\"     mid        TRUE      \r\n6 1                  6 Progr~ Prog~ PROPN \"\"     end_root   TRUE      \r\n# ... with 2 more variables: is_punct <lgl>, is_stop <lgl>\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-27T15:46:13-05:00",
    "input_file": "headline-expanded.knit.md"
  },
  {
    "path": "posts/headline-analysis/",
    "title": "Analysis of Main vs. Print Headlines: Phase 1",
    "description": "Text as Data Project Headline Comparison Research Using API Query \"Afghanistan Withdrawal\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/NYT-Analysis/"
      }
    ],
    "date": "2022-04-17",
    "categories": [
      "text as data",
      "NYT text analysis project"
    ],
    "contents": "\r\n\r\nContents\r\nComparing\r\nMain vs. Print Headlines in the New York Times\r\nResearch Background\r\nMaking\r\nChoices on Inclusion of Observations\r\nGathering Data\r\nPrevious Process\r\nLoad Data\r\nLoad Data\r\nCreate Corpus\r\nAssign Type to Docvars\r\nTokenization\r\nDocument Feature Matrix\r\n\r\nDictionary Analysis\r\nliwcalike()\r\nNRC\r\nLSD 2015\r\nGeneral Inquirer\r\nAnnotation\r\n\r\n\r\n\r\nComparing\r\nMain vs. Print Headlines in the New York Times\r\nResearch Background\r\nDuring the Fall 2021 semester, my research group hand coded PDF\r\ncopies of articles resulting from a simple search on the websites of the\r\nNew York Times and Wall Street Journal from Feburary 29, 2020 through\r\nSeptember 30, 2021 using the term “Afghanistan withdrawal”. One thing I\r\nnoticed was that when loading the PDF articles into NVivo for coding was\r\nthat it was difficult to match the New York Times articles to the\r\ncitation information in Zotero for many of the articles because the\r\narticle titles did not match. I realized that in the process of saving\r\nthe articles in Zotero, they were saved with the title viewable on the\r\nweb version of the article; however, once the article had been preserved\r\nby using the site’s “Print to PDF” function, the article title that it\r\nused as a default file name was different than the web version.\r\nThis semester, I began this project to be one expanding on last\r\nsemester’s research and looking to expand a machine analysis of articles\r\npulling articles beginning January 2020 through December 2021. For my\r\ninitial text collection, I collected articles using the New York Times\r\nAPI for the search query “Afghanistan”, and hoped to be able to analyze\r\nthe full text of a larger range of articles.\r\nHowever, I found that I am limited in that the article search API for\r\nthe New York Times does not pull the entire article; rather, I was able\r\nto pull the abstract/summary, lead paragraph, and snippet for each\r\narticle as well as the keywords, authors, sections, and url. In\r\naddition, I was able to get the article titles for both the print and\r\nonline versions of the article.\r\nThe API’s lack of full article text was not optimal for my purposes;\r\nto examine sentiment and co-occurence of various sources. Sources are\r\nnot necessarily detailed in the lead paragraph or abstract of an\r\narticle, so I moved to a different research path.\r\nRemembering the differences in headlines from our manual coding\r\nresearch and noting that the API provides both headlines in the article\r\nsearch API, I turned to analyzing the differences in the main vs. print\r\nheadlines for articles from the same research period as our first\r\nexamination. This way, I can potentially use a sample of the full\r\narticles collected in our previous research and take the additional step\r\nof analyzing the sentiment of full articles and how they may or may not\r\nrelate to the differing headlines.\r\nMaking Choices on\r\nInclusion of Observations\r\nIn my initial look at the headline data, it was clear that not all of\r\nthe articles had different headlines; some are the same entries, and\r\nsome have “N/A” in the “print” version only, indicating they were\r\nonline-only stories. Although I initially felt inclined to leave the\r\n“N/A” observations in the analysis, I removed those observations as they\r\nwould not be relevant to my new research questions comparing the framing\r\nfor different audiences.\r\nI also removed whole sections where the API returned an observation\r\nas there was apparently use of the term “Afghanistan withdrawal”\r\nsomewhere in the article/entry, but the type of entry was clearly not\r\nbeing represented in the headline. For example, “Corrections” entries\r\nhave headlines consisting only of the term “Corrections” and the\r\ncorresponding date. Similar choices were made on the “Arts”, Books”, and\r\n“Podcasts” sections when entries are primarily the names of the things\r\nbeing reviewed that may have a reference to the Afghanistan withdrawal\r\nsomewhere in the text, but it is not relevant specifically to the\r\nwithdrawal time period being analyzed.\r\nWith few exceptions, this left the entirety of the “U.S.” and “World”\r\nnews sections, even if the content related to Afghanistan is not readily\r\nobservable. The count (~650) matched the number of articles pulled for\r\nthe hand coding research as well.\r\nGathering Data\r\nPrevious Process\r\nTo pull the data, I had to reduce the queries into more workable\r\ngroups that would not time out, given the NYT API limits. I was able to\r\npull the ~700 articles by chunk, then assemble them into a dataframe\r\nafter cleaning. I will not run the code in this post, as it was already\r\nrun and is an exhaustive process.\r\n\r\n\r\n\r\nAfter compiling the data, I re-formatted the date column and saving\r\nthe formatted tibble for offline access.\r\n\r\n\r\n\r\nLoad Data\r\nNow to the active review of the data. Loading the data from my\r\ncollection phase:\r\nLoad Data\r\n\r\n  article_id      date\r\n1          1 2/29/2020\r\n2          2 2/29/2020\r\n3          3  3/1/2020\r\n4          4  3/2/2020\r\n5          5  3/2/2020\r\n6          6  3/3/2020\r\n                                                                 headline_main\r\n1                              4 Takeaways From the U.S. Deal With the Taliban\r\n2    Taliban and U.S. Strike Deal to Withdraw American Troops From Afghanistan\r\n3           Afghanistan War Enters New Stage as U.S. Military Prepares to Exit\r\n4                 At Center of Taliban Deal, a U.S. Envoy Who Made It Personal\r\n5 U.S. Announces Troop Withdrawal in Afghanistan as Respite From Violence Ends\r\n6                                           Trump Speaks With a Taliban Leader\r\n  article_id      date\r\n1          1 2/29/2020\r\n2          2 2/29/2020\r\n3          3  3/1/2020\r\n4          4  3/2/2020\r\n5          5  3/2/2020\r\n6          6  3/3/2020\r\n                                                        headline_print\r\n1                                 Table Is Set For a Pullout And Talks\r\n2                              U.S. and Taliban Sign Withdrawal Accord\r\n3                                      A Mission Shift for Afghanistan\r\n4 At the Center of the Taliban Deal, a U.S. Envoy Who Made It Personal\r\n5                           U.S. Troop Reduction Begins in Afghanistan\r\n6               Pursuing Exit, Trump Talks  To a Leader Of the Taliban\r\n\r\nCreate Corpus\r\n\r\n\r\nmain_corpus <- corpus(main_headlines, docid_field = \"article_id\", text_field = \"headline_main\")\r\nprint_corpus <- corpus(print_headlines, docid_field = \"article_id\", text_field = \"headline_print\")\r\n\r\n\r\n\r\nAssign Type to Docvars\r\n\r\n\r\nmain_corpus$type <- \"Main Headline\"\r\nprint_corpus$type <- \"Print Headline\"\r\ndocvars(main_corpus, field = \"type\") <- main_corpus$type\r\ndocvars(print_corpus, field = \"type\") <- print_corpus$type\r\n\r\n\r\n\r\nTokenization\r\nAfter many process posts, I finally realized how to remove the “�”\r\nsymbol that has plagued me since starting working with this API by using\r\n“remove_symbols=TRUE” in addition to removing the punctuation when\r\ntokenizing. I also want to remove stopwords.\r\nMain Headlines\r\n\r\n[1] 346\r\nTokens consisting of 346 documents and 2 docvars.\r\n1 :\r\n[1] \"4\"         \"Takeaways\" \"U.S\"       \"Deal\"      \"Taliban\"  \r\n\r\n2 :\r\n[1] \"Taliban\"     \"U.S\"         \"Strike\"      \"Deal\"       \r\n[5] \"Withdraw\"    \"American\"    \"Troops\"      \"Afghanistan\"\r\n\r\n3 :\r\n[1] \"Afghanistan\" \"War\"         \"Enters\"      \"New\"        \r\n[5] \"Stage\"       \"U.S\"         \"Military\"    \"Prepares\"   \r\n[9] \"Exit\"       \r\n\r\n4 :\r\n[1] \"Center\"   \"Taliban\"  \"Deal\"     \"U.S\"      \"Envoy\"    \"Made\"    \r\n[7] \"Personal\"\r\n\r\n5 :\r\n[1] \"U.S\"         \"Announces\"   \"Troop\"       \"Withdrawal\" \r\n[5] \"Afghanistan\" \"Respite\"     \"Violence\"    \"Ends\"       \r\n\r\n6 :\r\n[1] \"Trump\"   \"Speaks\"  \"Taliban\" \"Leader\" \r\n\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\nPrint Headlines\r\n\r\n[1] 346\r\nTokens consisting of 346 documents and 2 docvars.\r\n1 :\r\n[1] \"Table\"   \"Set\"     \"Pullout\" \"Talks\"  \r\n\r\n2 :\r\n[1] \"U.S\"        \"Taliban\"    \"Sign\"       \"Withdrawal\" \"Accord\"    \r\n\r\n3 :\r\n[1] \"Mission\"     \"Shift\"       \"Afghanistan\"\r\n\r\n4 :\r\n[1] \"Center\"   \"Taliban\"  \"Deal\"     \"U.S\"      \"Envoy\"    \"Made\"    \r\n[7] \"Personal\"\r\n\r\n5 :\r\n[1] \"U.S\"         \"Troop\"       \"Reduction\"   \"Begins\"     \r\n[5] \"Afghanistan\"\r\n\r\n6 :\r\n[1] \"Pursuing\" \"Exit\"     \"Trump\"    \"Talks\"    \"Leader\"   \"Taliban\" \r\n\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\nDocument Feature Matrix\r\n\r\n\r\n\r\n\r\n\r\n#create a word frequency variable and the rankings\r\nmain_counts <- as.data.frame(sort(colSums(main_dfm),dec=T))\r\ncolnames(main_counts) <- c(\"Frequency\")\r\nmain_counts$Rank <- c(1:ncol(main_dfm))\r\nhead(main_counts)\r\n\r\n\r\n            Frequency Rank\r\nu.s               100    1\r\nafghanistan        87    2\r\nafghan             85    3\r\ntaliban            65    4\r\nbiden              53    5\r\nwar                30    6\r\n\r\nprint_counts <- as.data.frame(sort(colSums(print_dfm),dec=T))\r\ncolnames(print_counts) <- c(\"Frequency\")\r\nprint_counts$Rank <- c(1:ncol(print_dfm))\r\nhead(print_counts)\r\n\r\n\r\n            Frequency Rank\r\nu.s               100    1\r\ntaliban            66    2\r\nafghan             64    3\r\nafghanistan        56    4\r\nbiden              41    5\r\nexit               27    6\r\n\r\nNow I can take a look at this network of feature co-occurrences for\r\nthe main headlines:\r\n\r\n[1] 1232 1232\r\n[1] 20 20\r\n\r\n\r\nand for the print headlines:\r\n\r\n[1] 1178 1178\r\n[1] 20 20\r\n\r\n\r\nThis brings me to where I had previously stopped in my comparison and\r\nanalysis, and now that I’m able to produce a cleaner result, I’ll move\r\non to further analysis using the quanteda dictionary.\r\nDictionary Analysis\r\nliwcalike()\r\n\r\n [1] \"docname\"      \"Segment\"      \"WPS\"          \"WC\"          \r\n [5] \"Sixltr\"       \"Dic\"          \"anger\"        \"anticipation\"\r\n [9] \"disgust\"      \"fear\"         \"joy\"          \"negative\"    \r\n[13] \"positive\"     \"sadness\"      \"surprise\"     \"trust\"       \r\n[17] \"AllPunc\"      \"Period\"       \"Comma\"        \"Colon\"       \r\n[21] \"SemiC\"        \"QMark\"        \"Exclam\"       \"Dash\"        \r\n[25] \"Quote\"        \"Apostro\"      \"Parenth\"      \"OtherP\"      \r\n\r\nNRC\r\n\r\n\r\n# convert tokens from each headline data set to DFM using the dictionary \"NRC\"\r\nmain_nrc <- dfm(main_tokens) %>%\r\n  dfm_lookup(data_dictionary_NRC)\r\nprint_nrc <- dfm(print_tokens) %>%\r\n  dfm_lookup(data_dictionary_NRC)\r\n\r\ndim(main_nrc)\r\n\r\n\r\n[1] 346  10\r\n\r\nmain_nrc\r\n\r\n\r\nDocument-feature matrix of: 346 documents, 10 features (69.36% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            1       0    0   1        0        1       0\r\n   2     1            1       0    0   1        2        1       1\r\n   3     0            0       0    2   0        1        0       0\r\n   4     0            1       0    0   1        0        2       0\r\n   5     1            0       0    1   1        1        1       1\r\n   6     0            0       0    0   0        0        1       0\r\n    features\r\ndocs surprise trust\r\n   1        1     1\r\n   2        1     1\r\n   3        0     0\r\n   4        1     3\r\n   5        0     1\r\n   6        1     1\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\ndim(print_nrc)\r\n\r\n\r\n[1] 346  10\r\n\r\nprint_nrc\r\n\r\n\r\nDocument-feature matrix of: 346 documents, 10 features (71.47% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            0       0    0   0        0        0       0\r\n   2     0            0       0    0   0        0        1       0\r\n   3     0            0       0    0   0        0        0       0\r\n   4     0            1       0    0   1        0        2       0\r\n   5     0            0       0    0   0        0        0       0\r\n   6     0            0       0    0   0        0        1       0\r\n    features\r\ndocs surprise trust\r\n   1        0     0\r\n   2        0     1\r\n   3        0     0\r\n   4        1     3\r\n   5        0     0\r\n   6        1     1\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\nAnd use the information in a data frame to plot the output:\r\n\r\n\r\n#for the main headlines\r\ndf_main_nrc <- convert(main_nrc, to = \"data.frame\")\r\ndf_main_nrc$polarity <- (df_main_nrc$positive - df_main_nrc$negative)/(df_main_nrc$positive + df_main_nrc$negative)\r\ndf_main_nrc$polarity[which((df_main_nrc$positive + df_main_nrc$negative) == 0)] <- 0\r\n\r\nggplot(df_main_nrc) + \r\n  geom_histogram(aes(x=polarity)) + \r\n  theme_bw()\r\n\r\n\r\n\r\n#and the print headlines\r\ndf_print_nrc <- convert(print_nrc, to = \"data.frame\")\r\ndf_print_nrc$polarity <- (df_print_nrc$positive - df_print_nrc$negative)/(df_print_nrc$positive + df_print_nrc$negative)\r\ndf_print_nrc$polarity[which((df_print_nrc$positive + df_print_nrc$negative) == 0)] <- 0\r\n\r\nggplot(df_print_nrc) + \r\n  geom_histogram(aes(x=polarity)) + \r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nLooking at the headlines that are indicated as “1”, or positive in\r\nsentiment, it’s clear that this dictionary is not capturing the\r\nsentiment accurately.\r\n\r\n\r\nhead(main_corpus[which(df_main_nrc$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n1 :\r\n\"4 Takeaways From the U.S. Deal With the Taliban\"\r\n\r\n4 :\r\n\"At Center of Taliban Deal, a U.S. Envoy Who Made It Personal\"\r\n\r\n6 :\r\n\"Trump Speaks With a Taliban Leader\"\r\n\r\n8 :\r\n\"After Tours in Afghanistan, U.S. Veterans Weigh Peace With t...\"\r\n\r\n9 :\r\n\"Javier Pérez de Cuéllar Dies at 100; U.N. Chief Brokered Pea...\"\r\n\r\n10 :\r\n\"From the Afghan Peace Deal, a Weak and Pliable Neighbor for ...\"\r\n\r\nhead(print_corpus[which(df_print_nrc$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n2 :\r\n\"U.S. and Taliban Sign Withdrawal Accord\"\r\n\r\n4 :\r\n\"At the Center of the Taliban Deal, a U.S. Envoy Who Made It ...\"\r\n\r\n6 :\r\n\"Pursuing Exit, Trump Talks  To a Leader Of the Taliban\"\r\n\r\n7 :\r\n\"Attacks on Afghans by Taliban Rise After Signing of Peace De...\"\r\n\r\n8 :\r\n\"After Afghanistan Tours,  U.S. Veterans Appraise  Peace Deal...\"\r\n\r\n9 :\r\n\"Javier Pérez de Cuéllar, U.N. Chief  Behind Vital Peace Pact...\"\r\n\r\nLSD 2015\r\nI am going to want to look at multiple dictionaries to see if one can\r\nbest apply to this data. First, the LSD 2015 dictionary:\r\n\r\n\r\n# convert main corpus to DFM using the LSD2015 dictionary\r\nmain_lsd2015 <- dfm(tokens(main_corpus, remove_punct = TRUE),\r\n                              tolower = TRUE) %>%\r\n                          dfm_lookup(data_dictionary_LSD2015)\r\n# create main polarity measure for LSD2015\r\nmain_lsd2015 <- convert(main_lsd2015, to = \"data.frame\")\r\nmain_lsd2015$polarity <- (main_lsd2015$positive - main_lsd2015$negative)/(main_lsd2015$positive + main_lsd2015$negative)\r\nmain_lsd2015$polarity[which((main_lsd2015$positive + main_lsd2015$negative) == 0)] <- 0\r\n# convert print corpus to DFM using the LSD2015 dictionary\r\nprint_lsd2015 <- dfm(tokens(print_corpus, remove_punct = TRUE),\r\n                              tolower = TRUE) %>%\r\n                          dfm_lookup(data_dictionary_LSD2015)\r\n# create print polarity measure for LSD2015\r\nprint_lsd2015 <- convert(print_lsd2015, to = \"data.frame\")\r\nprint_lsd2015$polarity <- (print_lsd2015$positive - print_lsd2015$negative)/(print_lsd2015$positive + print_lsd2015$negative)\r\nprint_lsd2015$polarity[which((print_lsd2015$positive + print_lsd2015$negative) == 0)] <- 0\r\n\r\n\r\n\r\nGeneral Inquirer\r\nand the General Inquirer dictionary:\r\n\r\n\r\n# convert main corpus to DFM using the General Inquirer dictionary\r\nmain_geninq <- dfm(tokens(main_corpus, remove_punct = TRUE),\r\n                             tolower = TRUE) %>%\r\n                    dfm_lookup(data_dictionary_geninqposneg)\r\n# create main polarity measure for GenInq\r\nmain_geninq <- convert(main_geninq, to = \"data.frame\")\r\nmain_geninq$polarity <- (main_geninq$positive - main_geninq$negative)/(main_geninq$positive + main_geninq$negative)\r\nmain_geninq$polarity[which((main_geninq$positive + main_geninq$negative) == 0)] <- 0\r\n# convert print corpus to DFM using the General Inquirer dictionary\r\nprint_geninq <- dfm(tokens(print_corpus, remove_punct = TRUE),\r\n                             tolower = TRUE) %>%\r\n                    dfm_lookup(data_dictionary_geninqposneg)\r\n# create print polarity measure for GenInq\r\nprint_geninq <- convert(print_geninq, to = \"data.frame\")\r\nprint_geninq$polarity <- (print_geninq$positive - print_geninq$negative)/(print_geninq$positive + print_geninq $negative)\r\nprint_geninq$polarity[which((print_geninq$positive + print_geninq$negative) == 0)] <- 0\r\n\r\n\r\n\r\nNow I’m going to be able to compare the different dictionary scores\r\nin one data frame for each type of headline.\r\n\r\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy\r\n1          1         0                1           0        0       1\r\n2         10         0                3           0        0       2\r\n3        100         0                0           0        0       0\r\n4        101         0                2           0        0       1\r\n5        102         1                0           0        2       0\r\n6        103         1                1           0        1       0\r\n  nrc_negative nrc_positive nrc_sadness nrc_surprise nrc_trust\r\n1            0            1           0            1         1\r\n2            0            3           0            1         3\r\n3            1            0           1            0         0\r\n4            2            1           0            0         2\r\n5            2            0           2            1         0\r\n6            0            2           0            0         1\r\n  nrc_polarity lsd2015_negative lsd2015_positive lsd2015_neg_positive\r\n1    1.0000000                0                0                    0\r\n2    1.0000000                1                1                    0\r\n3   -1.0000000                1                0                    0\r\n4   -0.3333333                1                0                    0\r\n5   -1.0000000                2                0                    0\r\n6    1.0000000                0                0                    0\r\n  lsd2015_neg_negative lsd2015_polarity geninq_positive\r\n1                    0                0               1\r\n2                    0                0               2\r\n3                    0               -1               0\r\n4                    0               -1               0\r\n5                    0               -1               1\r\n6                    0                0               1\r\n  geninq_negative geninq_polarity\r\n1               1       0.0000000\r\n2               1       0.3333333\r\n3               0       0.0000000\r\n4               2      -1.0000000\r\n5               1       0.0000000\r\n6               1       0.0000000\r\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy\r\n1          1         0                0           0        0       0\r\n2         10         0                2           0        0       1\r\n3        100         0                0           0        0       0\r\n4        101         0                2           0        0       1\r\n5        102         1                0           0        4       0\r\n6        103         1                1           0        1       0\r\n  nrc_negative nrc_positive nrc_sadness nrc_surprise nrc_trust\r\n1            0            0           0            0         0\r\n2            0            2           0            1         2\r\n3            1            0           1            0         0\r\n4            2            1           0            0         2\r\n5            3            0           3            1         0\r\n6            0            1           0            0         0\r\n  nrc_polarity lsd2015_negative lsd2015_positive lsd2015_neg_positive\r\n1    0.0000000                0                0                    0\r\n2    1.0000000                1                0                    0\r\n3   -1.0000000                1                0                    0\r\n4   -0.3333333                1                0                    0\r\n5   -1.0000000                2                0                    0\r\n6    1.0000000                0                0                    0\r\n  lsd2015_neg_negative lsd2015_polarity geninq_positive\r\n1                    0                0               0\r\n2                    0               -1               2\r\n3                    0               -1               0\r\n4                    0               -1               0\r\n5                    0               -1               1\r\n6                    0                0               1\r\n  geninq_negative geninq_polarity\r\n1               0       0.0000000\r\n2               1       0.3333333\r\n3               1      -1.0000000\r\n4               2      -1.0000000\r\n5               0       1.0000000\r\n6               0       1.0000000\r\n\r\nNow that we have them all in a single data frame, it’s\r\nstraightforward to figure out a bit about how well our different\r\nmeasures of polarity agree across the different approaches by looking at\r\ntheir correlation using the “cor()” function.\r\n\r\n\r\ncor(main_sent$nrc_polarity, main_sent$lsd2015_polarity)\r\n\r\n\r\n[1] 0.4968335\r\n\r\ncor(main_sent$nrc_polarity, main_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4813359\r\n\r\ncor(main_sent$lsd2015_polarity, main_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.5327856\r\n\r\n\r\n\r\ncor(print_sent$nrc_polarity, print_sent$lsd2015_polarity)\r\n\r\n\r\n[1] 0.4197161\r\n\r\ncor(print_sent$nrc_polarity, print_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4917256\r\n\r\ncor(print_sent$lsd2015_polarity, print_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4921922\r\n\r\nAnnotation\r\n\r\n\r\nlibrary(cleanNLP)\r\ncnlp_init_udpipe()\r\n\r\n\r\n\r\n\r\n\r\nlibrary(tidyr)\r\n#amain <- as_tibble(headlines_main)\r\n#annotated_main <- cnlp_annotate(main_corpus)\r\n#annotated_main\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/headline-analysis/unnamed-chunk-12-1.png",
    "last_modified": "2022-04-26T14:34:19-05:00",
    "input_file": "headline-analysis.knit.md"
  },
  {
    "path": "posts/pdf-analysis/",
    "title": "Analysis of PDF Articles",
    "description": "Text as Data Project-Article Sentiment Research",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/NYT-Analysis/"
      }
    ],
    "date": "2022-04-17",
    "categories": [
      "text as data",
      "NYT text analysis project"
    ],
    "contents": "\r\n\r\nContents\r\nGetting Started\r\nPulling in the PDF docs\r\nExtracting\r\nPDF Files being examined (random at this time - exploratory)\r\nInspect the first\r\narticle\r\nInspecting Individual\r\nArticles\r\nUnlist\r\n\r\nConclusion\r\n\r\nGetting Started\r\nThe primary goal of this aspect of research is to refine the process\r\nfor examining the content of the full articles for which the main\r\nvs. print headlines are the most different from each other in the\r\nprimary project analysis.\r\nPulling in the PDF docs\r\nI have the PDF files in my working directory. Using the\r\n“list.files()” function from the “pdftools” package, I can create a\r\nvector of PDF file names, specifying only files that end in “.pdf”.\r\n\r\n\r\nShow code\r\n\r\n#load libraries\r\nlibrary(pdftools)\r\nlibrary(readtext)\r\nlibrary(readr)\r\nlibrary(tm)\r\nlibrary(tidytext)\r\nlibrary(stringr)\r\nlibrary(MASS)\r\nlibrary(tidyverse)\r\nlibrary(plyr); library(dplyr)\r\nlibrary(quanteda)\r\nlibrary(purrr)\r\nlibrary(here)\r\n\r\n\r\n\r\nExtracting\r\nPDF Files being examined (random at this time - exploratory)\r\n\r\n\r\n#create file names\r\nfiles <- list.files(pattern = \"pdf$\")\r\n\r\n#extract the pdf file data\r\nnyt_articles <- lapply(files, pdf_text)\r\n\r\n#apply length functions\r\nlapply(nyt_articles, length)\r\n\r\n\r\n[[1]]\r\n[1] 4\r\n\r\n[[2]]\r\n[1] 2\r\n\r\n[[3]]\r\n[1] 3\r\n\r\n[[4]]\r\n[1] 4\r\n\r\n[[5]]\r\n[1] 10\r\n\r\n[[6]]\r\n[1] 6\r\n\r\n[[7]]\r\n[1] 2\r\n\r\n[[8]]\r\n[1] 5\r\n\r\n[[9]]\r\n[1] 5\r\n\r\n[[10]]\r\n[1] 2\r\n\r\n#view the structure of the list\r\nstr(nyt_articles)\r\n\r\n\r\nList of 10\r\n $ : chr [1:4] \"                                 https://www.nytimes.com/2021/04/13/us/politics/samantha-power-biden.html\\n\\n\\n\"| __truncated__ \"                                Secretary of State Antony J. Blinken at the opening session of talks with China\"| __truncated__ \"The mostly benign prodding by Democrats and Republicans during the hearing signaled how countering China has be\"| __truncated__ \"“There is so much that can be done between bombing and nothing,” Mr. Prendergast said, paraphrasing Luis Moreno\"| __truncated__\r\n $ : chr [1:2] \"                            https://www.nytimes.com/2021/04/29/world/asia/central-asia-border-\\n               \"| __truncated__ \"In announcing the cease-fire, the Kyrgyz Ministry of Interior said that it “does not have\\ndesigns on foreign t\"| __truncated__\r\n $ : chr [1:3] \"                                https://www.nytimes.com/2021/08/05/us/politics/taliban-afghanistan-peace-deal.h\"| __truncated__ \"The statement came as Taliban representatives met with Afghan government officials, including Mr. Abdullah, for\"| __truncated__ \"“The Taliban is not interested in negotiating seriously right now because of what’s happening on the battlefiel\"| __truncated__\r\n $ : chr [1:4] \"                                https://www.nytimes.com/2021/08/08/us/politics/taliban-afghanistan-united-state\"| __truncated__ \"Over the past week, Taliban fighters have moved swiftly to retake cities around Afghanistan, assassinated gover\"| __truncated__ \"                                 Ms. Psaki speaking to reporters at the White House, on Friday. Tom Brenner for\"| __truncated__ \"Mr. Biden, declaring that the United States had long ago accomplished its mission of denying terrorists a haven\"| __truncated__\r\n $ : chr [1:10] \"                                https://www.nytimes.com/2021/08/30/world/asia/us-withdrawal-afghanistan-kabul.h\"| __truncated__ \"Old Soviet tanks litter the grounds of Bala Hissar, outside Kunduz. Jim Huylebroek for The New York Times\\n\" \"  Khalil Haqqani, a Taliban leader, appeared at Friday prayers in Kabul this month with an American-made M-4 ri\"| __truncated__ \"The Taliban’s leverage, earned after years of fighting the world’s most advanced military, multiplied as they c\"| __truncated__ ...\r\n $ : chr [1:6] \"                                 https://www.nytimes.com/2021/09/01/world/asia/afghanistan-taliban-government-l\"| __truncated__ \"  Internally displaced Afghans fleeing the fighting in the north still live at a camp in the Sarawi Shomali par\"| __truncated__ \"  A vendor selling Taliban flags in Kabul on Friday near posters of the senior Taliban officials Amir Khan Mutt\"| __truncated__ \"The Taliban are also fighting stubborn opposition forces led by National Resistance Front leaders in Panjshir P\"| __truncated__ ...\r\n $ : chr [1:2] \"                               https://www.nytimes.com/2021/09/02/us/politics/congress-pentagon-budget-biden.ht\"| __truncated__ \"The lopsided vote underscored another reality: Even as the hard-charging liberal bloc of lawmakers pledging to \"| __truncated__\r\n $ : chr [1:5] \"                                 https://www.nytimes.com/2021/09/07/us/politics/afghan-war-iraq-veterans.html\\n\"| __truncated__ \"                                Jen Burch said the doctors who examined her in 2014 found ground glass nodules \"| __truncated__ \"                                 Melissa Gauntner has dealt with dual traumas and has at times been gripped wit\"| __truncated__ \"In military families, scholars find what they call secondary traumatic distress, symptoms of anxiety stemming f\"| __truncated__ ...\r\n $ : chr [1:5] \"                                 https://www.nytimes.com/2020/10/05/world/asia/afghan-peace-talks-children.html\"| __truncated__ \"                                   Fatima Gailani, whose father was one of the leaders of the mujahedeen resist\"| __truncated__ \"                                Anas Haqqani, the youngest son of the insurgent chief Jalaluddin Haqqani, is pa\"| __truncated__ \"                                 Jalaluddin Haqqani in an undated photo from a video released by the Taliban on\"| __truncated__ ...\r\n $ : chr [1:2] \"                                https://www.nytimes.com/2020/03/04/world/asia/afghanistan-taliban-violence.html\"| __truncated__ \"     Understand the Taliban Takeover in Afghanistan\\n\\n     Who are the Taliban? The Taliban arose in 1994 amid\"| __truncated__\r\n\r\nInspect the first article\r\n\r\n\r\nhead(nyt_articles[1])\r\n\r\n\r\n[[1]]\r\n[1] \"                                 https://www.nytimes.com/2021/04/13/us/politics/samantha-power-biden.html\\n\\n\\n\\nAfter Backing Military Force in Past, U.S.A.I.D. Nominee Focuses on Deploying Soft\\nPower\\nIf confirmed to oversee the U.S. Agency for International Development, Samantha Power will confront adversaries by bolstering\\ndemocracy and human rights. China is an early focus.\\n\\n\\n          By Lara Jakes\\n\\nPublished April 13, 2021   Updated April 14, 2021\\n\\n\\nWASHINGTON — Near the end of the 2014 documentary “Watchers of the Sky,” which chronicles the origins of the legal definition\\nof genocide, Samantha Power grows emotional. At the time, Ms. Power was President Barack Obama’s ambassador to the United\\nNations, and, she said, had “great visibility into a lot of the pain” in the world.\\n\\nFrom that perch, preventing mass atrocities abroad required “thinking through what we can do about it, to exhaust the tools at your\\ndisposal,” Ms. Power said in the film. “And I always think about the privilege of, you know, of getting to try — just to try.”\\n\\nFew doubt Ms. Power’s zeal — given her career as a war correspondent, human rights activist, academic expert and foreign policy\\nadviser — even if it has meant advocating military force to stop widespread killings.\\n\\nNow, as President Biden’s nominee to lead the United States Agency for International Development, she is preparing to rejoin the\\ngovernment as an administrator of soft power, and resist using weapons as a means of deterrence and punishment that she has\\npushed for in the past.\\n\\nA Senate committee is expected to vote Thursday on her nomination to lead one of the world’s largest distributors of humanitarian\\naid.\\n\\nIf she is confirmed, Mr. Biden will also seat her on the National Security Council, where during the Obama administration she\\npressed for military intervention to protect civilians from state-sponsored attacks in Libya in 2011 and Syria in 2013. (However, she\\nalso opposed the 2003 invasion of Iraq.)\\n\\nThat she will be back at the table at the council — and again almost certain to be debating whether to entangle American forces in\\nenduring conflicts — has concerned some officials, analysts and think tank experts who demand military restraint from the Biden\\nadministration. Mr. Biden appears to be leaning that way: He has embraced economic sanctions as a tool of hard power and is\\nexpected to announce a full withdrawal of American troops from Afghanistan by Sept. 11, ending the United States’ longest war.\\n\\n“If you’re talking about humanitarianism, famine, the wars — really, other than natural causes, war is the No. 1 cause of famine\\naround the world,” Senator Rand Paul, Republican of Kentucky, told Ms. Power last month during her Senate confirmation hearing.\\n“Are you willing to admit that the Libyan and Syrian interventions that you advocated for were a mistake?”\\n\\nMs. Power did not. “When these situations arise, it’s a question almost of lesser evils — that the choices are very challenging,” she\\nsaid.\\n\\nBy its very nature, the U.S. aid agency takes a long-term view of the world compared with the immediacy of military action. Beyond\\nthe roughly $6 billion in humanitarian aid it is delivering this year to disaster-ridden nations, the agency seeks to prevent conflict at\\nits roots, largely bolstering economies, countering state corruption and fostering democracy and human rights.\\n\\nThat mission is central to Mr. Biden’s foreign policy, and will perhaps prove nowhere more pivotal than in his global competition\\nwith China.\\n\\nLast month, Secretary of State Antony J. Blinken assured allies that they would not be backed into an “‘us-or-them’ choice with\\nChina” as the two superpowers vie for economic, diplomatic and military advantage.\\n\"\r\n[2] \"                                Secretary of State Antony J. Blinken at the opening session of talks with China at the\\n                                Captain Cook hotel in Anchorage. Pool photo by Frederic J. Brown\\n\\n\\n\\nInstead, the United States is highlighting what officials call China’s malign ideology and self-interests as it expands an influence\\ncampaign across Africa, Europe and South America with financial loans, infrastructure funds, coronavirus vaccines and advanced\\ntechnology.\\n\\nThe Trump administration also seized on China’s human rights abuses — particularly against ethnic Uyghurs in the country’s\\nwestern region of Xinjiang — to persuade allies to turn against Beijing. On the Trump administration’s final day in office, Mike\\nPompeo, the secretary of state, declared China’s oppression against Uyghurs as an act of genocide, and he criticized Beijing’s\\nviolent suppression of dissidents in Hong Kong and military harassment of Taiwan.\\n\\n\\n                                Sign Up for On Politics A guide to the political news cycle, cutting\\n                                through the spin and delivering clarity from the chaos. Get it sent to your\\n                                inbox.\\n\\n\\nOfficials said China’s much-debated Belt and Road Initiative was a prime battleground for U.S.A.I.D. to challenge Beijing.\\n\\nRepresentative Tom Malinowski, Democrat of New Jersey and a former assistant secretary of state for democracy and human\\nrights for Mr. Obama, described a “perception that China is exporting corruption” with its loans and development projects.\\n\\nFor example, a study in February by the International Republican Institute, a private nonprofit group that receives government\\nfunding and promotes democracy, concluded that Panama’s decision in 2017 to sever diplomatic ties with Taiwan “appears to have\\nbeen driven by payoffs” from China. It also noted that Nepal regularly revoked the legal status of Tibetan refugees after becoming\\neconomically reliant on Beijing.\\n\\nThe American aid agency alone cannot match the funds that China has seeded in developing countries. But Mr. Malinowski said its\\nsupport to journalists, legal advisers and legitimate opposition groups could “expose and combat” corrosive foreign leaders who\\nhad benefited from Beijing’s financial backing and playbook for how to remain in power.\\n\\n“There is one issue that has risen to the top in this administration that I know she is very focused on, and that’s fighting corruption,”\\nMr. Malinowski said of Ms. Power. “And U.S.A.I.D. has a very important role to play there, potentially.”\\n\\nAt her confirmation hearing in March, Ms. Power told senators she was moved to pursue a career in foreign policy after the 1989\\nmassacre of protesters in Tiananmen Square in Beijing. She described China’s “coercive and predatory approach, which is so\\ntransactional” in its dealings with developing countries that ultimately become dependent on Beijing through what she called “debt-\\ntrap diplomacy.”\\n\\n“I think it’s not going over that well, and that creates an opening for the United States,” Ms. Power told Senator Todd Young,\\nRepublican of Indiana.\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \r\n[3] \"The mostly benign prodding by Democrats and Republicans during the hearing signaled how countering China has become a rare,\\nif reliable, issue of bipartisanship in Congress. “It’s absolutely essential that our development dollars, I think, be used to advance\\nour geostrategic priorities,” Mr. Young said.\\n\\nThe aid agency and the State Department have budgeted about $2 billion on programs to foster democracy, human rights and open\\ngovernance abroad in the 2021 fiscal year — one-third as much as funding for humanitarian assistance.\\n\\nIt is an area that Ms. Power is expected to expand. The Biden administration’s first budget blueprint, released on Friday, asserted it\\nwould commit an unspecified but “significant increase in resources” to advance human rights and democracy while thwarting\\ncorruption and authoritarianism.\\n\\n\\n\\n\\n                               Asylum seekers from Central America crossing the Paso del Norte International Bridge,\\n                               in Ciudad Juarez, Mexico. One of Ms. Power’s priorities will be to target corruption,\\n                               violence and poverty in the region. Jose Luis Gonzalez/Reuters\\n\\n\\n\\nThe spending plan also will support another of Ms. Power’s priorities: targeting corruption, violence and poverty in Central\\nAmerica as a means to curb the flow of thousands of migrants who head to the southwestern border each year. The Biden\\nadministration is banking on a $4 billion strategy through 2025 — including an initial tranche of $861 million proposed this year — to\\nhelp stabilize the region.\\n\\nIn El Salvador, for example, homicides dropped 61 percent after a U.S.A.I.D. effort to reduce violence from 2015 to 2017, Ms. Power\\ntold the senators, and the agency’s programs in Honduras have yielded similar results. The programs not only supported local\\nprosecutors but also brought together government officials, businesses and church and community leaders to divert young people\\nfrom gangs through job training, tutoring and artistic activities.\\n\\nShe was met with some skepticism.\\n\\nSenator Rob Portman, Republican of Ohio, noted that the number of children from Central America at the border had steadily\\nincreased since January, even though the United States spent $3.6 billion over the past five years on similar efforts.\\n\\n“The results are not impressive,” Mr. Portman said. “It’s an economic issue, primarily,” and “people will still be looking to come to\\nthe United States.”\\n\\nExplaining foreign policy decisions to the American people, and making it relevant to their lives, is a driving theme of the State\\nDepartment under Mr. Biden. Ms. Power can reach back to her own experiences as both an immigrant from Ireland and a\\nstoryteller to make the case for easing the border crisis by attacking its root causes.\\n\\n“That’s part of the job, too — you’ve got to be a salesperson, you’ve got to go out there and explain to people, ‘Here’s why we need\\nmore resources to do this work, and here’s where U.S.A.I.D. can be an incredibly important partner,’” said John Prendergast, a\\nlongtime human rights and anticorruption activist and close friend to Ms. Power.\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \r\n[4] \"“There is so much that can be done between bombing and nothing,” Mr. Prendergast said, paraphrasing Luis Moreno Ocampo, the\\nformer prosector of the International Criminal Court who was featured in the same documentary about genocide as Ms. Power.\\n“And Samantha’s whole work and life has been between those two extremes.”\\n\\nGayle Smith, who ran the aid agency for Mr. Obama and is now the State Department’s coronavirus vaccine envoy, put it more\\nbluntly.\\n\\n“It’s not like U.S.A.I.D. is going to invade somebody,” she said.\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \r\n\r\nInspecting Individual\r\nArticles\r\nNow I’m going to use “purrr” to “pluck()” each of the articles as\r\nits’ own vector and create a corpus of each article to examine.\r\n\r\n\r\narticle_111 <- nyt_articles %>% \r\n  pluck(1)\r\narticle_111 <- as_vector(article_111)\r\n\r\narticle_111_corpus <- corpus(article_111)\r\narticle_111_summary <- summary(article_111_corpus)\r\narticle_111_summary\r\n\r\n\r\nCorpus consisting of 4 documents, showing 4 documents:\r\n\r\n  Text Types Tokens Sentences\r\n text1   357    688        24\r\n text2   289    523        19\r\n text3   304    562        18\r\n text4    76    105         4\r\n\r\nI also found a very interesting way to pul the text and save them as\r\nindividual .txt files, but for now I’m just going to note that as an\r\nalternative process. I’ve struggled quite a bit to get the PDF text read\r\ncompared to the headlines.\r\n\r\n\r\nShow code\r\n\r\nconvertpdf2txt <- function(dirpath){\r\n  files <- list.files(dirpath, full.names = T)\r\n  x <- sapply(files, function(x){\r\n  x <- pdftools::pdf_text(x) %>%\r\n  paste(sep = \" \") %>%\r\n  stringr::str_replace_all(fixed(\"\\n\"), \" \") %>%\r\n  stringr::str_replace_all(fixed(\"\\r\"), \" \") %>%\r\n  stringr::str_replace_all(fixed(\"\\t\"), \" \") %>%\r\n  stringr::str_replace_all(fixed(\"\\\"\"), \" \") %>%\r\n  paste(sep = \" \", collapse = \" \") %>%\r\n  stringr::str_squish() %>%\r\n  stringr::str_replace_all(\"- \", \"\") \r\n  return(x)\r\n    })\r\n}\r\n# apply function\r\ntxts <- convertpdf2txt(\"./files\")\r\n# inspect the structure of the txts element\r\nstr(txts)\r\n\r\n\r\n Named chr [1:10] \"https://www.nytimes.com/2021/04/13/us/politics/samantha-power-biden.html After Backing Military Force in Past, \"| __truncated__ ...\r\n - attr(*, \"names\")= chr [1:10] \"./files/article_111.pdf\" \"./files/article_132.pdf\" \"./files/article_193.pdf\" \"./files/article_196.pdf\" ...\r\n\r\n\r\n\r\nShow code\r\n\r\n#apply length functions\r\nlapply(txts, length)\r\n\r\n\r\n$`./files/article_111.pdf`\r\n[1] 1\r\n\r\n$`./files/article_132.pdf`\r\n[1] 1\r\n\r\n$`./files/article_193.pdf`\r\n[1] 1\r\n\r\n$`./files/article_196.pdf`\r\n[1] 1\r\n\r\n$`./files/article_278.pdf`\r\n[1] 1\r\n\r\n$`./files/article_288.pdf`\r\n[1] 1\r\n\r\n$`./files/article_293.pdf`\r\n[1] 1\r\n\r\n$`./files/article_300.pdf`\r\n[1] 1\r\n\r\n$`./files/article_56.pdf`\r\n[1] 1\r\n\r\n$`./files/article_7.pdf`\r\n[1] 1\r\n\r\nShow code\r\n\r\n#view the structure of the list\r\nstr(txts)\r\n\r\n\r\n Named chr [1:10] \"https://www.nytimes.com/2021/04/13/us/politics/samantha-power-biden.html After Backing Military Force in Past, \"| __truncated__ ...\r\n - attr(*, \"names\")= chr [1:10] \"./files/article_111.pdf\" \"./files/article_132.pdf\" \"./files/article_193.pdf\" \"./files/article_196.pdf\" ...\r\n\r\nShow code\r\n\r\n# add names to txt files\r\nnames(txts) <- paste(\"nyt\", 1:length(txts), sep = \"\")\r\n# save result to disc\r\nlapply(seq_along(txts), function(i)writeLines(text = unlist(txts[i]),\r\n    con = paste(\"./txts\", names(txts)[i],\".txt\", sep = \"\")))\r\n\r\n\r\n[[1]]\r\nNULL\r\n\r\n[[2]]\r\nNULL\r\n\r\n[[3]]\r\nNULL\r\n\r\n[[4]]\r\nNULL\r\n\r\n[[5]]\r\nNULL\r\n\r\n[[6]]\r\nNULL\r\n\r\n[[7]]\r\nNULL\r\n\r\n[[8]]\r\nNULL\r\n\r\n[[9]]\r\nNULL\r\n\r\n[[10]]\r\nNULL\r\n\r\nUnlist\r\nDocumenting, for now, the ways I’m struggling with so I can find out\r\nwhy.\r\n\r\n\r\n#convert list to vector\r\n#nyt_vector <- unlist(nyt_articles, recursive = TRUE)\r\n#put articles into data frame\r\n#nyt_df <- as.data.frame(nyt_vector, row.names = NULL, stringsAsFactors = FALSE)\r\n\r\n\r\n\r\n\r\n\r\n#create corpus\r\n#nyt_corpus <- corpus(txts)\r\n#confirming class of corpus\r\n#class(nyt_corpus)\r\n#confirm length of corpus\r\n#length(nyt_corpus)\r\n\r\n\r\n\r\nConclusion\r\nI will not be able to fit this type of analysis into the scope of my\r\ncurrent project. I will use this in further studies.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-23T13:23:57-05:00",
    "input_file": "pdf-analysis.knit.md"
  },
  {
    "path": "posts/lit-review/",
    "title": "Literature Review",
    "description": "Text as Data Project-Literature Review",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/NYT-Analysis/"
      }
    ],
    "date": "2022-04-15",
    "categories": [
      "text as data",
      "NYT text analysis project",
      "literature review"
    ],
    "contents": "\r\n\r\nContents\r\nAnnotated\r\nBibliography (Starting with Abstracts Only - In Development)\r\nLiu & Huang (2022)\r\nChan, et al. (2021)\r\nVan Atteveldt, et\r\nal. (2021)\r\nBurggraff & Trilling\r\n(2020)\r\nBoukes, et al. (2020)\r\nSong, et al. (2020)\r\nRudkowsky, et al. (2018)\r\nSilva (2017)\r\nGottlieb (2015)\r\nDiakopoulos (2015)\r\nGrimmer & Stewart\r\n(2013)\r\nKothari (2010)\r\nKiousis (2004)\r\nAlthaus & Tewksbury\r\n(2002)\r\n\r\n\r\nAnnotated\r\nBibliography (Starting with Abstracts Only - In Development)\r\nLiu & Huang (2022)\r\nLiu, M., & Huang, J. (2022). “Climate change” versus “global\r\nwarming”: A corpus-assisted discourse analysis of two popular terms in\r\nthe New York Times. Journal of World Languages. https://doi.org/10.1515/jwl-2022-0004\r\nAbstract\r\n“Climate change” and “global warming” are two popular terms that may\r\nbe often used interchangeably in news media. This study proposes to give\r\na corpusassisted discourse study of the representations of climate\r\nchange and global warming in the New York Times (2000–2019) in order to\r\nexamine how they are actually used in the newspaper. The findings show\r\nboth similarities and differences in their representations in terms of\r\nthe associated topics/themes, the particular ways of framing, and the\r\nperspectivization strategy employed. It is argued that a corpus-assisted\r\ndiscourse study of a large sample of news articles presents a more\r\naccurate picture of the actual use of the two terms in news media.\r\nChan, et al. (2021)\r\nChan, C., Bajjalieh, J., Auvil, L., Wessler, H., Althaus, S.,\r\nWelbers, K., Atteveldt, W. van, & Jungblut, M. (2021). Four best\r\npractices for measuring news sentiment using ‘off-the-shelf’\r\ndictionaries: A large-scale p-hacking experiment. Computational\r\nCommunication Research, 3(1), 1–27.\r\nAbstract\r\nWe examined the validity of 37 sentiment scores based on\r\ndictionary-based methods using a large news corpus and demonstrated the\r\nrisk of generating a spectrum of results with different levels of\r\nstatistical significance by presenting an analysis of relationships\r\nbetween news sentiment and U.S. presidential approval. We summarize our\r\nfindings into four best practices: 1) use a suitable sentiment\r\ndictionary; 2) do not assume that the validity and reliability of the\r\ndictionary is ‘built-in’; 3) check for the influence of content length\r\nand 4) do not use multiple dictionaries to test the same statistical\r\nhypothesis.\r\nVan Atteveldt, et al. (2021)\r\nvan Atteveldt, W., van der Velden, M. A. C. G., & Boukes, M.\r\n(2021). The Validity of Sentiment Analysis: Comparing Manual Annotation,\r\nCrowd-Coding, Dictionary Approaches, and Machine Learning Algorithms.\r\nCommunication Methods and Measures, 15(2), 121–140. https://doi.org/10.1080/19312458.2020.1869198\r\nAbstract\r\nSentiment is central to many studies of communication science, from\r\nnegativity and polarization in political communication to analyzing\r\nproduct reviews and social media comments in other sub-fields. This\r\nstudy provides an exhaustive comparison of sentiment analysis methods,\r\nusing a validation set of Dutch economic headlines to compare the\r\nperformance of manual annotation, crowd coding, numerous dictionaries\r\nand machine learning using both traditional and deep learning\r\nalgorithms. The three main conclusions of this article are that: (1) The\r\nbest performance is still attained with trained human or crowd coding;\r\n(2) None of the used dictionaries come close to acceptable levels of\r\nvalidity; and (3) machine learning, especially deep learning,\r\nsubstantially outperforms dictionary-based methods but falls short of\r\nhuman performance. From these findings, we stress the importance of\r\nalways validating automatic text analysis methods before usage.\r\nMoreover, we provide a recommended step-bystep approach for (automated)\r\ntext analysis projects to ensure both efficiency and validity.\r\nBurggraff & Trilling (2020)\r\nBurggraaff, C., & Trilling, D. (2020). Through a different\r\ngate: An automated content analysis of how online news and print news\r\ndiffer. Journalism, 21(1), 112–129. https://doi.org/10.1177/1464884917716699\r\nAbstract\r\nWe investigate how news values differ between online and print news\r\narticles. We hypothesize that print and online articles differ in terms\r\nof news values because of differences in the routines used to produce\r\nthem. Based on a quantitative automated content analysis of N = 762,095\r\nDutch news items, we show that online news items are more likely to be\r\nfollow-up items than print items, and that there are further differences\r\nregarding news values like references to persons, the power elite,\r\nnegativity, and positivity. In order to conduct this large-scale\r\nanalysis, we developed innovative methods to automatically code a wide\r\nrange of news values. In particular, this article demonstrates how\r\ntechniques such as sentiment analysis, named entity recognition,\r\nsupervised machine learning, and automated queries of external databases\r\ncan be combined and used to study journalistic content. Possible\r\nexplanations for the difference found between online and offline news\r\nare discussed.\r\nBoukes, et al. (2020)\r\nBoukes, M., van de Velde, B., Araujo, T., & Vliegenthart, R.\r\n(2020). What’s the Tone? Easy Doesn’t Do It: Analyzing Performance and\r\nAgreement Between Off-the-Shelf Sentiment Analysis Tools. Communication\r\nMethods and Measures, 14(2), 83–104. https://doi.org/10.1080/19312458.2019.1671966\r\nAbstract\r\nThis article scrutinizes the method of automated content analysis to\r\nmeasure the tone of news coverage. We compare a range of off-the-shelf\r\nsentiment analysis tools to manually coded economic news as well as\r\nexamine the agreement between these dictionary approaches themselves. We\r\nassess the performance of five off-the-shelf sentiment analysis tools\r\nand two tailor-made dictionary-based approaches. The analyses result in\r\nfive conclusions. First, there is little overlap between the\r\noff-the-shelf tools; causing wide divergence in terms of tone\r\nmeasurement. Second, there is no stronger overlap with manual coding for\r\nshort texts (i.e., headlines) than for long texts (i.e., full articles).\r\nThird, an approach that combines individual dictionaries achieves a\r\ncomparably good performance. Fourth, precision may increase to\r\nacceptable levels at higher levels of granularity. Fifth, performance of\r\ndictionary approaches depends more on the number of relevant keywords in\r\nthe dictionary than on the number of valenced words as such; a small\r\ntailor-made lexicon was not inferior to large established dictionaries.\r\nAltogether, we conclude that off-the-shelf sentiment analysis tools are\r\nmostly unreliable and unsuitable for research purposes – at least in the\r\ncontext of Dutch economic news – and manual validation for the specific\r\nlanguage, domain, and genre of the research project at hand is always\r\nwarranted.\r\nSong, et al. (2020)\r\nSong, H., Tolochko, P., Eberl, J.-M., Eisele, O., Greussing, E.,\r\nHeidenreich, T., Lind, F., Galyga, S., & Boomgaarden, H. G. (2020).\r\nIn Validations We Trust? The Impact of Imperfect Human Annotations as a\r\nGold Standard on the Quality of Validation of Automated Content\r\nAnalysis. Political Communication, 37(4), 550–572. https://doi.org/10.1080/10584609.2020.1723752\r\nAbstract\r\nPolitical communication has become one of the central arenas of\r\ninnovation in the application of automated analysis approaches to\r\never-growing quantities of digitized texts. However, although\r\nresearchers routinely and conveniently resort to certain forms of human\r\ncoding to validate the results derived from automated procedures, in\r\npractice the actual “quality assurance” of such a “gold standard” often\r\ngoes unchecked. Contemporary practices of validation via manual\r\nannotations are far from being acknowledged as best practices in the\r\nliterature, and the reporting and interpretation of validation\r\nprocedures differ greatly. We systematically assess the connection\r\nbetween the quality of human judgment in manual annotations and the\r\nrelative performance evaluations of automated procedures against true\r\nstandards by relying on large-scale Monte Carlo simulations. The results\r\nfrom the simulations confirm that there is a substantially greater risk\r\nof a researcher reaching an incorrect conclusion regarding the\r\nperformance of automated procedures when the quality of manual\r\nannotations used for validation is not properly ensured. Our\r\ncontribution should therefore be regarded as a call for the systematic\r\napplication of high-quality manual validation materials in any political\r\ncommunication study, drawing on automated text analysis procedures.\r\nRudkowsky, et al. (2018)\r\nRudkowsky, E., Haselmayer, M., Wastian, M., Jenny, M., Emrich,\r\nŠ., & Sedlmair, M. (2018). More than Bags of Words: Sentiment\r\nAnalysis with Word Embeddings. Communication Methods and Measures,\r\n12(2–3), 140–157. https://doi.org/10.1080/19312458.2018.1455817\r\nAbstract\r\nMoving beyond the dominant bag-of-words approach to sentiment\r\nanalysis we introduce an alternative procedure based on distributed word\r\nembeddings. The strength of word embeddings is the ability to capture\r\nsimilarities in word meaning. We use word embeddings as part of a\r\nsupervised machine learning procedure which estimates levels of\r\nnegativity in parliamentary speeches. The procedure’s accuracy is\r\nevaluated with crowdcoded training sentences; its external validity\r\nthrough a study of patterns of negativity in Austrian parliamentary\r\nspeeches. The results show the potential of the word embeddings approach\r\nfor sentiment analysis in the social sciences.\r\nSilva (2017)\r\nSilva, D. M. D. (2017). The Othering of Muslims: Discourses of\r\nRadicalization in the New York Times, 1969–2014. Sociological Forum,\r\n32(1), 138–161. https://doi.org/10.1111/socf.12321\r\nAbstract\r\nIn this article, I engage with Edward Said’s Orientalism and various\r\nperspectives within the othering paradigm to analyze the emergence and\r\ntransformation of radicalization discourses in the news media. Employing\r\ndiscourse analysis of 607 New York Times articles from 1969 to 2014,\r\nthis article demonstrates that radicalization discourses are not new but\r\nare the result of complex sociolinguistic and historical developments\r\nthat cannot be reduced to dominant contemporary understandings of the\r\nconcept or to singular events or crises. The news articles were then\r\ncompared to 850 government documents, speeches, and other official\r\ncommunications. The analysis of the data indicates that media\r\nconceptualizations of radicalization, which once denoted political and\r\neconomic differences, have now shifted to overwhelmingly focus on Islam.\r\nAs such, radicalization discourse now evokes the construct\r\nradicalization as symbolic marker of conflict between the West and the\r\nEast. I also advanced the established notion that the news media employ\r\nstrategic discursive strategies that contribute to conceptual\r\ndistinctions that are used to construct Muslims as an “alien other” to\r\nthe West.\r\nGottlieb (2015)\r\nGottlieb, J. (2015). Protest News Framing Cycle: How The New York\r\nTimes Covered Occupy Wall Street. International Journal of\r\nCommunication, 9(0), 23.\r\nAbstract\r\nThis article introduces a protest news framing cycle and presents the\r\nresults of a longitudinal analysis of news attention and framing of\r\nprotest movements. To identify the frame-changing dynamic occurring over\r\ntime, a content analysis of the news coverage of Occupy Wall Street was\r\nconducted on 228 articles and 37 editorials in The New York Times from\r\nthe start of the protest in September 2011 until long after the protest\r\nhad subsided in July 2014. The article identifies longitudinal changes\r\nin news frames about the economic substance of the protest and the\r\nensuing conflict between protesters and city officials during the\r\noccupation. Findings suggest that conflict had a significant impact on\r\nthe number of news stories about the protest. Further, the results\r\ndemonstrate how news framing opportunities changed as the movement\r\nreached different stages of the news attention cycle. As the movement\r\ngrew, journalists focused on the movement’s economic grievances,\r\nincluding economic inequality, bank bailouts, and foreclosures. As the\r\nmovement peaked, news attention shifted to the intensifying conflict\r\nbetween city officials and protesters.\r\nDiakopoulos (2015)\r\nDiakopoulos, N. A. (2015). The Editor’s Eye: Curation and Comment\r\nRelevance on the New York Times. Proceedings of the 18th ACM Conference\r\non Computer Supported Cooperative Work & Social Computing,\r\n1153–1157. https://doi.org/10.1145/2675133.2675160\r\nAbstract\r\nThe journalistic curation of social media content from platforms like\r\nFacebook and YouTube or from commenting systems is underscored by an\r\nimperative for publishing accurate and quality content. This work\r\nexplores the manifestation of editorial quality criteria in comments\r\nthat have been curated and selected on the New York Times website as\r\n“NYT Picks.” The relationship between comment selection and comment\r\nrelevance is examined through the analysis of 331,785 comments,\r\nincluding 12,542 editor’s selections. A robust association between\r\neditorial selection and article relevance or conversational relevance\r\nwas found. The results are discussed in terms of their implications for\r\nreducing journalistic curatorial work load, or scaling the ability to\r\nexamine more comments for editorial selection , as well as how end-user\r\ncommenting experiences might be improved.\r\nGrimmer & Stewart (2013)\r\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The\r\nPromise and Pitfalls of Automatic Content Analysis Methods for Political\r\nTexts. Political Analysis, 21(3), 267–297.\r\nAbstract\r\nPolitics and political conflict often occur in the written and spoken\r\nword. Scholars have long recognized this, but the massive costs of\r\nanalyzing even moderately sized collections of texts have hindered their\r\nuse in political science research. Here lies the promise of automated\r\ntext analysis: it substantially reduces the costs of analyzing large\r\ncollections of text. We provide a guide to this exciting new area of\r\nresearch and show how, in many instances, the methods have already\r\nobtained part of their promise. But there are pitfalls to using\r\nautomated methods–they are no substitute for careful thought and close\r\nreading and require extensive and problem-specific validation. We survey\r\na wide range of new methods, provide guidance on how to validate the\r\noutput of the models, and clarify misconceptions and errors in the\r\nliterature. To conclude, we argue that for automated text methods to\r\nbecome a standard tool for political scientists, methodologists must\r\ncontribute new methods and new methods of validation.\r\nKothari (2010)\r\nKothari, A. (2010). The Framing of the Darfur Conflict in the New\r\nYork Times: 2003–2006. Journalism Studies, 11(2), 209–224. https://doi.org/10.1080/14616700903481978\r\nAbstract\r\nThis multi-method study examines how the New York Times reported the\r\nDarfur conflict in the Sudan, which has led to an estimated 300,000\r\ndeaths and over 2.3 million people displaced by the fighting. Drawing on\r\nnormative media theories and prior studies of Africa’s representation,\r\nthe role of sources in the frame-building process was analyzed, together\r\nwith the impact of news-making processes on journalists’ reporting about\r\nDarfur. The textual analysis largely supports results of prior studies\r\non news framing of Africa. However, interviews with four New York Times\r\njournalists reveal that the individual biases and motives of the\r\njournalists and their sources significantly influenced the coverage.\r\nWhile the journalists participated in news-making processes\r\ndistinguishable by journalist goal, source availability, and source\r\ncredibility, their sources also provided information that reinforced\r\ncertain media frames.\r\nKiousis (2004)\r\nKiousis, S. (2004). Explicating Media Salience: A Factor Analysis\r\nof New York Times Issue Coverage During the 2000 U.S. Presidential\r\nElection. Journal of Communication, 54(1), 71–87. https://doi.org/10.1111/j.1460-2466.2004.tb02614.x\r\nAbstract\r\nMedia salience—the key independent variable in agenda-setting\r\nresearch—has traditionally been explicated as a singular construct.\r\nNevertheless, scholars have defined and measured it using a number of\r\ndifferent conceptualizations and empirical indicators. To address this\r\nlimitation in research, this study introduced a conceptual model of\r\nmedia salience, suggesting it is a multidimensional construct consisting\r\nof 3 core elements: attention, prominence, and valence. Furthermore, the\r\nmodel was tested through an exploratory factor analysis of The New York\r\nTimes news coverage of 8 major political issues during the 2000\r\npresidential election as a case study. The data revealed that 2\r\ndimensions of media salience emerge: visibility and valence. Based on\r\nthe factor analysis, 2 indices are created to measure the construct,\r\nwhich are intended for use in future investigations.\r\nAlthaus & Tewksbury (2002)\r\nAlthaus, S. L., & Tewksbury, D. (2002). Agenda Setting and\r\nthe “New” News: Patterns of Issue Importance Among Readers of the Paper\r\nand Online Versions of the New York Times. Communication Research,\r\n29(2), 180–207. https://doi.org/10.1177/0093650202029002004\r\nAbstract\r\nThis study examines whether readers of the paper and online versions\r\nof a national newspaper acquire different perceptions of the importance\r\nof political issues. Using data from a weeklong experiment in which\r\nsubjects either readthe print version of the New York Times, the online\r\nversion of that paper, or received no special exposure, this study finds\r\nevidence that people exposed to the Times for 5 days adjusted their\r\nagendas in response to that exposure and that print readers modified\r\ntheir agendas differently than did online readers.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-23T13:53:41-05:00",
    "input_file": "lit-review.knit.md"
  },
  {
    "path": "posts/welcome/",
    "title": "Analysis of New York Times Headlines",
    "description": "This is the project page for the analysis of differences between main and print headlines for New York Times articles published surrounding the U.S. withdrawal of the military in Afghanistan.",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/Grateful-Network/"
      }
    ],
    "date": "2022-04-13",
    "categories": [],
    "contents": "\r\n\r\nFor this project, I am using some data gathered in the DACSS 602\r\ncourse “Research Design”.\r\nI continued down the same path but with new data and a new direction\r\nthrough the DACSS 697D course “Text as Data”.\r\nMore background data can be found in this series of posts from my academic blog.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-23T00:25:35-05:00",
    "input_file": {}
  }
]
