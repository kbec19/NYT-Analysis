[
  {
    "path": "posts/topic-modeling/",
    "title": "Topic Modeling",
    "description": "Text as Data Project: Headline Topic Modeling",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/NYT-Analysis/"
      }
    ],
    "date": "2022-04-28",
    "categories": [
      "text as data",
      "NYT text analysis project"
    ],
    "contents": "\r\n\r\nContents\r\nPrepare for Topic\r\nModeling Analysis\r\nCreate Corpus\r\nSpecial Function\r\nPre-Processing\r\n\r\nCreate Document Term\r\nMatrix\r\nSample Headlines\r\n\r\nGibbs Method\r\nDetermine K\r\nChoose K\r\nCompute Model at K(5)\r\nModel Details\r\nModel Preview\r\nVisualization of\r\nTopic Distributions\r\nTopic Ranking\r\nWordcloud of Topic\r\n\r\nTopic Distribution\r\nAdjustment\r\nModel Details\r\nModel Preview\r\nVisualization of\r\nNew Distribution Alpha\r\nTopic Ranking\r\nWordcloud of New Topic\r\nModel\r\n\r\nFiltering Documents by\r\n“Withdrawal”\r\nVisualizing Topic\r\nProportions Over Time\r\n\r\n\r\nVEM Method\r\nDetermine K\r\nChoose K\r\nCompute Model at K(3)\r\nModel Details\r\nModel Preview\r\nVisualization of\r\nTopic Distributions\r\nTopic Ranking\r\nWordcloud of Topic\r\n\r\nTopic Distribution\r\nAdjustment\r\nModel Details\r\nModel Preview\r\nVisualization\r\nof New Distribution Alpha\r\nTopic Ranking\r\nWordcloud of Topic\r\n\r\nFiltering Documents\r\nby “Withdrawal”\r\nVisualizing\r\nTopic Proportions Over Time\r\n\r\nVisualizing Topic\r\nProportions By Article (Dated)\r\nGibbs Method\r\nVEM Method\r\n\r\n\r\n\r\nPrepare for Topic Modeling\r\nAnalysis\r\n\r\n\r\nShow code\r\n\r\n#no automatic data transformation\r\noptions(stringsAsFactors = F)  \r\n#supress math annotation\r\noptions(\"scipen\" = 100, \"digits\" = 4)\r\n#load packages\r\nlibrary(knitr) \r\nlibrary(kableExtra) \r\nlibrary(DT)\r\nlibrary(tm)\r\nlibrary(topicmodels)\r\nlibrary(reshape2)\r\nlibrary(ggplot2)\r\nlibrary(wordcloud)\r\nlibrary(pals)\r\nlibrary(SnowballC)\r\nlibrary(lda)\r\nlibrary(ldatuning)\r\nlibrary(flextable)\r\nlibrary(tidyverse)\r\n\r\n\r\n\r\nI am using code from a wonderfully helpful tutorial to do some\r\nexploratory topic modeling with the headline data.\r\nSchweinberger, Martin. 2022. Topic Modeling with R. Brisbane: The\r\nUniversity of Queensland. url: https://slcladal.github.io/topicmodels.html (Version\r\n2022.03.18).\r\nCreate Corpus\r\nLoading the entirety of the headlines pulled from the New York Times\r\nAPI, I will pre-process and create a corpus object.\r\nI am transforming to lower case, removing English stopwords, removing\r\npunctuation, numbers, and stripping white space. I am going to use\r\nstemming, to begin the analysis, though I may go back and change\r\nthis.\r\nAdded on after running analyses in this post\r\nSpecial Function\r\nI had to create a function to remove ‘curly’ apostrophes after they\r\nshowed up as top results in 2 models\r\n\r\n\r\nShow code\r\n\r\n#creating functions using gsub to remove each of the curly apostrophes\r\nexchanger1 <- function(x) gsub(\"’\", \"\", x)\r\nexchanger2 <- function(x) gsub(\"‘\", \"\", x)\r\n\r\n#corpus <- tm_map(corpus, exchanger1)\r\n#corpus <- tm_map(corpus, exchanger2)\r\n\r\n\r\n\r\nPre-Processing\r\n\r\n\r\nShow code\r\n\r\n#load data\r\ntextdata <- read.csv(\"all_headlines.csv\")\r\n#load stop words\r\nenglish_stopwords <- readLines(\"https://slcladal.github.io/resources/stopwords_en.txt\", encoding = \"UTF-8\")\r\n#create corpus object\r\ncorpus <- Corpus(DataframeSource(textdata))\r\n#pre-processing chain\r\nprocessedCorpus <- tm_map(corpus, content_transformer(tolower))\r\nprocessedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)\r\nprocessedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)\r\nprocessedCorpus <- tm_map(processedCorpus, removeNumbers)\r\nprocessedCorpus <- tm_map(processedCorpus, stripWhitespace)\r\nprocessedCorpus <- tm_map(processedCorpus, exchanger1)\r\nprocessedCorpus <- tm_map(processedCorpus, exchanger2)\r\n\r\n\r\n\r\nCreate Document Term Matrix\r\nThe “topicmodels” package requires a Document Term Matrix.\r\n\r\n\r\nShow code\r\n\r\nminimumFrequency <- 1\r\nDTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))\r\n#preview\r\ndim(DTM)\r\n\r\n\r\n[1] 1872 3422\r\n\r\nI need to clean up the matrix to remove empty rows due to the\r\nvocabulary being stemmed/stop words being removed.\r\n\r\n\r\nShow code\r\n\r\nsel_idx <- slam::row_sums(DTM) > 0\r\nDTM <- DTM[sel_idx, ]\r\ntextdata <- textdata[sel_idx, ]\r\n\r\n\r\n\r\nSample Headlines\r\nChoose 5 random numbers to pull and use as examples when examining\r\nmodels.\r\n\r\n\r\nShow code\r\n\r\nexampleIds <- c(180, 250, 330, 430, 510)\r\nlapply(corpus[exampleIds], as.character)\r\n\r\n\r\n$`180`\r\n[1] \"Afghanistan to Release Last Taliban Prisoners, Removing Final Hurdle to Talks\"\r\n\r\n$`250`\r\n[1] \"Trump’s Campaign Talk of Troop Withdrawals Doesn’t Match Military Reality\"\r\n\r\n$`330`\r\n[1] \"U.S. Leaves Behind Afghan Bases — and a Legacy of Land Disputes\"\r\n\r\n$`430`\r\n[1] \"Blinken, Without Leaving Home, Tries to Mend Fences With Allies Abroad\"\r\n\r\n$`510`\r\n[1] \"War, Peace and Taliban Spreadsheets\"\r\n\r\nGibbs Method\r\nNow I can create models to examine the metrics that can lead to\r\nchoosing the optical number of topics. Since I am unsure about whether\r\nthe VEM or Gibbs method will be most appropriate I’m going to try\r\nboth.\r\nDetermine K\r\n\r\n\r\nShow code\r\n\r\n#create models with different number of topics\r\nresult <- ldatuning::FindTopicsNumber(\r\n  DTM,\r\n  topics = seq(from = 2, to = 20, by = 1),\r\n  metrics = c(\"CaoJuan2009\",  \"Deveaud2014\", \"Arun2010\", \"Griffiths2004\"),\r\n  method = \"Gibbs\",\r\n  control = list(seed = 11),\r\n  verbose = TRUE\r\n)\r\n\r\n\r\nfit models... done.\r\ncalculate metrics:\r\n  CaoJuan2009... done.\r\n  Deveaud2014... done.\r\n  Arun2010... done.\r\n  Griffiths2004... done.\r\n\r\nChoose K\r\nAfter inspecting the results of all four metrics from the “ldatuning”\r\npackage relevant to the Gibbs method, it seems that a good starting\r\npoint will be 5 topics, or “K”.\r\n\r\n\r\nShow code\r\n\r\nFindTopicsNumber_plot(result)\r\n\r\n\r\n\r\n\r\nCompute Model at K(5)\r\nI’ll set the topic model to run 1,000 iterations\r\n\r\n\r\nShow code\r\n\r\n#number of topics\r\nK <- 5\r\n#set random number generator seed\r\nset.seed(123)\r\n#compute the LDA model, inference via 1000 iterations of Gibbs sampling\r\ntopicModel <- LDA(DTM, K, method=\"Gibbs\", control=list(iter = 1000, verbose = 25, alpha = 1.0))\r\n\r\n\r\nK = 5; V = 3422; M = 1872\r\nSampling 1000 iterations!\r\nIteration 25 ...\r\nIteration 50 ...\r\nIteration 75 ...\r\nIteration 100 ...\r\nIteration 125 ...\r\nIteration 150 ...\r\nIteration 175 ...\r\nIteration 200 ...\r\nIteration 225 ...\r\nIteration 250 ...\r\nIteration 275 ...\r\nIteration 300 ...\r\nIteration 325 ...\r\nIteration 350 ...\r\nIteration 375 ...\r\nIteration 400 ...\r\nIteration 425 ...\r\nIteration 450 ...\r\nIteration 475 ...\r\nIteration 500 ...\r\nIteration 525 ...\r\nIteration 550 ...\r\nIteration 575 ...\r\nIteration 600 ...\r\nIteration 625 ...\r\nIteration 650 ...\r\nIteration 675 ...\r\nIteration 700 ...\r\nIteration 725 ...\r\nIteration 750 ...\r\nIteration 775 ...\r\nIteration 800 ...\r\nIteration 825 ...\r\nIteration 850 ...\r\nIteration 875 ...\r\nIteration 900 ...\r\nIteration 925 ...\r\nIteration 950 ...\r\nIteration 975 ...\r\nIteration 1000 ...\r\nGibbs sampling completed!\r\n\r\nModel Details\r\n\r\n\r\nShow code\r\n\r\n#look at posterior distributions\r\ntmResult <- posterior(topicModel)\r\n#format of the resulting object\r\nattributes(tmResult)\r\n\r\n\r\n$names\r\n[1] \"terms\"  \"topics\"\r\n\r\nShow code\r\n\r\n#lengthOfVocab\r\nnTerms(DTM)              \r\n\r\n\r\n[1] 3422\r\n\r\nShow code\r\n\r\n#topics are probability distributions over the entire vocabulary\r\n#get beta from results\r\nbeta <- tmResult$terms \r\n#K distributions over nTerms(DTM) terms\r\ndim(beta)                \r\n\r\n\r\n[1]    5 3422\r\n\r\nShow code\r\n\r\n#rows in beta sum to 1\r\nrowSums(beta)          \r\n\r\n\r\n1 2 3 4 5 \r\n1 1 1 1 1 \r\n\r\nShow code\r\n\r\n#size of collection\r\nnDocs(DTM)             \r\n\r\n\r\n[1] 1872\r\n\r\nShow code\r\n\r\n#for every document we have a probability distribution of its contained topics\r\ntheta <- tmResult$topics \r\n#nDocs(DTM) distributions over K topics\r\ndim(theta)\r\n\r\n\r\n[1] 1872    5\r\n\r\nShow code\r\n\r\n#rows in theta sum to 1\r\nrowSums(theta)[1:10] \r\n\r\n\r\n 1  2  3  4  5  6  7  8  9 10 \r\n 1  1  1  1  1  1  1  1  1  1 \r\n\r\nModel Preview\r\nNow I can look at the 10 most likely terms within the probabilities\r\nof the inferred topics. I’ll take a look at them for each of the 5\r\ntopics to get a clearer idea of how the topics are represented in this\r\nmodel.\r\n\r\n\r\nShow code\r\n\r\nterms(topicModel, 10)\r\n\r\n\r\n      Topic 1        Topic 2    Topic 3       Topic 4      \r\n [1,] \"trump\"        \"biden\"    \"taliban\"     \"afghanistan\"\r\n [2,] \"military\"     \"china\"    \"afghan\"      \"war\"        \r\n [3,] \"house\"        \"world\"    \"afghans\"     \"afghan\"     \r\n [4,] \"democrats\"    \"migrants\" \"peace\"       \"biden\"      \r\n [5,] \"bounties\"     \"home\"     \"women\"       \"kabul\"      \r\n [6,] \"russia\"       \"refugees\" \"afghanistan\" \"exit\"       \r\n [7,] \"russian\"      \"allies\"   \"talks\"       \"withdrawal\" \r\n [8,] \"top\"          \"takes\"    \"kabul\"       \"troops\"     \r\n [9,] \"general\"      \"pandemic\" \"forces\"      \"end\"        \r\n[10,] \"intelligence\" \"faces\"    \"deal\"        \"veterans\"   \r\n      Topic 5        \r\n [1,] \"pentagon\"     \r\n [2,] \"leader\"       \r\n [3,] \"pakistan\"     \r\n [4,] \"president\"    \r\n [5,] \"killed\"       \r\n [6,] \"troops\"       \r\n [7,] \"iran\"         \r\n [8,] \"attack\"       \r\n [9,] \"guant<e1>namo\"\r\n[10,] \"defense\"      \r\n\r\nVisualization of Topic\r\nDistributions\r\nTo look at the models more easily, I’ll name the strings with the top\r\n5 most likely terms for each topic.\r\n\r\n\r\nShow code\r\n\r\ntop5termsPerTopic <- terms(topicModel, 5)\r\ntopicNames <- apply(top5termsPerTopic, 2, paste, collapse=\" \")\r\n\r\n\r\n\r\nAfter looking into the documents, I can visualize the topic\r\ndistributions within the documents.\r\n\r\n\r\nShow code\r\n\r\nN <- length(exampleIds)\r\n# get topic proportions form example documents\r\ntopicProportionExamples <- theta[exampleIds,]\r\ncolnames(topicProportionExamples) <- topicNames\r\nvizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = \"topic\", id.vars = \"document\")  \r\nggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = \"proportion\") + \r\n  geom_bar(stat=\"identity\") +\r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  \r\n  coord_flip() +\r\n  facet_wrap(~ document, ncol = N)\r\n\r\n\r\n\r\n\r\nTopic Ranking\r\nI will try to get a more meaningful order of top terms per topic by\r\nre-ranking them with a specific score (Chang et al. 2009).\r\nThe idea of re-ranking terms is similar to the idea of TF-IDF. The\r\nmore a term appears in top levels with regard to its probability, the\r\nless meaningful it is to describe the topic. Hence, the scoring advanced\r\nfavors terms to describe a topic.\r\n\r\n\r\nShow code\r\n\r\n# re-rank top topic terms for topic names\r\ntopicNames <- apply(lda::top.topic.words(beta, 5, by.score = T), 2, paste, collapse = \" \")\r\n\r\n\r\n\r\nApproach 1\r\nSort topics according to their probability within the entire\r\ncollection:\r\n\r\n\r\nShow code\r\n\r\n#mean probablities over all paragraphs\r\ntopicProportions <- colSums(theta) / nDocs(DTM)  \r\n#assign the topic names we created before\r\nnames(topicProportions) <- topicNames     \r\n#show summed proportions in decreased order\r\nsort(topicProportions, decreasing = TRUE) \r\n\r\n\r\n     taliban afghan afghans peace women \r\n                                 0.2084 \r\n      afghanistan war afghan kabul exit \r\n                                 0.2043 \r\n   pentagon leader pakistan killed iran \r\n                                 0.1981 \r\ntrump military house democrats bounties \r\n                                 0.1961 \r\n    biden china world refugees migrants \r\n                                 0.1932 \r\n\r\nApproach 2\r\nWe count how often a topic appears as a primary topic within a\r\nparagraph This method is also called Rank-1.\r\n\r\n\r\nShow code\r\n\r\ncountsOfPrimaryTopics <- rep(0, K)\r\nnames(countsOfPrimaryTopics) <- topicNames\r\nfor (i in 1:nDocs(DTM)) {\r\n  topicsPerDoc <- theta[i, ] # select topic distribution for document i\r\n  # get first element position from ordered list\r\n  primaryTopic <- order(topicsPerDoc, decreasing = TRUE)[1] \r\n  countsOfPrimaryTopics[primaryTopic] <- countsOfPrimaryTopics[primaryTopic] + 1\r\n}\r\nsort(countsOfPrimaryTopics, decreasing = TRUE)\r\n\r\n\r\n     taliban afghan afghans peace women \r\n                                    431 \r\ntrump military house democrats bounties \r\n                                    412 \r\n    biden china world refugees migrants \r\n                                    380 \r\n      afghanistan war afghan kabul exit \r\n                                    349 \r\n   pentagon leader pakistan killed iran \r\n                                    300 \r\n\r\nSorting topics by the Rank-1 method places topics with rather\r\nspecific thematic coherences in upper ranks of the list.\r\nWordcloud of Topic\r\nThe wordcloud is a good preliminary way to look at the topics. I’ll\r\nchoose topic “3”, because across the five sample documents, topic “3”\r\nseems to be the most broad capture of the headline topic prevalence over\r\nthe time period in the modeling. It is also intuitively not an\r\nunreasonable choice.\r\n\r\n\r\nShow code\r\n\r\n# visualize topics as word cloud\r\ntopicToViz <- 3 # change for your own topic of interest\r\n#topicToViz <- grep('fear', topicNames)[1] # Or select a topic by a term contained in its name\r\n# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order\r\ntop40terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]\r\nwords <- names(top40terms)\r\n# extract the probabilites of each of the 40 terms\r\nprobabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]\r\n# visualize the terms as wordcloud\r\nmycolors <- brewer.pal(8, \"Dark2\")\r\nwordcloud(words, probabilities, random.order = FALSE, color = mycolors)\r\n\r\n\r\n\r\n\r\nTopic Distribution\r\nAdjustment\r\nChanging the alpha to a lower level\r\nFor alpha values greater than one, the samples start to congregate in\r\nthe center of the triangle. This means that as alpha gets bigger, your\r\nsamples will more likely be uniform — that is, represent an even mixture\r\nof all the topics. Since that was definitely the case in the first\r\nsample, I will lower it from 1.0 to 0.1.\r\n\r\n\r\nShow code\r\n\r\n#see alpha from previous model\r\nattr(topicModel, \"alpha\") \r\n\r\n\r\n[1] 1\r\n\r\nShow code\r\n\r\n#prior alpha was 1.0\r\n\r\ntopicModelb <- LDA(DTM, K, method=\"Gibbs\", control=list(iter = 1000, verbose = 25, alpha = 0.1))\r\n\r\n\r\nK = 5; V = 3422; M = 1872\r\nSampling 1000 iterations!\r\nIteration 25 ...\r\nIteration 50 ...\r\nIteration 75 ...\r\nIteration 100 ...\r\nIteration 125 ...\r\nIteration 150 ...\r\nIteration 175 ...\r\nIteration 200 ...\r\nIteration 225 ...\r\nIteration 250 ...\r\nIteration 275 ...\r\nIteration 300 ...\r\nIteration 325 ...\r\nIteration 350 ...\r\nIteration 375 ...\r\nIteration 400 ...\r\nIteration 425 ...\r\nIteration 450 ...\r\nIteration 475 ...\r\nIteration 500 ...\r\nIteration 525 ...\r\nIteration 550 ...\r\nIteration 575 ...\r\nIteration 600 ...\r\nIteration 625 ...\r\nIteration 650 ...\r\nIteration 675 ...\r\nIteration 700 ...\r\nIteration 725 ...\r\nIteration 750 ...\r\nIteration 775 ...\r\nIteration 800 ...\r\nIteration 825 ...\r\nIteration 850 ...\r\nIteration 875 ...\r\nIteration 900 ...\r\nIteration 925 ...\r\nIteration 950 ...\r\nIteration 975 ...\r\nIteration 1000 ...\r\nGibbs sampling completed!\r\n\r\nModel Details\r\n\r\n\r\nShow code\r\n\r\n#look at posterior distributions\r\ntmResultb <- posterior(topicModelb)\r\n#format of the resulting object\r\nattributes(tmResultb)\r\n\r\n\r\n$names\r\n[1] \"terms\"  \"topics\"\r\n\r\nShow code\r\n\r\n#topics are probability distributions over the entire vocabulary\r\n#get beta from results\r\nbetab <- tmResultb$terms \r\n#K distributions over nTerms(DTM) terms\r\ndim(betab)                \r\n\r\n\r\n[1]    5 3422\r\n\r\nShow code\r\n\r\n#rows in beta sum to 1\r\nrowSums(betab)          \r\n\r\n\r\n1 2 3 4 5 \r\n1 1 1 1 1 \r\n\r\nShow code\r\n\r\n#for every document we have a probability distribution of its contained topics\r\nthetab <- tmResultb$topics \r\n#nDocs(DTM) distributions over K topics\r\ndim(thetab)\r\n\r\n\r\n[1] 1872    5\r\n\r\nShow code\r\n\r\n#rows in theta sum to 1\r\nrowSums(thetab)[1:10] \r\n\r\n\r\n 1  2  3  4  5  6  7  8  9 10 \r\n 1  1  1  1  1  1  1  1  1  1 \r\n\r\nModel Preview\r\nNow I can look at the 10 most likely terms within the probabilities\r\nof the inferred topics. I’ll take a look at them for each of the 3\r\ntopics to get a clearer idea of how the topics are represented in this\r\nmodel.\r\n\r\n\r\nShow code\r\n\r\nterms(topicModelb, 10)\r\n\r\n\r\n      Topic 1     Topic 2       Topic 3       Topic 4      \r\n [1,] \"trump\"     \"troops\"      \"biden\"       \"afghanistan\"\r\n [2,] \"biden\"     \"afghanistan\" \"war\"         \"biden\"      \r\n [3,] \"military\"  \"afghan\"      \"afghanistan\" \"trump\"      \r\n [4,] \"house\"     \"military\"    \"afghan\"      \"world\"      \r\n [5,] \"iran\"      \"pentagon\"    \"exit\"        \"afghan\"     \r\n [6,] \"president\" \"trump\"       \"veterans\"    \"aid\"        \r\n [7,] \"democrats\" \"kabul\"       \"migrants\"    \"power\"      \r\n [8,] \"russia\"    \"war\"         \"end\"         \"policy\"     \r\n [9,] \"russian\"   \"years\"       \"withdrawal\"  \"india\"      \r\n[10,] \"bounties\"  \"drone\"       \"faces\"       \"takes\"      \r\n      Topic 5      \r\n [1,] \"taliban\"    \r\n [2,] \"afghan\"     \r\n [3,] \"afghanistan\"\r\n [4,] \"kabul\"      \r\n [5,] \"peace\"      \r\n [6,] \"afghans\"    \r\n [7,] \"women\"      \r\n [8,] \"talks\"      \r\n [9,] \"deal\"       \r\n[10,] \"attack\"     \r\n\r\nVisualization of New\r\nDistribution Alpha\r\n\r\n\r\nShow code\r\n\r\ntmResultb <- posterior(topicModelb)\r\nthetab <- tmResultb$topics\r\nbetab <- tmResultb$terms\r\n#reset topicnames\r\ntopicNamesb <- apply(terms(topicModelb, 5), 2, paste, collapse = \" \")  \r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# get topic proportions form example documents\r\ntopicProportionExamplesb <- thetab[exampleIds,]\r\ncolnames(topicProportionExamplesb) <- topicNamesb\r\nvizDataFrameb <- melt(cbind(data.frame(topicProportionExamplesb), document = factor(1:N)), variable.name = \"topic\", id.vars = \"document\")  \r\nggplot(data = vizDataFrameb, aes(topic, value, fill = document), ylab = \"proportion\") + \r\n  geom_bar(stat=\"identity\") +\r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  \r\n  coord_flip() +\r\n  facet_wrap(~ document, ncol = N)\r\n\r\n\r\n\r\n\r\nTopic Ranking\r\nI will try to get a more meaningful order of top terms per topic by\r\nre-ranking them with a specific score (Chang et al. 2009).\r\nThe idea of re-ranking terms is similar to the idea of TF-IDF. The\r\nmore a term appears in top levels with regard to its probability, the\r\nless meaningful it is to describe the topic. Hence, the scoring advanced\r\nfavors terms to describe a topic.\r\n\r\n\r\nShow code\r\n\r\n# re-rank top topic terms for topic names\r\ntopicNamesb <- apply(lda::top.topic.words(betab, 5, by.score = T), 2, paste, collapse = \" \")\r\n\r\n\r\n\r\nApproach 1\r\nSort topics according to their probability within the entire\r\ncollection:\r\n\r\n\r\nShow code\r\n\r\n#mean probablities over all paragraphs\r\ntopicProportionsb <- colSums(thetab) / nDocs(DTM)  \r\n#assign the topic names we created before\r\nnames(topicProportionsb) <- topicNamesb    \r\n#show summed proportions in decreased order\r\nsort(topicProportionsb, decreasing = TRUE) \r\n\r\n\r\ntaliban afghan peace kabul afghanistan \r\n                                0.2432 \r\n   biden war veterans afghanistan exit \r\n                                0.1992 \r\n  military pentagon troops kabul drone \r\n                                0.1990 \r\n       trump house military biden iran \r\n                                0.1807 \r\n          world biden trump policy aid \r\n                                0.1779 \r\n\r\nApproach 2\r\nWe count how often a topic appears as a primary topic within a\r\nparagraph This method is also called Rank-1.\r\n\r\n\r\nShow code\r\n\r\ncountsOfPrimaryTopicsb <- rep(0, K)\r\nnames(countsOfPrimaryTopicsb) <- topicNamesb\r\nfor (i in 1:nDocs(DTM)) {\r\n  topicsPerDocb <- thetab[i, ] # select topic distribution for document i\r\n  # get first element position from ordered list\r\n  primaryTopicb <- order(topicsPerDocb, decreasing = TRUE)[1] \r\n  countsOfPrimaryTopicsb[primaryTopicb] <- countsOfPrimaryTopicsb[primaryTopicb] + 1\r\n}\r\nsort(countsOfPrimaryTopicsb, decreasing = TRUE)\r\n\r\n\r\ntaliban afghan peace kabul afghanistan \r\n                                   455 \r\n  military pentagon troops kabul drone \r\n                                   388 \r\n   biden war veterans afghanistan exit \r\n                                   367 \r\n       trump house military biden iran \r\n                                   346 \r\n          world biden trump policy aid \r\n                                   316 \r\n\r\nSorting topics by the Rank-1 method places topics with rather\r\nspecific thematic coherences in upper ranks of the list.\r\nWordcloud of New Topic Model\r\nThe wordcloud is a good preliminary way to look at the topics. It is\r\nclearly more difficult to know the overall topic model prevalence with\r\nthe lower alpha and its more granular results in the visualization, but\r\nusing the ranking I’ll look at “4”.\r\n\r\n\r\nShow code\r\n\r\n# visualize topics as word cloud\r\ntopicToVizb <- 4 # change for your own topic of interest\r\n#topicToViz <- grep('fear', topicNames)[1] # Or select a topic by a term contained in its name\r\n# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order\r\ntop40termsb <- sort(tmResultb$terms[topicToVizb,], decreasing=TRUE)[1:40]\r\nwordsb <- names(top40termsb)\r\n# extract the probabilites of each of the 40 terms\r\nprobabilitiesb <- sort(tmResultb$terms[topicToVizb,], decreasing=TRUE)[1:40]\r\n# visualize the terms as wordcloud\r\nmycolors <- brewer.pal(8, \"Dark2\")\r\nwordcloud(wordsb, probabilitiesb, random.order = FALSE, color = mycolors)\r\n\r\n\r\n\r\n\r\nFiltering Documents by\r\n“Withdrawal”\r\nThe fact that a topic model conveys of topic probabilities for each\r\ndocument, resp. paragraph in our case, makes it possible to use it for\r\nthematic filtering of a collection. AS filter we select only those\r\ndocuments which exceed a certain threshold of their probability value\r\nfor certain topics (for example, each document which contains topic 4 to\r\nmore than 20 percent).\r\n\r\n\r\nShow code\r\n\r\ntopicToFilterb <- 4  # you can set this manually ...\r\n# ... or have it selected by a term in the topic name (e.g. 'children')\r\n#topicToFilterb <- grep('withdrawal', topicNamesb)[1] \r\ntopicThresholdb <- 0.2\r\nselectedDocumentIndexesb <- which(thetab[, topicToFilterb] >= topicThresholdb)\r\nfilteredCorpusb <- corpus[selectedDocumentIndexesb]\r\n# show length of filtered corpus\r\nfilteredCorpusb\r\n\r\n\r\n<<SimpleCorpus>>\r\nMetadata:  corpus specific: 1, document level (indexed): 2\r\nContent:  documents: 468\r\n\r\nVisualizing Topic\r\nProportions Over Time\r\nIn a last step, we provide a distant view on the topics in the data\r\nover time. For this, we aggregate mean topic proportions per month for\r\nall of the topics. These aggregated topic proportions can then be\r\nvisualized, e.g. as a bar plot.\r\n\r\n\r\nShow code\r\n\r\n#convert non-graph characters to combat error in grid.Call\r\ntopicNamesb=str_replace_all(topicNamesb,\"[^[:graph:]]\", \" \") \r\n# append month information for aggregation\r\ntextdata$month <- paste0(substr(textdata$month.ended, 0, 3), \"0\")\r\n# get mean topic proportions per month\r\ntopic_proportion_per_month <- aggregate(thetab, by = list(month = textdata$month), mean)\r\n# set topic names to aggregated columns\r\ncolnames(topic_proportion_per_month)[2:(K+1)] <- topicNamesb\r\n# reshape data frame\r\nvizDataFrame <- melt(topic_proportion_per_month, id.vars = \"month\")\r\n# plot topic proportions per month as bar plot\r\nggplot(vizDataFrame, aes(x=month, y=value, fill=variable)) + \r\n  geom_bar(stat = \"identity\") + ylab(\"proportion\") + \r\n  scale_fill_manual(values = paste0(alphabet(20), \"FF\"), name = \"topic\") + \r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\r\n\r\n\r\n\r\n\r\nVEM Method\r\nDetermine K\r\nThe VEM method gives 3 metrics; the “Griffiths2004” metric is not\r\ncompatible with this method.\r\n\r\n\r\nShow code\r\n\r\n#create models with different number of topics\r\nresult2 <- ldatuning::FindTopicsNumber(\r\n  DTM,\r\n  topics = seq(from = 2, to = 20, by = 1),\r\n  metrics = c(\"CaoJuan2009\",  \"Deveaud2014\", \"Arun2010\", \"Griffiths2004\"),\r\n  method = \"VEM\",\r\n  control = list(seed = 11),\r\n  verbose = TRUE\r\n)\r\n\r\n\r\n'Griffiths2004' is incompatible with 'VEM' method, excluded.\r\nfit models... done.\r\ncalculate metrics:\r\n  CaoJuan2009... done.\r\n  Deveaud2014... done.\r\n  Arun2010... done.\r\n\r\nChoose K\r\nAfter inspecting the results of all three metrics from the\r\n“ldatuning” package relevant to the VEM method, it seems that a good\r\nstarting point will be 3 topics, or “K”.\r\n\r\n\r\nShow code\r\n\r\nFindTopicsNumber_plot(result2)\r\n\r\n\r\n\r\n\r\nCompute Model at K(3)\r\nUsing the VEM model, the alpha is automatically generated\r\n\r\n\r\nShow code\r\n\r\n#number of topics\r\nK2 <- 3\r\n#set random number generator seed\r\nset.seed(11)\r\n#compute the LDA model via VEM method\r\ntopicModel2 <- LDA(DTM, K2, method=\"VEM\", control=list(alpha = 1.0))\r\n\r\n\r\n\r\nModel Details\r\n\r\n\r\nShow code\r\n\r\n#look at posterior distributions\r\ntmResult2 <- posterior(topicModel2)\r\n#format of the resulting object\r\nattributes(tmResult2)\r\n\r\n\r\n$names\r\n[1] \"terms\"  \"topics\"\r\n\r\nShow code\r\n\r\n#topics are probability distributions over the entire vocabulary\r\n#get beta from results\r\nbeta2 <- tmResult2$terms \r\n#K distributions over nTerms(DTM) terms\r\ndim(beta2)                \r\n\r\n\r\n[1]    3 3422\r\n\r\nShow code\r\n\r\n#rows in beta sum to 1\r\nrowSums(beta2)          \r\n\r\n\r\n1 2 3 \r\n1 1 1 \r\n\r\nShow code\r\n\r\n#for every document we have a probability distribution of its contained topics\r\ntheta2 <- tmResult2$topics \r\n#nDocs(DTM) distributions over K topics\r\ndim(theta2)\r\n\r\n\r\n[1] 1872    3\r\n\r\nShow code\r\n\r\n#rows in theta sum to 1\r\nrowSums(theta2)[1:10] \r\n\r\n\r\n 1  2  3  4  5  6  7  8  9 10 \r\n 1  1  1  1  1  1  1  1  1  1 \r\n\r\nModel Preview\r\nNow I can look at the 10 most likely terms within the probabilities\r\nof the inferred topics. I’ll take a look at them for each of the 5\r\ntopics to get a clearer idea of how the topics are represented in this\r\nmodel.\r\n\r\n\r\nShow code\r\n\r\nterms(topicModel2, 10)\r\n\r\n\r\n      Topic 1         Topic 2       Topic 3      \r\n [1,] \"biden\"         \"afghan\"      \"afghanistan\"\r\n [2,] \"military\"      \"trump\"       \"taliban\"    \r\n [3,] \"trump\"         \"kabul\"       \"afghan\"     \r\n [4,] \"house\"         \"taliban\"     \"biden\"      \r\n [5,] \"war\"           \"afghanistan\" \"war\"        \r\n [6,] \"world\"         \"troops\"      \"afghans\"    \r\n [7,] \"iran\"          \"military\"    \"peace\"      \r\n [8,] \"guant<e1>namo\" \"attack\"      \"pakistan\"   \r\n [9,] \"democrats\"     \"pentagon\"    \"withdrawal\" \r\n[10,] \"pentagon\"      \"talks\"       \"years\"      \r\n\r\nVisualization of Topic\r\nDistributions\r\nTo look at the models more easily, I’ll name the strings with the top\r\n5 most likely terms for each topic.\r\n\r\n\r\nShow code\r\n\r\ntop5termsPerTopic2 <- terms(topicModel2, 5)\r\ntopicNames2 <- apply(top5termsPerTopic2, 2, paste, collapse=\" \")\r\nEncoding(topicNames2) <- \"UTF-8\"\r\n\r\n\r\n\r\nAfter looking into the documents, I can visualize the topic\r\ndistributions within the documents.\r\n\r\n\r\nShow code\r\n\r\nN <- length(exampleIds)\r\n# get topic proportions form example documents\r\ntopicProportionExamples2 <- theta2[exampleIds,]\r\ncolnames(topicProportionExamples2) <- topicNames2\r\nvizDataFrame2 <- melt(cbind(data.frame(topicProportionExamples2), document = factor(1:N)), variable.name = \"topic\", id.vars = \"document\")  \r\nggplot(data = vizDataFrame2, aes(topic, value, fill = document), ylab = \"proportion\") + \r\n  geom_bar(stat=\"identity\") +\r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  \r\n  coord_flip() +\r\n  facet_wrap(~ document, ncol = N)\r\n\r\n\r\n\r\n\r\nTopic Ranking\r\nI will try to get a more meaningful order of top terms per topic by\r\nre-ranking them with a specific score (Chang et al. 2009).\r\nThe idea of re-ranking terms is similar to the idea of TF-IDF. The\r\nmore a term appears in top levels with regard to its probability, the\r\nless meaningful it is to describe the topic. Hence, the scoring advanced\r\nfavors terms to describe a topic.\r\n\r\n\r\nShow code\r\n\r\n# re-rank top topic terms for topic names\r\ntopicNames2 <- apply(lda::top.topic.words(beta2, 5, by.score = T), 2, paste, collapse = \" \")\r\n\r\n\r\n\r\nApproach 1\r\nSort topics according to their probability within the entire\r\ncollection:\r\n\r\n\r\nShow code\r\n\r\n#mean probablities over all paragraphs\r\ntopicProportions2 <- colSums(theta2) / nDocs(DTM)  \r\n#assign the topic names we created before\r\nnames(topicProportions2) <- topicNames2   \r\n#show summed proportions in decreased order\r\nsort(topicProportions2, decreasing = TRUE) \r\n\r\n\r\ntaliban afghanistan afghan war biden \r\n                              0.3519 \r\n   kabul afghan trump taliban troops \r\n                              0.3270 \r\n     biden house military iran world \r\n                              0.3211 \r\n\r\nApproach 2\r\nWe count how often a topic appears as a primary topic within a\r\nparagraph This method is also called Rank-1.\r\n\r\n\r\nShow code\r\n\r\ncountsOfPrimaryTopics2 <- rep(0, K2)\r\nnames(countsOfPrimaryTopics2) <- topicNames2\r\nfor (i in 1:nDocs(DTM)) {\r\n  topicsPerDoc2 <- theta2[i, ] # select topic distribution for document i\r\n  # get first element position from ordered list\r\n  primaryTopic2 <- order(topicsPerDoc2, decreasing = TRUE)[1] \r\n  countsOfPrimaryTopics2[primaryTopic2] <- countsOfPrimaryTopics2[primaryTopic2] + 1\r\n}\r\nsort(countsOfPrimaryTopics2, decreasing = TRUE)\r\n\r\n\r\ntaliban afghanistan afghan war biden \r\n                                 672 \r\n   kabul afghan trump taliban troops \r\n                                 614 \r\n     biden house military iran world \r\n                                 586 \r\n\r\nSorting topics by the Rank-1 method places topics with rather\r\nspecific thematic coherences in upper ranks of the list.\r\nWordcloud of Topic\r\nThe wordcloud is a good preliminary way to look at the topics. I’ll\r\nlook at topic 2, because the ranking validates what was my intuitive\r\nguess. In the visualization of this model the topics are all distributed\r\nso broadly this doesn’t help much in this case.\r\n\r\n\r\nShow code\r\n\r\n#visualize topics as word cloud\r\ntopicToViz2 <- 2 # change for your own topic of interest\r\n#topicToViz <- grep('fear', topicNames)[1] # Or select a topic by a term contained in its name\r\n#select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order\r\ntop40terms2 <- sort(tmResult2$terms[topicToViz2,], decreasing=TRUE)[1:40]\r\nwords2 <- names(top40terms2)\r\n#convert non-graph characters to combat error in wordcloud\r\nwords2=str_replace_all(words2,\"[^[:graph:]]\", \" \") \r\n#extract the probabilites of each of the 40 terms\r\nprobabilities2 <- sort(tmResult2$terms[topicToViz2,], decreasing=TRUE)[1:40]\r\n#visualize the terms as wordcloud\r\nmycolors <- brewer.pal(8, \"Dark2\")\r\nwordcloud(words2, probabilities2, random.order = FALSE, color = mycolors)\r\n\r\n\r\n\r\n\r\nTopic Distribution\r\nAdjustment\r\nChanging the alpha to the lower level from 0.1228 to 0.03\r\n\r\n\r\nShow code\r\n\r\n# see alpha from previous model\r\nattr(topicModel2, \"alpha\") \r\n\r\n\r\n[1] 0.1253\r\n\r\nShow code\r\n\r\ntopicModel2b <- LDA(DTM, K2, method=\"VEM\", control=list(alpha = 0.03))\r\n\r\n\r\n\r\nModel Details\r\n\r\n\r\nShow code\r\n\r\n#look at posterior distributions\r\ntmResult2b <- posterior(topicModel2b)\r\n#format of the resulting object\r\nattributes(tmResult2b)\r\n\r\n\r\n$names\r\n[1] \"terms\"  \"topics\"\r\n\r\nShow code\r\n\r\n#topics are probability distributions over the entire vocabulary\r\n#get beta from results\r\nbeta2b <- tmResult2b$terms \r\n#K distributions over nTerms(DTM) terms\r\ndim(beta2b)                \r\n\r\n\r\n[1]    3 3422\r\n\r\nShow code\r\n\r\n#rows in beta sum to 1\r\nrowSums(beta2b)          \r\n\r\n\r\n1 2 3 \r\n1 1 1 \r\n\r\nShow code\r\n\r\n#for every document we have a probability distribution of its contained topics\r\ntheta2b <- tmResult2b$topics \r\n#nDocs(DTM) distributions over K topics\r\ndim(theta2b)\r\n\r\n\r\n[1] 1872    3\r\n\r\nShow code\r\n\r\n#rows in theta sum to 1\r\nrowSums(theta2b)[1:10] \r\n\r\n\r\n 1  2  3  4  5  6  7  8  9 10 \r\n 1  1  1  1  1  1  1  1  1  1 \r\n\r\nModel Preview\r\nNow I can look at the 10 most likely terms within the probabilities\r\nof the inferred topics. I’ll take a look at them for each of the 5\r\ntopics to get a clearer idea of how the topics are represented in this\r\nmodel.\r\n\r\n\r\nShow code\r\n\r\nterms(topicModel2b, 10)\r\n\r\n\r\n      Topic 1       Topic 2       Topic 3      \r\n [1,] \"taliban\"     \"biden\"       \"afghan\"     \r\n [2,] \"trump\"       \"afghan\"      \"afghanistan\"\r\n [3,] \"biden\"       \"afghanistan\" \"taliban\"    \r\n [4,] \"war\"         \"kabul\"       \"trump\"      \r\n [5,] \"afghans\"     \"taliban\"     \"war\"        \r\n [6,] \"troops\"      \"pentagon\"    \"military\"   \r\n [7,] \"afghanistan\" \"military\"    \"biden\"      \r\n [8,] \"kabul\"       \"trump\"       \"peace\"      \r\n [9,] \"pentagon\"    \"women\"       \"china\"      \r\n[10,] \"afghan\"      \"talks\"       \"years\"      \r\n\r\nVisualization of New\r\nDistribution Alpha\r\n\r\n\r\nShow code\r\n\r\ntmResult2b <- posterior(topicModel2b)\r\ntheta2b <- tmResult2b$topics\r\nbeta2b <- tmResult2b$terms\r\n#reset topicnames\r\ntopicNames2b <- apply(terms(topicModel2b, 5), 2, paste, collapse = \" \")  \r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# get topic proportions form example documents\r\ntopicProportionExamples2b <- theta2b[exampleIds,]\r\ncolnames(topicProportionExamples2b) <- topicNames2b\r\nvizDataFrame2b <- melt(cbind(data.frame(topicProportionExamples2b), document = factor(1:N)), variable.name = \"topic\", id.vars = \"document\")  \r\nggplot(data = vizDataFrame2b, aes(topic, value, fill = document), ylab = \"proportion\") + \r\n  geom_bar(stat=\"identity\") +\r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  \r\n  coord_flip() +\r\n  facet_wrap(~ document, ncol = N)\r\n\r\n\r\n\r\n\r\nTopic Ranking\r\nI will try to get a more meaningful order of top terms per topic by\r\nre-ranking them with a specific score (Chang et al. 2009).\r\nThe idea of re-ranking terms is similar to the idea of TF-IDF. The\r\nmore a term appears in top levels w.r.t. its probability, the less\r\nmeaningful it is to describe the topic. Hence, the scoring advanced\r\nfavors terms to describe a topic.\r\n\r\n\r\nShow code\r\n\r\n# re-rank top topic terms for topic names\r\ntopicNames2b <- apply(lda::top.topic.words(beta2b, 5, by.score = T), 2, paste, collapse = \" \")\r\n\r\n\r\n\r\nApproach 1\r\nSort topics according to their probability within the entire\r\ncollection:\r\n\r\n\r\nShow code\r\n\r\n#mean probablities over all paragraphs\r\ntopicProportions2b <- colSums(theta2b) / nDocs(DTM)  \r\n#assign the topic names we created before\r\nnames(topicProportions2b) <- topicNames2b    \r\n#show summed proportions in decreased order\r\nsort(topicProportions2b, decreasing = TRUE) \r\n\r\n\r\nafghan base guant<e1>namo afghanistan security \r\n                                        0.3744 \r\n             biden afghan migrants talks kabul \r\n                                        0.3148 \r\n          democrats afghans trump house troops \r\n                                        0.3108 \r\n\r\nApproach 2\r\nWe count how often a topic appears as a primary topic within a\r\nparagraph This method is also called Rank-1.\r\n\r\n\r\nShow code\r\n\r\ncountsOfPrimaryTopics2b <- rep(0, K2)\r\nnames(countsOfPrimaryTopics2b) <- topicNames2b\r\nfor (i in 1:nDocs(DTM)) {\r\n  topicsPerDoc2b <- theta2b[i, ] # select topic distribution for document i\r\n  # get first element position from ordered list\r\n  primaryTopic2b <- order(topicsPerDoc2b, decreasing = TRUE)[1] \r\n  countsOfPrimaryTopics2b[primaryTopic2b] <- countsOfPrimaryTopics2b[primaryTopic2b] + 1\r\n}\r\nsort(countsOfPrimaryTopics2b, decreasing = TRUE)\r\n\r\n\r\nafghan base guant<e1>namo afghanistan security \r\n                                           705 \r\n             biden afghan migrants talks kabul \r\n                                           587 \r\n          democrats afghans trump house troops \r\n                                           580 \r\n\r\nSorting topics by the Rank-1 method places topics with rather\r\nspecific thematic coherences in upper ranks of the list.\r\nWordcloud of Topic\r\nThe wordcloud is a good preliminary way to look at the topics. I’ll\r\nlook at topic 3, again based on the ranking predominance.\r\n\r\n\r\nShow code\r\n\r\n#visualize topics as word cloud\r\ntopicToViz2b <- 3 # change for your own topic of interest\r\n#topicToViz <- grep('fear', topicNames)[1] # Or select a topic by a term contained in its name\r\n#select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order\r\ntop40terms2b <- sort(tmResult2b$terms[topicToViz2b,], decreasing=TRUE)[1:40]\r\nwords2b <- names(top40terms2b)\r\n#convert non-graph characters to combat error in wordcloud\r\nwords2b=str_replace_all(words2,\"[^[:graph:]]\", \" \") \r\n#extract the probabilites of each of the 40 terms\r\nprobabilities2b <- sort(tmResult2b$terms[topicToViz2b,], decreasing=TRUE)[1:40]\r\n#visualize the terms as wordcloud\r\nmycolors <- brewer.pal(8, \"Dark2\")\r\nwordcloud(words2b, probabilities2b, random.order = FALSE, color = mycolors)\r\n\r\n\r\n\r\n\r\nFiltering Documents by\r\n“Withdrawal”\r\nThe fact that a topic model conveys of topic probabilities for each\r\ndocument, resp. paragraph in our case, makes it possible to use it for\r\nthematic filtering of a collection. AS filter we select only those\r\ndocuments which exceed a certain threshold of their probability value\r\nfor certain topics (for example, each document which contains the topic\r\n“Withdrawal” (topic 2) to more than 20 percent).\r\n\r\n\r\nShow code\r\n\r\ntopicToFilter2b <- 3  # you can set this manually ...\r\n# ... or have it selected by a term in the topic name (e.g. 'children')\r\n#topicToFilter2b <- grep('withdrawal', topicNames2)[1] \r\ntopicThreshold2b <- 0.2\r\nselectedDocumentIndexes2b <- which(theta2b[, topicToFilter2b] >= topicThreshold2b)\r\nfilteredCorpus2b <- corpus[selectedDocumentIndexes2b]\r\n# show length of filtered corpus\r\nfilteredCorpus2b\r\n\r\n\r\n<<SimpleCorpus>>\r\nMetadata:  corpus specific: 1, document level (indexed): 2\r\nContent:  documents: 740\r\n\r\nVisualizing Topic\r\nProportions Over Time\r\nIn a last step, we provide a distant view on the topics in the data\r\nover time. For this, we aggregate mean topic proportions per month for\r\nall of the topics. These aggregated topic proportions can then be\r\nvisualized, e.g. as a bar plot.\r\n\r\n\r\nShow code\r\n\r\n#convert non-graph characters to combat error in grid.Call\r\ntopicNames2b=str_replace_all(topicNames2b,\"[^[:graph:]]\", \" \") \r\n# append month information for aggregation\r\ntextdata$month <- paste0(substr(textdata$month.ended, 0, 3), \"0\")\r\n# get mean topic proportions per month\r\ntopic_proportion_per_month2b <- aggregate(theta2b, by = list(month = textdata$month), mean)\r\n# set topic names to aggregated columns\r\ncolnames(topic_proportion_per_month2b)[2:(K2+1)] <- topicNames2b\r\n# reshape data frame\r\nvizDataFrame2b <- melt(topic_proportion_per_month2b, id.vars = \"month\")\r\n# plot topic proportions per month as bar plot\r\nggplot(vizDataFrame2b, aes(x=month, y=value, fill=variable)) + \r\n  geom_bar(stat = \"identity\") + ylab(\"proportion\") + \r\n  scale_fill_manual(values = paste0(alphabet(20), \"FF\"), name = \"topic\") + \r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\r\n\r\n\r\n\r\n\r\nUnfortunately, the month variable in my data is giving me a lot of\r\ntrouble visualizing in chronological order.\r\nVisualizing\r\nTopic Proportions By Article (Dated)\r\nSince the date column is being difficult, I will try this\r\nvisualization by article number (doc_id), since the numbers have been\r\ngenerated by date published before being grouped by month.\r\nGibbs Method\r\n\r\n\r\nShow code\r\n\r\ntextdatacleaned <- read.csv(\"textdatacleaned.csv\")\r\n\r\n#convert non-graph characters to combat error in grid.Call\r\n#topicNames2b=str_replace_all(topicNames2b,\"[^[:graph:]]\", \" \") \r\n# append month information for aggregation\r\n#textdata$month <- paste0(substr(textdata$month.ended, 0, 3), \"0\")\r\n# get mean topic proportions per month\r\ntopic_proportion_per_month <- aggregate(theta, by = list(month = textdatacleaned$month), mean)\r\n# set topic names to aggregated columns\r\ncolnames(topic_proportion_per_month)[2:(K+1)] <- topicNames\r\n# reshape data frame\r\nvizDataFrame <- melt(topic_proportion_per_month, id.vars = \"month\")\r\n# plot topic proportions per month as bar plot\r\nggplot(vizDataFrame, aes(x=month, y=value, fill=variable)) + \r\n  geom_bar(stat = \"identity\") + ylab(\"proportion\") + \r\n  scale_fill_manual(values = paste0(alphabet(20), \"FF\"), name = \"topic\") + \r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\r\n\r\n\r\n\r\n\r\nVEM Method\r\n\r\n\r\nShow code\r\n\r\ntextdatacleaned <- read.csv(\"textdatacleaned.csv\")\r\n\r\n#convert non-graph characters to combat error in grid.Call\r\n#topicNames2b=str_replace_all(topicNames2b,\"[^[:graph:]]\", \" \") \r\n# append month information for aggregation\r\n#textdata$month <- paste0(substr(textdata$month.ended, 0, 3), \"0\")\r\n# get mean topic proportions per month\r\ntopic_proportion_per_month2b <- aggregate(theta2b, by = list(month = textdatacleaned$month), mean)\r\n# set topic names to aggregated columns\r\ncolnames(topic_proportion_per_month2b)[2:(K2+1)] <- topicNames2b\r\n# reshape data frame\r\nvizDataFrame2b <- melt(topic_proportion_per_month2b, id.vars = \"month\")\r\n# plot topic proportions per month as bar plot\r\nggplot(vizDataFrame2b, aes(x=month, y=value, fill=variable)) + \r\n  geom_bar(stat = \"identity\") + ylab(\"proportion\") + \r\n  scale_fill_manual(values = paste0(alphabet(20), \"FF\"), name = \"topic\") + \r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\r\n\r\n\r\n\r\n\r\nThat gives us a much better idea of how the topics changed over the\r\ncourse of the 19 months being analyzed!\r\n\r\n\r\n\r\n",
    "preview": "posts/topic-modeling/photoc.png",
    "last_modified": "2022-05-01T15:56:57-04:00",
    "input_file": {}
  },
  {
    "path": "posts/new-lexicon/",
    "title": "New Direction of Sentiment Analysis",
    "description": "Adding on to my initial sentiment analysis using new methods and dictionaries.",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/NYT-Analysis/"
      }
    ],
    "date": "2022-04-27",
    "categories": [
      "text as data",
      "NYT text analysis project"
    ],
    "contents": "\r\n\r\nContents\r\nUsing New Dictionaries\r\nLoad Data\r\nBing Lexicon\r\nCreate Function\r\nApply Sentiments and\r\nFunction\r\nSummary\r\nPreview Results\r\n\r\nNRC\r\nCreate Function\r\nApply Function\r\nSummary\r\nPreview Results\r\n\r\nAFINN\r\nCreate Function\r\nSummary\r\nPreview Results\r\n\r\nCongruence\r\nCreate Function\r\nApply Function\r\nEvaluation\r\nFinal Results\r\n\r\nVisualization\r\n\r\nCitations\r\n\r\nUsing New Dictionaries\r\nThis time, my analysis will be through a different process of\r\nanalysis than I used in my first two analyses.\r\nThis anaysis will use an alternative sequence to analyze a different\r\nset of three sentiment datasets within the “tidytext” package.\r\nThe dictionaries are “bing”, “nrc”, (which I used previously) and\r\n“AFINN”.\r\nLoad Data\r\nLoading the data from the expanded analysis:\r\n\r\n\r\nShow code\r\n\r\n#load data\r\nmain_headlines <- read.csv(\"afghanistan_headlines_main.csv\")\r\nmain_headlines <- as.data.frame(main_headlines)\r\n#turn into data frame\r\nprint_headlines <- read.csv(\"afghanistan_headlines_print.csv\")\r\nprint_headlines <- as.data.frame(print_headlines)\r\n\r\n\r\n\r\nBing Lexicon\r\nThe bing lexicon sorts words into positive or negative positions.\r\nFirst I’ll create tokens for the main and print headlines.\r\n\r\n\r\nShow code\r\n\r\n#create tokens without stop words for main headlines\r\ntkn_l_main <- apply(main_headlines, 1, function(x) { data.frame(text=x, stringsAsFactors = FALSE) %>% unnest_tokens(word, text)})\r\nmain_news_tokens <- lapply(tkn_l_main, function(x) {anti_join(x, stop_words)})\r\nstr(main_news_tokens, list.len = 5)\r\n\r\n\r\nList of 936\r\n $ :'data.frame':   12 obs. of  1 variable:\r\n  ..$ word: chr [1:12] \"1\" \"7\" \"17\" \"2020\" ...\r\n $ :'data.frame':   12 obs. of  1 variable:\r\n  ..$ word: chr [1:12] \"2\" \"8\" \"30\" \"2020\" ...\r\n $ :'data.frame':   12 obs. of  1 variable:\r\n  ..$ word: chr [1:12] \"3\" \"6\" \"2\" \"2021\" ...\r\n $ :'data.frame':   11 obs. of  1 variable:\r\n  ..$ word: chr [1:11] \"4\" \"12\" \"20\" \"2020\" ...\r\n $ :'data.frame':   10 obs. of  1 variable:\r\n  ..$ word: chr [1:10] \"5\" \"9\" \"11\" \"2021\" ...\r\n  [list output truncated]\r\n\r\nShow code\r\n\r\nmain_news_tokens[[1]]\r\n\r\n\r\n             word\r\ndoc_id          1\r\ndate...2        7\r\ndate...3       17\r\ndate...4     2020\r\ntext...5      174\r\ntext...6  million\r\ntext...7   afghan\r\ntext...8    drone\r\ntext...9  program\r\ntext...10 riddled\r\ntext...11     u.s\r\ntext...12  report\r\n\r\nShow code\r\n\r\n#create tokens without stop words for print headlines\r\ntkn_l_print <- apply(print_headlines, 1, function(x) { data.frame(text=x, stringsAsFactors = FALSE) %>% unnest_tokens(word, text)})\r\nprint_news_tokens <- lapply(tkn_l_print, function(x) {anti_join(x, stop_words)})\r\nstr(print_news_tokens, list.len = 5)\r\n\r\n\r\nList of 936\r\n $ :'data.frame':   11 obs. of  1 variable:\r\n  ..$ word: chr [1:11] \"1\" \"7\" \"17\" \"2020\" ...\r\n $ :'data.frame':   10 obs. of  1 variable:\r\n  ..$ word: chr [1:10] \"2\" \"8\" \"30\" \"2020\" ...\r\n $ :'data.frame':   10 obs. of  1 variable:\r\n  ..$ word: chr [1:10] \"3\" \"6\" \"2\" \"2021\" ...\r\n $ :'data.frame':   12 obs. of  1 variable:\r\n  ..$ word: chr [1:12] \"4\" \"12\" \"20\" \"2020\" ...\r\n $ :'data.frame':   7 obs. of  1 variable:\r\n  ..$ word: chr [1:7] \"5\" \"9\" \"11\" \"2021\" ...\r\n  [list output truncated]\r\n\r\nShow code\r\n\r\nprint_news_tokens[[1]]\r\n\r\n\r\n              word\r\ndoc_id           1\r\ndate...2         7\r\ndate...3        17\r\ndate...4      2020\r\ntext...5       174\r\ntext...6   million\r\ntext...7     drone\r\ntext...8   program\r\ntext...9   afghans\r\ntext...10  riddled\r\ntext...11 pentagon\r\n\r\nCreate Function\r\nI need to next create a function to assign sentiment labels.\r\n\r\n\r\nShow code\r\n\r\ncompute_sentiment <- function(d) {\r\n  if (nrow(d) == 0) {\r\n    return(NA)\r\n  }\r\n  neg_score <- d %>% filter(sentiment==\"negative\") %>% nrow()\r\n  pos_score <- d %>% filter(sentiment==\"positive\") %>% nrow()\r\n  pos_score - neg_score\r\n} \r\n\r\n\r\n\r\nApply Sentiments and\r\nFunction\r\nThen I can apply that sentiment function to the headline data\r\nsets\r\n\r\n\r\nShow code\r\n\r\nsentiments_bing <- get_sentiments(\"bing\")\r\n\r\n#apply sentiment to main headlines\r\nmain_news_sentiment_bing <- sapply(main_news_tokens, function(x) { x %>% inner_join(sentiments_bing) %>% compute_sentiment()})\r\n#apply sentiment to print headlines\r\nprint_news_sentiment_bing <- sapply(print_news_tokens, function(x) { x %>% inner_join(sentiments_bing) %>% compute_sentiment()})\r\n\r\n\r\n\r\nSummary\r\nThe summaries of each show the number of NA’s are minimal.\r\n\r\n\r\nShow code\r\n\r\nsummary(main_news_sentiment_bing)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \r\n-4.0000 -1.0000 -1.0000 -0.5945  1.0000  2.0000     349 \r\n\r\nShow code\r\n\r\nsummary(print_news_sentiment_bing)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \r\n-4.0000 -1.0000 -1.0000 -0.6209  0.0000  3.0000     353 \r\n\r\nPreview Results\r\nNow I can look at the first 10 headlines and the corresponding bing\r\nanalysis scores. I can see that the scores vary, even in the first 10\r\nheadlines.\r\n\r\n\r\nShow code\r\n\r\n#head 10 main headlines with bing analysis scores\r\nmain_news_sentiment_bing_df <- data.frame(main_text=main_headlines$text, score = main_news_sentiment_bing)\r\nhead(main_news_sentiment_bing_df, 10)\r\n\r\n\r\n                                                                       main_text\r\n1   $174 Million Afghan Drone Program Is Riddled With Problems, U.S. Report Says\r\n2           ‘A Hail Mary’: Psychedelic Therapy Draws Veterans to Jungle Retreats\r\n3    ‘Come On In, Boys’: A Wave of the Hand Sets Off Spain-Morocco Migrant Fight\r\n4  ‘Covid Can’t Compete.’ In a Place Mired in War, the Virus Is an Afterthought.\r\n5     ‘Everything Changed Overnight’: Afghan Reporters Face an Intolerant Regime\r\n6       ‘Finally, I Am Safe’: U.S. Air Base Becomes Temporary Refuge for Afghans\r\n7                    ‘Find Him and Kill Him’: An Afghan Pilot’s Desperate Escape\r\n8     ‘Football Is Like Food’: Afghan Female Soccer Players Find a Home in Italy\r\n9    ‘Go Big’ on Coronavirus Stimulus, Trump Says, Pitching Checks for Americans\r\n10            ‘Hospital Needs to Be Quarantined,’ but Works On in Country at War\r\n   score\r\n1     NA\r\n2      1\r\n3     NA\r\n4     -1\r\n5     NA\r\n6      1\r\n7     -2\r\n8     NA\r\n9      1\r\n10    NA\r\n\r\nShow code\r\n\r\n#head 10 print headlines with bing analysis scores\r\nprint_news_sentiment_bing_df <- data.frame(print_text=print_headlines$text, score = print_news_sentiment_bing)\r\nhead(print_news_sentiment_bing_df, 10)\r\n\r\n\r\n                                                                       print_text\r\n1  $174 Million Drone Program for Afghans Is Riddled With Problems, Pentagon Says\r\n2                 Psychedelic Therapy In the Jungle Soothes The Pain for Veterans\r\n3                                  Morocco Sends Spanish Outpost a Migrant Influx\r\n4          ‘It’s a Lie’: Denial and Skepticism Permeate a Nation Embroiled in War\r\n5                                      ‘Everything Changed’: Media Face Crackdown\r\n6          ‘Finally, I Am Safe’: Thousands Find Temporary Refuge at U.S. Air Base\r\n7                  ‘Find Him and Kill Him’: A Pilot’s Desperate Escape From Kabul\r\n8                                     Soccer Players Under Threat Escape to Italy\r\n9                                    Plan Would Inject $1 Trillion Into Economy  \r\n10  As Pandemic Takes Toll on Afghan Doctors, Hospitals Still Tend to War Wounded\r\n   score\r\n1     NA\r\n2     -1\r\n3     NA\r\n4     -4\r\n5     NA\r\n6      1\r\n7     -2\r\n8     -1\r\n9     NA\r\n10    -1\r\n\r\nNRC\r\nAs I saw in my first two analyses, the NRC lexicon uses 10 different\r\nsentiments, including negative and positive but with additional\r\nsentiments as well.\r\n\r\n\r\nShow code\r\n\r\nsentiments_nrc <- get_sentiments(\"nrc\")\r\n(unique_sentiments_nrc <- unique(sentiments_nrc$sentiment))\r\n\r\n\r\n [1] \"trust\"        \"fear\"         \"negative\"     \"sadness\"     \r\n [5] \"anger\"        \"surprise\"     \"positive\"     \"disgust\"     \r\n [9] \"joy\"          \"anticipation\"\r\n\r\nCreate Function\r\nNext again I will create a function to assign sentiment labels that\r\napply ‘positive’ and ‘negative’ in a binary interpretation of each of\r\nthe 8 other sentiments.\r\n\r\n\r\nShow code\r\n\r\ncompute_pos_neg_sentiments_nrc <- function(the_sentiments_nrc) {\r\n  s <- unique(the_sentiments_nrc$sentiment)\r\n  df_sentiments <- data.frame(sentiment = s, \r\n                              mapped_sentiment = c(\"positive\", \"negative\", \"negative\", \"negative\",\r\n                                                    \"negative\", \"positive\", \"positive\", \"negative\", \r\n                                                    \"positive\", \"positive\"))\r\n  ss <- sentiments_nrc %>% inner_join(df_sentiments)\r\n  the_sentiments_nrc$sentiment <- ss$mapped_sentiment\r\n  the_sentiments_nrc\r\n}\r\n\r\nnrc_sentiments_pos_neg_scale <- compute_pos_neg_sentiments_nrc(sentiments_nrc)\r\n\r\n\r\n\r\nApply Function\r\nThen I can apply that sentiment function to the headline data\r\nsets\r\n\r\n\r\nShow code\r\n\r\n#calculating NRC sentiment for main headlines\r\nmain_news_sentiment_nrc <- sapply(main_news_tokens, function(x) { x %>% inner_join(nrc_sentiments_pos_neg_scale) %>% compute_sentiment()})\r\n\r\n#calculating NRC sentiment for print headlines\r\nprint_news_sentiment_nrc <- sapply(print_news_tokens, function(x) { x %>% inner_join(nrc_sentiments_pos_neg_scale) %>% compute_sentiment()})\r\n\r\n\r\n\r\nSummary\r\nThe summaries of each show the number of NA’s are even more\r\nminimal.\r\n\r\n\r\nShow code\r\n\r\nsummary(main_news_sentiment_nrc)\r\n\r\n\r\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \r\n-12.0000  -3.0000  -1.0000  -0.7417   1.0000  13.0000      150 \r\n\r\nShow code\r\n\r\nsummary(print_news_sentiment_nrc)\r\n\r\n\r\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \r\n-12.0000  -3.0000  -1.0000  -0.4994   2.0000  13.0000      151 \r\n\r\nPreview Results\r\nNow I can look at the first 10 headlines and the corresponding NRC\r\nanalysis scores. I can see that the scores vary as well, even in the\r\nfirst 10 headlines.\r\n\r\n\r\nShow code\r\n\r\n#data frame of main NRC sentiment\r\nmain_news_sentiment_nrc_df <- data.frame(main_text=main_headlines$text, score = main_news_sentiment_nrc)\r\nhead(main_news_sentiment_nrc_df, 10)\r\n\r\n\r\n                                                                       main_text\r\n1   $174 Million Afghan Drone Program Is Riddled With Problems, U.S. Report Says\r\n2           ‘A Hail Mary’: Psychedelic Therapy Draws Veterans to Jungle Retreats\r\n3    ‘Come On In, Boys’: A Wave of the Hand Sets Off Spain-Morocco Migrant Fight\r\n4  ‘Covid Can’t Compete.’ In a Place Mired in War, the Virus Is an Afterthought.\r\n5     ‘Everything Changed Overnight’: Afghan Reporters Face an Intolerant Regime\r\n6       ‘Finally, I Am Safe’: U.S. Air Base Becomes Temporary Refuge for Afghans\r\n7                    ‘Find Him and Kill Him’: An Afghan Pilot’s Desperate Escape\r\n8     ‘Football Is Like Food’: Afghan Female Soccer Players Find a Home in Italy\r\n9    ‘Go Big’ on Coronavirus Stimulus, Trump Says, Pitching Checks for Americans\r\n10            ‘Hospital Needs to Be Quarantined,’ but Works On in Country at War\r\n   score\r\n1     -2\r\n2      0\r\n3     -3\r\n4     -3\r\n5     -5\r\n6      8\r\n7     -4\r\n8      6\r\n9      1\r\n10    -3\r\n\r\nShow code\r\n\r\n#data frame of print NRC sentiment\r\nprint_news_sentiment_nrc_df <- data.frame(print_text=print_headlines$text, score = print_news_sentiment_nrc)\r\nhead(print_news_sentiment_nrc_df, 10)\r\n\r\n\r\n                                                                       print_text\r\n1  $174 Million Drone Program for Afghans Is Riddled With Problems, Pentagon Says\r\n2                 Psychedelic Therapy In the Jungle Soothes The Pain for Veterans\r\n3                                  Morocco Sends Spanish Outpost a Migrant Influx\r\n4          ‘It’s a Lie’: Denial and Skepticism Permeate a Nation Embroiled in War\r\n5                                      ‘Everything Changed’: Media Face Crackdown\r\n6          ‘Finally, I Am Safe’: Thousands Find Temporary Refuge at U.S. Air Base\r\n7                  ‘Find Him and Kill Him’: A Pilot’s Desperate Escape From Kabul\r\n8                                     Soccer Players Under Threat Escape to Italy\r\n9                                    Plan Would Inject $1 Trillion Into Economy  \r\n10  As Pandemic Takes Toll on Afghan Doctors, Hospitals Still Tend to War Wounded\r\n   score\r\n1     -2\r\n2     -4\r\n3     -1\r\n4     -7\r\n5     NA\r\n6      8\r\n7     -4\r\n8     -3\r\n9      2\r\n10    -5\r\n\r\nAFINN\r\nThe AFINN lexicon has valence ratings between -5 (negative) and +5\r\n(positive).\r\n\r\n\r\nShow code\r\n\r\nsentiments_afinn <- get_sentiments(\"afinn\")\r\n\r\ncolnames(sentiments_afinn) <- c(\"word\", \"sentiment\")\r\n\r\n\r\n\r\nCreate Function\r\nAgain I’ll create a function to assign sentiment labels.\r\n\r\n\r\nShow code\r\n\r\n#applying AFINN sentiment to main headlines\r\nmain_news_sentiment_afinn_df <- lapply(main_news_tokens, function(x) { x %>% inner_join(sentiments_afinn)})\r\nmain_news_sentiment_afinn <- sapply(main_news_sentiment_afinn_df, function(x) { \r\n      ifelse(nrow(x) > 0, sum(x$sentiment), NA)\r\n  })\r\n#applying AFINN sentiment to print headlines\r\nprint_news_sentiment_afinn_df <- lapply(print_news_tokens, function(x) { x %>% inner_join(sentiments_afinn)})\r\nprint_news_sentiment_afinn <- sapply(print_news_sentiment_afinn_df, function(x) { \r\n      ifelse(nrow(x) > 0, sum(x$sentiment), NA)\r\n  })\r\n\r\n\r\n\r\nSummary\r\nThe summaries of each show the number of NA’s are similar to that in\r\nthe bing lexicon.\r\n\r\n\r\nShow code\r\n\r\nsummary(main_news_sentiment_afinn)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \r\n-10.000  -3.000  -2.000  -1.769  -1.000   5.000     368 \r\n\r\nShow code\r\n\r\nsummary(print_news_sentiment_afinn)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \r\n -10.00   -3.00   -2.00   -1.52   -1.00    6.00     359 \r\n\r\nPreview Results\r\nNow I can look at the first 10 headlines and the corresponding AFINN\r\nanalysis scores. I can see that the scores vary a lot less than in the\r\nfirst two lexicons.\r\n\r\n\r\nShow code\r\n\r\n#data frame of AFINN main headlines\r\nmain_news_sentiment_afinn_df <- data.frame(main_text=main_headlines$text, score = main_news_sentiment_afinn)\r\nhead(main_news_sentiment_afinn_df, 10)\r\n\r\n\r\n                                                                       main_text\r\n1   $174 Million Afghan Drone Program Is Riddled With Problems, U.S. Report Says\r\n2           ‘A Hail Mary’: Psychedelic Therapy Draws Veterans to Jungle Retreats\r\n3    ‘Come On In, Boys’: A Wave of the Hand Sets Off Spain-Morocco Migrant Fight\r\n4  ‘Covid Can’t Compete.’ In a Place Mired in War, the Virus Is an Afterthought.\r\n5     ‘Everything Changed Overnight’: Afghan Reporters Face an Intolerant Regime\r\n6       ‘Finally, I Am Safe’: U.S. Air Base Becomes Temporary Refuge for Afghans\r\n7                    ‘Find Him and Kill Him’: An Afghan Pilot’s Desperate Escape\r\n8     ‘Football Is Like Food’: Afghan Female Soccer Players Find a Home in Italy\r\n9    ‘Go Big’ on Coronavirus Stimulus, Trump Says, Pitching Checks for Americans\r\n10            ‘Hospital Needs to Be Quarantined,’ but Works On in Country at War\r\n   score\r\n1     NA\r\n2      2\r\n3     -1\r\n4     -2\r\n5     NA\r\n6      1\r\n7     -7\r\n8     NA\r\n9     NA\r\n10    -2\r\n\r\nShow code\r\n\r\n#data frame of AFINN print headlines\r\nprint_news_sentiment_afinn_df <- data.frame(print_text=print_headlines$text, score = print_news_sentiment_afinn)\r\nhead(print_news_sentiment_afinn_df, 10)\r\n\r\n\r\n                                                                       print_text\r\n1  $174 Million Drone Program for Afghans Is Riddled With Problems, Pentagon Says\r\n2                 Psychedelic Therapy In the Jungle Soothes The Pain for Veterans\r\n3                                  Morocco Sends Spanish Outpost a Migrant Influx\r\n4          ‘It’s a Lie’: Denial and Skepticism Permeate a Nation Embroiled in War\r\n5                                      ‘Everything Changed’: Media Face Crackdown\r\n6          ‘Finally, I Am Safe’: Thousands Find Temporary Refuge at U.S. Air Base\r\n7                  ‘Find Him and Kill Him’: A Pilot’s Desperate Escape From Kabul\r\n8                                     Soccer Players Under Threat Escape to Italy\r\n9                                    Plan Would Inject $1 Trillion Into Economy  \r\n10  As Pandemic Takes Toll on Afghan Doctors, Hospitals Still Tend to War Wounded\r\n   score\r\n1     NA\r\n2     -2\r\n3     NA\r\n4     -4\r\n5     NA\r\n6      1\r\n7     -7\r\n8     -3\r\n9     NA\r\n10    -2\r\n\r\nCongruence\r\nHaving obtained for each headline data set three potential results as\r\nsentiment evaluation, next I will calculate their congruence.\r\nBy congruence, I am looking at the fact that all three lexicons\r\nexpress a positive or negative result. In other words, the same score\r\nsignal the same sentiment independently from the lexicon’s respective\r\nscale of magnitude. If “NA” values are present, the congruence is\r\ncomputed until at least two non-“NA” values are available, otherwise the\r\nvalue is equal to “NA”.\r\nThen, I compute the final news sentiment as based upon the sum of\r\neach lexicon sentiment score.\r\nCreate Function\r\n\r\n\r\nShow code\r\n\r\ncompute_congruence <- function(x,y,z) {\r\n  v <- c(sign(x), sign(y), sign(z))\r\n  # if only one lexicon reports the score, we cannot check for congruence\r\n  if (sum(is.na(v)) >= 2) {\r\n    return (NA)\r\n  }\r\n  # removing NA and zero value\r\n  v <- na.omit(v)\r\n  v_sum <- sum(v)\r\n  abs(v_sum) == length(v)\r\n}\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ncompute_final_sentiment <- function(x,y,z) {\r\n  if (is.na(x) && is.na(y) && is.na(z)) {\r\n    return (NA)\r\n  }\r\n\r\n  s <- sum(x, y, z, na.rm=TRUE)\r\n  # positive sentiments have score strictly greater than zero\r\n  # negative sentiments have score strictly less than zero\r\n  # neutral sentiments have score equal to zero \r\n  ifelse(s > 0, \"positive\", ifelse(s < 0, \"negative\", \"neutral\"))\r\n}\r\n\r\n\r\n\r\nApply Function\r\nNow I will put the sentiment results in new data frames and apply the\r\nanalyses.\r\n\r\n\r\nShow code\r\n\r\nmain_sentiments_results <- data.frame(main_text = main_headlines$text, \r\n                                 bing_score = main_news_sentiment_bing, \r\n                                 nrc_score = main_news_sentiment_nrc, \r\n                                 afinn_score = main_news_sentiment_afinn,\r\n                                 stringsAsFactors = FALSE)\r\n\r\nprint_sentiments_results <- data.frame(print_text = print_headlines$text, \r\n                                 bing_score = print_news_sentiment_bing, \r\n                                 nrc_score = print_news_sentiment_nrc, \r\n                                 afinn_score = print_news_sentiment_afinn,\r\n                                 stringsAsFactors = FALSE)\r\n\r\n\r\nmain_sentiments_results <- main_sentiments_results %>% rowwise() %>% \r\n  mutate(final_sentiment = compute_final_sentiment(bing_score, nrc_score, afinn_score),\r\n         congruence = compute_congruence(bing_score, nrc_score, afinn_score))\r\n\r\nprint_sentiments_results <- print_sentiments_results %>% rowwise() %>% \r\n  mutate(final_sentiment = compute_final_sentiment(bing_score, nrc_score, afinn_score),\r\n         congruence = compute_congruence(bing_score, nrc_score, afinn_score))\r\n\r\nhead(main_sentiments_results, 10)\r\n\r\n\r\n# A tibble: 10 x 6\r\n# Rowwise: \r\n   main_text          bing_score nrc_score afinn_score final_sentiment\r\n   <chr>                   <int>     <int>       <dbl> <chr>          \r\n 1 $174 Million Afgh~         NA        -2          NA negative       \r\n 2 ‘A Hail Mary’: Ps~          1         0           2 negative       \r\n 3 ‘Come On In, Boys~         NA        -3          -1 negative       \r\n 4 ‘Covid Can’t Comp~         -1        -3          -2 negative       \r\n 5 ‘Everything Chang~         NA        -5          NA negative       \r\n 6 ‘Finally, I Am Sa~          1         8           1 negative       \r\n 7 ‘Find Him and Kil~         -2        -4          -7 negative       \r\n 8 ‘Football Is Like~         NA         6          NA negative       \r\n 9 ‘Go Big’ on Coron~          1         1          NA negative       \r\n10 ‘Hospital Needs t~         NA        -3          -2 negative       \r\n# ... with 1 more variable: congruence <lgl>\r\n\r\nShow code\r\n\r\nhead(print_sentiments_results, 10)\r\n\r\n\r\n# A tibble: 10 x 6\r\n# Rowwise: \r\n   print_text         bing_score nrc_score afinn_score final_sentiment\r\n   <chr>                   <int>     <int>       <dbl> <chr>          \r\n 1 \"$174 Million Dro~         NA        -2          NA negative       \r\n 2 \"Psychedelic Ther~         -1        -4          -2 negative       \r\n 3 \"Morocco Sends Sp~         NA        -1          NA negative       \r\n 4 \"‘It’s a Lie’: De~         -4        -7          -4 negative       \r\n 5 \"‘Everything Chan~         NA        NA          NA negative       \r\n 6 \"‘Finally, I Am S~          1         8           1 negative       \r\n 7 \"‘Find Him and Ki~         -2        -4          -7 negative       \r\n 8 \"Soccer Players U~         -1        -3          -3 negative       \r\n 9 \"Plan Would Injec~         NA         2          NA negative       \r\n10 \"As Pandemic Take~         -1        -5          -2 negative       \r\n# ... with 1 more variable: congruence <lgl>\r\n\r\nEvaluation\r\nIt seems like I need to do more work on the congruence function, as I\r\nhave all “NA” results.\r\n\r\n\r\nShow code\r\n\r\n#If it would be useful to replace the numeric score with same {negative, neutral, positive} scale.\r\nreplace_score_with_sentiment <- function(v_score) {\r\n  v_score[v_score > 0] <- \"positive\"\r\n  v_score[v_score < 0] <- \"negative\"\r\n  v_score[v_score == 0] <- \"neutral\"\r\n  v_score\r\n} \r\n\r\n\r\n\r\nI’ll combine all of the normalized and binary ‘positive’ and\r\n‘negative’ sentiments from all three lexicons into one data frame for\r\neach headline set.\r\n\r\n\r\nShow code\r\n\r\n#apply scale to main results\r\nmain_sentiments_results$bing_score <- replace_score_with_sentiment(main_sentiments_results$bing_score)\r\nmain_sentiments_results$nrc_score <- replace_score_with_sentiment(main_sentiments_results$nrc_score)\r\nmain_sentiments_results$afinn_score <- replace_score_with_sentiment(main_sentiments_results$afinn_score)\r\nmain_sentiments_results[,2:5] <- lapply(main_sentiments_results[,2:5], as.factor)\r\nhead(main_sentiments_results, 40)\r\n\r\n\r\n# A tibble: 40 x 6\r\n# Rowwise: \r\n   main_text          bing_score nrc_score afinn_score final_sentiment\r\n   <chr>              <fct>      <fct>     <fct>       <fct>          \r\n 1 $174 Million Afgh~ <NA>       negative  <NA>        negative       \r\n 2 ‘A Hail Mary’: Ps~ positive   neutral   positive    negative       \r\n 3 ‘Come On In, Boys~ <NA>       negative  negative    negative       \r\n 4 ‘Covid Can’t Comp~ negative   negative  negative    negative       \r\n 5 ‘Everything Chang~ <NA>       negative  <NA>        negative       \r\n 6 ‘Finally, I Am Sa~ positive   positive  positive    negative       \r\n 7 ‘Find Him and Kil~ negative   negative  negative    negative       \r\n 8 ‘Football Is Like~ <NA>       positive  <NA>        negative       \r\n 9 ‘Go Big’ on Coron~ positive   positive  <NA>        negative       \r\n10 ‘Hospital Needs t~ <NA>       negative  negative    negative       \r\n# ... with 30 more rows, and 1 more variable: congruence <lgl>\r\n\r\nShow code\r\n\r\n#apply scale to print results\r\nprint_sentiments_results$bing_score <- replace_score_with_sentiment(print_sentiments_results$bing_score)\r\nprint_sentiments_results$nrc_score <- replace_score_with_sentiment(print_sentiments_results$nrc_score)\r\nprint_sentiments_results$afinn_score <- replace_score_with_sentiment(print_sentiments_results$afinn_score)\r\nprint_sentiments_results[,2:5] <- lapply(print_sentiments_results[,2:5], as.factor)\r\nhead(print_sentiments_results, 40)\r\n\r\n\r\n# A tibble: 40 x 6\r\n# Rowwise: \r\n   print_text         bing_score nrc_score afinn_score final_sentiment\r\n   <chr>              <fct>      <fct>     <fct>       <fct>          \r\n 1 \"$174 Million Dro~ <NA>       negative  <NA>        negative       \r\n 2 \"Psychedelic Ther~ negative   negative  negative    negative       \r\n 3 \"Morocco Sends Sp~ <NA>       negative  <NA>        negative       \r\n 4 \"‘It’s a Lie’: De~ negative   negative  negative    negative       \r\n 5 \"‘Everything Chan~ <NA>       <NA>      <NA>        negative       \r\n 6 \"‘Finally, I Am S~ positive   positive  positive    negative       \r\n 7 \"‘Find Him and Ki~ negative   negative  negative    negative       \r\n 8 \"Soccer Players U~ negative   negative  negative    negative       \r\n 9 \"Plan Would Injec~ <NA>       positive  <NA>        negative       \r\n10 \"As Pandemic Take~ negative   negative  negative    negative       \r\n# ... with 30 more rows, and 1 more variable: congruence <lgl>\r\n\r\nFinal Results\r\nI’ll take the overall sentiment score and join them in one data frame\r\nand visualize it. After taking the value ‘positive’ or ‘negative’ that\r\nis in the majority of the 3 evaluations, the dataset is overwhelmingly\r\n‘negative’ (100%).\r\n\r\n\r\nShow code\r\n\r\nfinal_total <- read.csv(\"all_sentiments_binary.csv\")\r\n\r\nhead(final_total)\r\n\r\n\r\n  article print_bing print_nrc print_afinn print_final main_bing\r\n1       1       <NA>  negative        <NA>    negative      <NA>\r\n2       2   negative  negative    negative    negative  positive\r\n3       3       <NA>  negative        <NA>    negative      <NA>\r\n4       4   negative  negative    negative    negative  negative\r\n5       5       <NA>      <NA>        <NA>    negative      <NA>\r\n6       6   positive  positive    positive    negative  positive\r\n  main_nrc main_afinn main_final\r\n1 negative       <NA>   negative\r\n2  neutral   positive   negative\r\n3 negative   negative   negative\r\n4 negative   negative   negative\r\n5 negative       <NA>   negative\r\n6 positive   positive   negative\r\n\r\nVisualization\r\n\r\n\r\nShow code\r\n\r\nmain_graph <- read.csv(\"main_graph.csv\")\r\n\r\nlibrary(ggplot2)\r\n\r\nmain_plot <- main_graph %>%\r\n  ggplot(aes(date, sentiment, fill = lexicon)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~lexicon, ncol = 1, scales = \"free_y\") +\r\n  scale_fill_manual(values=c(\"#993333\", \"#336699\", \"#669900\")) +\r\n  theme_minimal()\r\nmain_plot\r\n\r\n\r\n\r\n\r\nCitations\r\nCitations:\r\nThis research makes use of the NRC\r\nWord-Emotion Association Lexicon, created by Saif Mohammad and Peter\r\nTurney at the National Research Council Canada.\r\nThis research makes use of the Bing\r\nLexicon. This dataset was first published in Minqing Hu and Bing\r\nLiu, ``Mining and summarizing customer reviews.’’, Proceedings of the\r\nACM SIGKDD International Conference on Knowledge Discovery & Data\r\nMining (KDD-2004), 2004.\r\nThis research makes use of the AFINN\r\nLexicon, Nielsen, F. Å. (2011). A new ANEW: Evaluation of a word\r\nlist for sentiment analysis in microblogs. arXiv preprint\r\narXiv:1103.2903.\r\n\r\n\r\n\r\n",
    "preview": "posts/new-lexicon/new_sentiment.png",
    "last_modified": "2022-05-02T02:53:39-04:00",
    "input_file": {}
  },
  {
    "path": "posts/headline-expanded/",
    "title": "Analysis of Main vs. Print Headlines: Phase 2",
    "description": "Text as Data Project Headline Comparison Research Using API Query \"Afghanistan\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/NYT-Analysis/"
      }
    ],
    "date": "2022-04-26",
    "categories": [
      "text as data",
      "NYT text analysis project"
    ],
    "contents": "\r\n\r\nContents\r\nComparing\r\nMain vs. Print Headlines in the New York Times\r\nMaking\r\nDifferent Choices on Inclusion of Observations\r\nGathering Data\r\nPrevious Process\r\nLoad Data\r\nCreate Corpus\r\nAssign Type to Docvars\r\nTokenization\r\nDocument Feature Matrix\r\nWord Frequency Ratings\r\n\r\nFeature Co-Occurrence\r\nMatrix\r\nDictionary Analysis\r\nNRC\r\nNRC as DFM\r\nNRC Polarity Plot\r\nNRC Sample Results\r\nLSD 2015\r\nLSD Sample Results\r\nLSD Polarity Plot\r\nGeneral Inquirer\r\nGeneral Inquirer Sample\r\nResults\r\nGeneral Inquirer Polarity\r\nPlot\r\n\r\nComparison Study\r\nCreate Data Frame of All\r\nResults\r\nCorrelation\r\nCorrelation of NRC\r\nSentiments\r\nLinear Model Testing\r\n\r\nVisualizing NRC Sentiment\r\nMain Headlines\r\nPrint Headlines\r\nClustered Bar Chart\r\n\r\n\r\nSummary\r\nCitations\r\n\r\nComparing\r\nMain vs. Print Headlines in the New York Times\r\nThis is a continuation of my analysis started in my last\r\npost.\r\nMaking\r\nDifferent Choices on Inclusion of Observations\r\nIn my initial analysis of the headline data, I used the scope of the\r\nproject from a prior term to set the parameters for the headline API\r\nsearch. I decided to expand the term to include just “Afghanistan”\r\nrather than “Afghanistan withdrawal” for a couple of reasons. First, to\r\nincrease the volume of observations and increase reliability. Second,\r\nbecause I want to look at the comparison between the two search terms\r\nfor any change.\r\nGathering Data\r\nPrevious Process\r\nThe data was pulled via API using the same process as in my first\r\nphase of the comparison research, with the only change in the query term\r\n“Afghanistan” as opposed to “Afghanistan Withdrawal”. This led to a\r\nsignificantly larger dataset for comparison, though most of the increase\r\nin count was filtered out due to their classification as not\r\nnews-related. Still, the number of observed, relevant headlines\r\nincreased from 346 to 936 (for each type; main and print headlines).\r\nLoad Data\r\nNow to the active review of the data. Loading the data from my\r\ncollection phase:\r\n\r\n\r\nShow code\r\n\r\n#load data\r\nmain_headlines <- read.csv(\"afghanistan_headlines_main.csv\")\r\nmain_headlines <- as.data.frame(main_headlines)\r\n#turn into data frame\r\nprint_headlines <- read.csv(\"afghanistan_headlines_print.csv\")\r\nprint_headlines <- as.data.frame(print_headlines)\r\n#inspect data\r\nhead(main_headlines)\r\n\r\n\r\n  doc_id       date\r\n1      1  7/17/2020\r\n2      2  8/30/2020\r\n3      3   6/2/2021\r\n4      4 12/20/2020\r\n5      5  9/11/2021\r\n6      6   9/1/2021\r\n                                                                           text\r\n1  $174 Million Afghan Drone Program Is Riddled With Problems, U.S. Report Says\r\n2          ‘A Hail Mary’: Psychedelic Therapy Draws Veterans to Jungle Retreats\r\n3   ‘Come On In, Boys’: A Wave of the Hand Sets Off Spain-Morocco Migrant Fight\r\n4 ‘Covid Can’t Compete.’ In a Place Mired in War, the Virus Is an Afterthought.\r\n5    ‘Everything Changed Overnight’: Afghan Reporters Face an Intolerant Regime\r\n6      ‘Finally, I Am Safe’: U.S. Air Base Becomes Temporary Refuge for Afghans\r\n\r\nShow code\r\n\r\nhead(print_headlines)\r\n\r\n\r\n  doc_id       date\r\n1      1  7/17/2020\r\n2      2  8/30/2020\r\n3      3   6/2/2021\r\n4      4 12/20/2020\r\n5      5  9/11/2021\r\n6      6   9/1/2021\r\n                                                                            text\r\n1 $174 Million Drone Program for Afghans Is Riddled With Problems, Pentagon Says\r\n2                Psychedelic Therapy In the Jungle Soothes The Pain for Veterans\r\n3                                 Morocco Sends Spanish Outpost a Migrant Influx\r\n4         ‘It’s a Lie’: Denial and Skepticism Permeate a Nation Embroiled in War\r\n5                                     ‘Everything Changed’: Media Face Crackdown\r\n6         ‘Finally, I Am Safe’: Thousands Find Temporary Refuge at U.S. Air Base\r\n\r\nShow code\r\n\r\nall_results <- read.csv(\"all_results.csv\")\r\n\r\n\r\n\r\nCreate Corpus\r\n\r\n\r\nShow code\r\n\r\nmain_corpus <- corpus(main_headlines, docid_field = \"doc_id\", text_field = \"text\")\r\nprint_corpus <- corpus(print_headlines, docid_field = \"doc_id\", text_field = \"text\")\r\n\r\n\r\n\r\nAssign Type to Docvars\r\n\r\n\r\nShow code\r\n\r\nmain_corpus$type <- \"Main Headline\"\r\nprint_corpus$type <- \"Print Headline\"\r\n\r\ndocvars(main_corpus, field = \"type\") <- main_corpus$type\r\ndocvars(print_corpus, field = \"type\") <- print_corpus$type\r\n\r\n\r\n\r\nTokenization\r\nI want to optimize pre-processing by removing the “�” symbol that has\r\nplagued me since starting working with this API by using\r\n“remove_symbols=TRUE” in addition to removing the punctuation when\r\ntokenizing. I also want to remove stopwords. I do NOT want to use\r\nstemming at this point.\r\nMain Headlines\r\n\r\n\r\nShow code\r\n\r\nmain_tokens <- tokens(main_corpus) %>%\r\n  tokens(main_corpus, remove_punct = TRUE) %>%\r\n  tokens(main_corpus, remove_numbers = TRUE) %>%\r\n  tokens(main_corpus, remove_symbols = TRUE) %>%\r\n  tokens_remove(stopwords(\"english\")) %>%\r\n  tokens_remove(c(\"s\"))\r\n\r\nmain_dfm <- dfm(main_tokens)\r\n\r\nlength(main_tokens)\r\n\r\n\r\n[1] 936\r\n\r\nShow code\r\n\r\nprint(main_tokens)\r\n\r\n\r\nTokens consisting of 936 documents and 2 docvars.\r\n1 :\r\n[1] \"Million\"  \"Afghan\"   \"Drone\"    \"Program\"  \"Riddled\"  \"Problems\"\r\n[7] \"U.S\"      \"Report\"   \"Says\"    \r\n\r\n2 :\r\n[1] \"Hail\"        \"Mary\"        \"Psychedelic\" \"Therapy\"    \r\n[5] \"Draws\"       \"Veterans\"    \"Jungle\"      \"Retreats\"   \r\n\r\n3 :\r\n[1] \"Come\"          \"Boys\"          \"Wave\"          \"Hand\"         \r\n[5] \"Sets\"          \"Spain-Morocco\" \"Migrant\"       \"Fight\"        \r\n\r\n4 :\r\n[1] \"Covid\"        \"Compete\"      \"Place\"        \"Mired\"       \r\n[5] \"War\"          \"Virus\"        \"Afterthought\"\r\n\r\n5 :\r\n[1] \"Everything\" \"Changed\"    \"Overnight\"  \"Afghan\"     \"Reporters\" \r\n[6] \"Face\"       \"Intolerant\" \"Regime\"    \r\n\r\n6 :\r\n[1] \"Finally\"   \"Safe\"      \"U.S\"       \"Air\"       \"Base\"     \r\n[6] \"Becomes\"   \"Temporary\" \"Refuge\"    \"Afghans\"  \r\n\r\n[ reached max_ndoc ... 930 more documents ]\r\n\r\nPrint Headlines\r\n\r\n\r\nShow code\r\n\r\nprint_tokens <- tokens(print_corpus) %>%\r\n  tokens(print_corpus, remove_punct = TRUE) %>%\r\n  tokens(print_corpus, remove_numbers = TRUE) %>%\r\n  tokens(print_corpus, remove_symbols = TRUE) %>%\r\n  tokens_remove(stopwords(\"english\")) %>%\r\n  tokens_remove(c(\"s\"))\r\n\r\nmain_dfm <- dfm(print_tokens)\r\n\r\nlength(print_tokens)\r\n\r\n\r\n[1] 936\r\n\r\nShow code\r\n\r\nprint(print_tokens)\r\n\r\n\r\nTokens consisting of 936 documents and 2 docvars.\r\n1 :\r\n[1] \"Million\"  \"Drone\"    \"Program\"  \"Afghans\"  \"Riddled\"  \"Problems\"\r\n[7] \"Pentagon\" \"Says\"    \r\n\r\n2 :\r\n[1] \"Psychedelic\" \"Therapy\"     \"Jungle\"      \"Soothes\"    \r\n[5] \"Pain\"        \"Veterans\"   \r\n\r\n3 :\r\n[1] \"Morocco\" \"Sends\"   \"Spanish\" \"Outpost\" \"Migrant\" \"Influx\" \r\n\r\n4 :\r\n[1] \"Lie\"        \"Denial\"     \"Skepticism\" \"Permeate\"   \"Nation\"    \r\n[6] \"Embroiled\"  \"War\"       \r\n\r\n5 :\r\n[1] \"Everything\" \"Changed\"    \"Media\"      \"Face\"       \"Crackdown\" \r\n\r\n6 :\r\n[1] \"Finally\"   \"Safe\"      \"Thousands\" \"Find\"      \"Temporary\"\r\n[6] \"Refuge\"    \"U.S\"       \"Air\"       \"Base\"     \r\n\r\n[ reached max_ndoc ... 930 more documents ]\r\n\r\nDocument Feature Matrix\r\nAgain, this will show me the occurrence of words within each ‘doc’ or\r\nheadline observation.\r\n\r\n\r\nShow code\r\n\r\n#print dfm\r\nprint_dfm <- dfm(print_tokens)\r\n#main dfm\r\nmain_dfm <- dfm(main_tokens)\r\n#look at each dfm\r\nprint_dfm\r\n\r\n\r\nDocument-feature matrix of: 936 documents, 2,661 features (99.75% sparse) and 2 docvars.\r\n    features\r\ndocs million drone program afghans riddled problems pentagon says\r\n   1       1     1       1       1       1        1        1    1\r\n   2       0     0       0       0       0        0        0    0\r\n   3       0     0       0       0       0        0        0    0\r\n   4       0     0       0       0       0        0        0    0\r\n   5       0     0       0       0       0        0        0    0\r\n   6       0     0       0       0       0        0        0    0\r\n    features\r\ndocs psychedelic therapy\r\n   1           0       0\r\n   2           1       1\r\n   3           0       0\r\n   4           0       0\r\n   5           0       0\r\n   6           0       0\r\n[ reached max_ndoc ... 930 more documents, reached max_nfeat ... 2,651 more features ]\r\n\r\nShow code\r\n\r\nmain_dfm\r\n\r\n\r\nDocument-feature matrix of: 936 documents, 2,745 features (99.74% sparse) and 2 docvars.\r\n    features\r\ndocs million afghan drone program riddled problems u.s report says\r\n   1       1      1     1       1       1        1   1      1    1\r\n   2       0      0     0       0       0        0   0      0    0\r\n   3       0      0     0       0       0        0   0      0    0\r\n   4       0      0     0       0       0        0   0      0    0\r\n   5       0      1     0       0       0        0   0      0    0\r\n   6       0      0     0       0       0        0   1      0    0\r\n    features\r\ndocs hail\r\n   1    0\r\n   2    1\r\n   3    0\r\n   4    0\r\n   5    0\r\n   6    0\r\n[ reached max_ndoc ... 930 more documents, reached max_nfeat ... 2,735 more features ]\r\n\r\nWord Frequency Ratings\r\nAgain, I can take a preliminary look at the data frame from each of\r\nthe headlines to see the most frequent words after pre-processing.\r\nThe only significant change by removing “withdrawal” from my search\r\nterm is that the term “exit” is not present any longer on the print\r\nheadline frequency header. This is logical.\r\n\r\n\r\nShow code\r\n\r\n#create a word frequency variable and the rankings\r\n#main headlines\r\nmain_counts <- as.data.frame(sort(colSums(main_dfm),dec=T))\r\ncolnames(main_counts) <- c(\"Frequency\")\r\nmain_counts$Rank <- c(1:ncol(main_dfm))\r\nhead(main_counts)\r\n\r\n\r\n            Frequency Rank\r\nu.s               166    1\r\nafghan            151    2\r\nafghanistan       135    3\r\ntaliban           110    4\r\nbiden              95    5\r\nwar                64    6\r\n\r\nShow code\r\n\r\n#print headlines\r\nprint_counts <- as.data.frame(sort(colSums(print_dfm),dec=T))\r\ncolnames(print_counts) <- c(\"Frequency\")\r\nprint_counts$Rank <- c(1:ncol(print_dfm))\r\nhead(print_counts)\r\n\r\n\r\n            Frequency Rank\r\nu.s               171    1\r\nafghan            109    2\r\ntaliban           108    3\r\nafghanistan        84    4\r\nbiden              76    5\r\nwar                53    6\r\n\r\nFeature Co-Occurrence Matrix\r\nNow I can take a look at this network of feature co-occurrences\r\nagain.\r\nFirst, for the main headlines:\r\n\r\n\r\nShow code\r\n\r\n# create fcm from dfm\r\nmain_fcm <- fcm(main_dfm)\r\n# check the dimensions (i.e., the number of rows and the number of columnns)\r\n# of the matrix we created\r\ndim(main_fcm)\r\n\r\n\r\n[1] 2745 2745\r\n\r\nShow code\r\n\r\n# pull the top features\r\nmyFeatures <- names(topfeatures(main_fcm, 30))\r\n# retain only those top features as part of our matrix\r\nsmaller_main_fcm <- fcm_select(main_fcm, pattern = myFeatures, selection = \"keep\")\r\n# check dimensions\r\ndim(smaller_main_fcm)\r\n\r\n\r\n[1] 30 30\r\n\r\nShow code\r\n\r\n# compute size weight for vertices in network\r\nsize <- log(colSums(smaller_main_fcm))\r\n# create plot\r\ntextplot_network(smaller_main_fcm, vertex_size = size / max(size) * 3)\r\n\r\n\r\n\r\n\r\nand for the print headlines:\r\n\r\n\r\nShow code\r\n\r\n# create fcm from dfm\r\nprint_fcm <- fcm(print_dfm)\r\n# check the dimensions (i.e., the number of rows and the number of columnns)\r\n# of the matrix we created\r\ndim(print_fcm)\r\n\r\n\r\n[1] 2661 2661\r\n\r\nShow code\r\n\r\n# pull the top features\r\nmyFeatures <- names(topfeatures(print_fcm, 30))\r\n# retain only those top features as part of our matrix\r\nsmaller_print_fcm <- fcm_select(print_fcm, pattern = myFeatures, selection = \"keep\")\r\n# check dimensions\r\ndim(smaller_print_fcm)\r\n\r\n\r\n[1] 30 30\r\n\r\nShow code\r\n\r\n# compute size weight for vertices in network\r\nsize <- log(colSums(smaller_print_fcm))\r\n# create plot\r\ntextplot_network(smaller_print_fcm, vertex_size = size / max(size) * 3)\r\n\r\n\r\n\r\n\r\nThe resulting matrices have definitely changed, at least\r\nslightly.\r\nDictionary Analysis\r\nTo compare equally both this and my initial sentiment analysis, I am\r\ngoing to use the three dictionaries we used in the course tutorial, the\r\nNRC, LSD(2015) and General Inquiry dictionaries.\r\nNRC\r\nI am first using the “liwcalike()” function from the\r\nquanteda.dictionaries package to apply the NRC dictionary. I can take a\r\nlook at the head or tail and choose to look at a snapshot of the\r\nsentiments that have been applied to the corpus for each text group.\r\nJust at first glance, I can again see some differences in the\r\nscoring.\r\n\r\n\r\nShow code\r\n\r\n#use liwcalike() to estimate sentiment using NRC dictionary\r\n#for main headlines\r\nmain_sentiment_nrc <- liwcalike(as.character(main_corpus), data_dictionary_NRC)\r\nhead(main_sentiment_nrc)[7:12]\r\n\r\n\r\n  anger anticipation disgust fear   joy negative\r\n1  0.00         0.00    0.00 0.00  0.00    13.33\r\n2  0.00         0.00    0.00 7.69  0.00     7.69\r\n3  5.00         0.00    0.00 5.00  0.00     5.00\r\n4  0.00         0.00    0.00 5.26  0.00    10.53\r\n5  8.33         0.00    8.33 8.33  0.00     8.33\r\n6  0.00         5.88    5.88 0.00 11.76     0.00\r\n\r\nShow code\r\n\r\n#and print headlines\r\nprint_sentiment_nrc <- liwcalike(as.character(print_corpus), data_dictionary_NRC)\r\nhead(print_sentiment_nrc)[11:16]\r\n\r\n\r\n    joy negative positive sadness surprise trust\r\n1  0.00    14.29     0.00    0.00     0.00  0.00\r\n2  0.00    10.00     0.00   10.00     0.00  0.00\r\n3  0.00     0.00     0.00    0.00     0.00  0.00\r\n4  0.00    26.67     0.00    6.67     0.00  6.67\r\n5  0.00     0.00     0.00    0.00     0.00  0.00\r\n6 11.76     0.00    11.76    0.00     5.88 17.65\r\n\r\nNRC as DFM\r\nI can also put the results into a document feature matrix for each\r\ntext group:\r\n\r\n\r\nShow code\r\n\r\n#convert tokens from each headline data set to DFM using the dictionary \"NRC\"\r\nmain_nrc <- dfm(main_tokens) %>%\r\n  dfm_lookup(data_dictionary_NRC)\r\nprint_nrc <- dfm(print_tokens) %>%\r\n  dfm_lookup(data_dictionary_NRC)\r\n\r\ndim(main_nrc)\r\n\r\n\r\n[1] 936  10\r\n\r\nShow code\r\n\r\nmain_nrc\r\n\r\n\r\nDocument-feature matrix of: 936 documents, 10 features (67.61% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            0       0    0   0        2        0       0\r\n   2     0            0       0    1   0        1        1       0\r\n   3     1            0       0    1   0        1        0       0\r\n   4     0            0       0    1   0        2        0       0\r\n   5     1            0       1    1   0        1        0       1\r\n   6     0            1       1    0   2        0        2       0\r\n    features\r\ndocs surprise trust\r\n   1        0     0\r\n   2        0     1\r\n   3        0     0\r\n   4        0     0\r\n   5        0     0\r\n   6        1     3\r\n[ reached max_ndoc ... 930 more documents ]\r\n\r\nShow code\r\n\r\ndim(print_nrc)\r\n\r\n\r\n[1] 936  10\r\n\r\nShow code\r\n\r\nprint_nrc\r\n\r\n\r\nDocument-feature matrix of: 936 documents, 10 features (69.21% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            0       0    0   0        2        0       0\r\n   2     0            0       0    2   0        1        0       1\r\n   3     0            0       0    1   0        0        0       0\r\n   4     1            0       1    1   0        4        0       1\r\n   5     0            0       0    0   0        0        0       0\r\n   6     0            1       1    0   2        0        2       0\r\n    features\r\ndocs surprise trust\r\n   1        0     0\r\n   2        0     0\r\n   3        0     0\r\n   4        0     1\r\n   5        0     0\r\n   6        1     3\r\n[ reached max_ndoc ... 930 more documents ]\r\n\r\nNRC Polarity Plot\r\nAnd use the information in a data frame to plot the output as\r\nrepresented by a calculation for polarity:\r\n\r\n\r\nShow code\r\n\r\nlibrary(cowplot)\r\n#for the main headlines\r\ndf_main_nrc <- convert(main_nrc, to = \"data.frame\")\r\ndf_main_nrc$polarity <- (df_main_nrc$positive - df_main_nrc$negative)/(df_main_nrc$positive + df_main_nrc$negative)\r\ndf_main_nrc$polarity[which((df_main_nrc$positive + df_main_nrc$negative) == 0)] <- 0\r\n\r\nggplot(df_main_nrc) + \r\n  geom_histogram(aes(x=polarity), bins = 15) + \r\n  theme_minimal_hgrid()\r\n\r\n\r\n\r\nShow code\r\n\r\n#and the print headlines\r\ndf_print_nrc <- convert(print_nrc, to = \"data.frame\")\r\ndf_print_nrc$polarity <- (df_print_nrc$positive - df_print_nrc$negative)/(df_print_nrc$positive + df_print_nrc$negative)\r\ndf_print_nrc$polarity[which((df_print_nrc$positive + df_print_nrc$negative) == 0)] <- 0\r\n\r\nggplot(df_print_nrc) + \r\n  geom_histogram(aes(x=polarity), bins = 15) + \r\n  theme_minimal_hgrid()\r\n\r\n\r\n\r\n\r\nNRC Sample Results\r\nLooking at the headlines that are indicated as “1”, or positive in\r\nsentiment, this expanded corpus reflects more positivity than the top\r\nresults from the smaller corpus.\r\n\r\n\r\nShow code\r\n\r\nhead(main_corpus[which(df_main_nrc$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n6 :\r\n\"‘Finally, I Am Safe’: U.S. Air Base Becomes Temporary Refuge...\"\r\n\r\n8 :\r\n\"‘Football Is Like Food’: Afghan Female Soccer Players Find a...\"\r\n\r\n18 :\r\n\"‘Is Austin on Your List?’: Biden’s Pentagon Pick Rose Despit...\"\r\n\r\n29 :\r\n\"‘We Have to Try’: Lawmakers Rush to Assist in Afghanistan Ev...\"\r\n\r\n40 :\r\n\"4 Takeaways From the U.S. Deal With the Taliban\"\r\n\r\n43 :\r\n\"98 Countries Pledge to Accept Afghans After U.S. Military De...\"\r\n\r\nShow code\r\n\r\nhead(print_corpus[which(df_print_nrc$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n6 :\r\n\"‘Finally, I Am Safe’: Thousands Find Temporary Refuge at U.S...\"\r\n\r\n15 :\r\n\"Veterans Feel Urgency to Aid Afghan Allies\"\r\n\r\n18 :\r\n\"How Biden’s Defense Nominee Overcame Barriers to Diversity\"\r\n\r\n25 :\r\n\"How Biden, by Turns Genial and Blunt, Built Diplomatic Bridg...\"\r\n\r\n45 :\r\n\"Rescue Flight To Germany Inspires Name For Newborn\"\r\n\r\n50 :\r\n\"A Call for the Return of Civility, And Truth as a Guiding Li...\"\r\n\r\nLSD 2015\r\nI am going to want to look at multiple dictionaries to see if one can\r\nbest apply to this data. Next, the LSD 2015 dictionary:\r\n\r\n\r\nShow code\r\n\r\n# convert main corpus to DFM using the LSD2015 dictionary\r\nmain_lsd2015 <- dfm(main_tokens) %>%\r\n  dfm_lookup(data_dictionary_LSD2015)\r\n# create main polarity measure for LSD2015\r\nmain_lsd2015 <- convert(main_lsd2015, to = \"data.frame\")\r\nmain_lsd2015$polarity <- (main_lsd2015$positive - main_lsd2015$negative)/(main_lsd2015$positive + main_lsd2015$negative)\r\nmain_lsd2015$polarity[which((main_lsd2015$positive + main_lsd2015$negative) == 0)] <- 0\r\n# convert print corpus to DFM using the LSD2015 dictionary\r\nprint_lsd2015 <- dfm(print_tokens) %>%\r\n  dfm_lookup(data_dictionary_LSD2015)\r\n# create print polarity measure for LSD2015\r\nprint_lsd2015 <- convert(print_lsd2015, to = \"data.frame\")\r\nprint_lsd2015$polarity <- (print_lsd2015$positive - print_lsd2015$negative)/(print_lsd2015$positive + print_lsd2015$negative)\r\nprint_lsd2015$polarity[which((print_lsd2015$positive + print_lsd2015$negative) == 0)] <- 0\r\n\r\n\r\n\r\nLSD Sample Results\r\nLooking at the headlines that are indicated as “1”, or positive in\r\nsentiment, I can again see why these specific headlines are being\r\nevaluated as ‘positive’ despite more aberrations than in the NRC\r\ndictionary.\r\n\r\n\r\nShow code\r\n\r\nhead(main_corpus[which(main_lsd2015$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n6 :\r\n\"‘Finally, I Am Safe’: U.S. Air Base Becomes Temporary Refuge...\"\r\n\r\n8 :\r\n\"‘Football Is Like Food’: Afghan Female Soccer Players Find a...\"\r\n\r\n19 :\r\n\"‘It’s Like Falling in Love’: Israeli Entrepreneurs Welcomed ...\"\r\n\r\n25 :\r\n\"‘Strategic Empathy’: How Biden’s Informal Diplomacy Shaped F...\"\r\n\r\n29 :\r\n\"‘We Have to Try’: Lawmakers Rush to Assist in Afghanistan Ev...\"\r\n\r\n43 :\r\n\"98 Countries Pledge to Accept Afghans After U.S. Military De...\"\r\n\r\nShow code\r\n\r\nhead(print_corpus[which(print_lsd2015$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n6 :\r\n\"‘Finally, I Am Safe’: Thousands Find Temporary Refuge at U.S...\"\r\n\r\n15 :\r\n\"Veterans Feel Urgency to Aid Afghan Allies\"\r\n\r\n29 :\r\n\"As Panicked Afghans Seek Help, Lawmakers Say, ‘We Have to Tr...\"\r\n\r\n31 :\r\n\"‘She Was Alone’: One Official’s Harrowing Escape From Kabul\"\r\n\r\n43 :\r\n\"98 Countries Pledge to Accept Afghans After U.S. Departure\"\r\n\r\n45 :\r\n\"Rescue Flight To Germany Inspires Name For Newborn\"\r\n\r\nLSD Polarity Plot\r\nAnd use the information in a data frame to plot the output as\r\nrepresented by a calculation for polarity:\r\n\r\n\r\nShow code\r\n\r\n#for the main headlines\r\nggplot(main_lsd2015) + \r\n  geom_histogram(aes(x=polarity), bins = 15) + \r\n  theme_minimal_hgrid()\r\n\r\n\r\n\r\nShow code\r\n\r\n#and the print headlines\r\nggplot(print_lsd2015) + \r\n  geom_histogram(aes(x=polarity), bins = 15) + \r\n  theme_minimal_hgrid()\r\n\r\n\r\n\r\n\r\nGeneral Inquirer\r\nand the General Inquirer dictionary:\r\n\r\n\r\nShow code\r\n\r\n# convert main corpus to DFM using the General Inquirer dictionary\r\nmain_geninq <- dfm(main_tokens) %>%\r\n                    dfm_lookup(data_dictionary_geninqposneg)\r\n# create main polarity measure for GenInq\r\nmain_geninq <- convert(main_geninq, to = \"data.frame\")\r\nmain_geninq$polarity <- (main_geninq$positive - main_geninq$negative)/(main_geninq$positive + main_geninq$negative)\r\nmain_geninq$polarity[which((main_geninq$positive + main_geninq$negative) == 0)] <- 0\r\n# convert print corpus to DFM using the General Inquirer dictionary\r\nprint_geninq <- dfm(print_tokens) %>%\r\n                    dfm_lookup(data_dictionary_geninqposneg)\r\n# create print polarity measure for GenInq\r\nprint_geninq <- convert(print_geninq, to = \"data.frame\")\r\nprint_geninq$polarity <- (print_geninq$positive - print_geninq$negative)/(print_geninq$positive + print_geninq $negative)\r\nprint_geninq$polarity[which((print_geninq$positive + print_geninq$negative) == 0)] <- 0\r\n\r\n\r\n\r\nGeneral Inquirer Sample\r\nResults\r\nLooking at the headlines that are indicated as “1”, or positive in\r\nsentiment, again - this one is even more of a mixed bag, with the\r\nsentiment rationale clear. However, it is also clear why the rationale\r\nis being used at the expense of subtle subject matter knowledge.\r\n\r\n\r\nShow code\r\n\r\nhead(main_corpus[which(main_geninq$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n6 :\r\n\"‘Finally, I Am Safe’: U.S. Air Base Becomes Temporary Refuge...\"\r\n\r\n8 :\r\n\"‘Football Is Like Food’: Afghan Female Soccer Players Find a...\"\r\n\r\n12 :\r\n\"‘I Forget About the World:’ Afghan Youth Find Escape in a Vi...\"\r\n\r\n19 :\r\n\"‘It’s Like Falling in Love’: Israeli Entrepreneurs Welcomed ...\"\r\n\r\n27 :\r\n\"‘They Just Left Us’: Greece Is Accused of Setting Migrants A...\"\r\n\r\n43 :\r\n\"98 Countries Pledge to Accept Afghans After U.S. Military De...\"\r\n\r\nShow code\r\n\r\nhead(print_corpus[which(print_geninq$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n6 :\r\n\"‘Finally, I Am Safe’: Thousands Find Temporary Refuge at U.S...\"\r\n\r\n15 :\r\n\"Veterans Feel Urgency to Aid Afghan Allies\"\r\n\r\n18 :\r\n\"How Biden’s Defense Nominee Overcame Barriers to Diversity\"\r\n\r\n33 :\r\n\"‘Why Do We Deserve to Die?’ Burying Hazara Girls in Kabul\"\r\n\r\n42 :\r\n\"New Wave of Refugees Faces A Much Chillier U.S. Welcome\"\r\n\r\n43 :\r\n\"98 Countries Pledge to Accept Afghans After U.S. Departure\"\r\n\r\nGeneral Inquirer Polarity\r\nPlot\r\nAnd use the information in a data frame to plot the output as\r\nrepresented by a calculation for polarity:\r\n\r\n\r\nShow code\r\n\r\n#for the main headlines\r\nggplot(main_geninq) + \r\n  geom_histogram(aes(x=polarity), bins = 15) + \r\n  theme_minimal_hgrid()\r\n\r\n\r\n\r\nShow code\r\n\r\n#and the print headlines\r\nggplot(print_geninq) + \r\n  geom_histogram(aes(x=polarity), bins = 15) + \r\n  theme_minimal_hgrid()\r\n\r\n\r\n\r\n\r\nComparison Study\r\nCreate Data Frame of All\r\nResults\r\nNow I’m going to be able to compare the different dictionary scores\r\nin one data frame for each type of headline.\r\nMain Headlines\r\n\r\n\r\nShow code\r\n\r\n# create unique names for each main headline dataframe\r\ncolnames(df_main_nrc) <- paste(\"nrc\", colnames(df_main_nrc), sep = \"_\")\r\ncolnames(main_lsd2015) <- paste(\"lsd2015\", colnames(main_lsd2015), sep = \"_\")\r\ncolnames(main_geninq) <- paste(\"geninq\", colnames(main_geninq), sep = \"_\")\r\n# now let's compare our estimates\r\nmain_sent <- merge(df_main_nrc, main_lsd2015, by.x = \"nrc_doc_id\", by.y = \"lsd2015_doc_id\")\r\nmain_sent <- merge(main_sent, main_geninq, by.x = \"nrc_doc_id\", by.y = \"geninq_doc_id\")\r\nhead(main_sent)[1:5]\r\n\r\n\r\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear\r\n1          1         0                0           0        0\r\n2         10         0                0           0        2\r\n3        100         0                0           0        1\r\n4        101         0                2           0        0\r\n5        102         0                1           0        0\r\n6        103         1                1           1        1\r\n\r\nPrint Headlines\r\n\r\n\r\nShow code\r\n\r\n# create unique names for each print headline dataframe\r\ncolnames(df_print_nrc) <- paste(\"nrc\", colnames(df_print_nrc), sep = \"_\")\r\ncolnames(print_lsd2015) <- paste(\"lsd2015\", colnames(print_lsd2015), sep = \"_\")\r\ncolnames(print_geninq) <- paste(\"geninq\", colnames(print_geninq), sep = \"_\")\r\n# now let's compare our estimates\r\nprint_sent <- merge(df_print_nrc, print_lsd2015, by.x = \"nrc_doc_id\", by.y = \"lsd2015_doc_id\")\r\nprint_sent <- merge(print_sent, print_geninq, by.x = \"nrc_doc_id\", by.y = \"geninq_doc_id\")\r\nhead(print_sent)[1:5]\r\n\r\n\r\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear\r\n1          1         0                0           0        0\r\n2         10         0                0           0        2\r\n3        100         0                0           0        1\r\n4        101         0                2           0        0\r\n5        102         0                1           0        0\r\n6        103         1                0           1        1\r\n\r\nShow code\r\n\r\nwrite.csv(main_sent, file=\"main_sent.csv\")\r\nwrite.csv(print_sent, file=\"print_sent.csv\")\r\n\r\n\r\n\r\nCorrelation\r\nNow that I have them all in a single data frame, it’s straightforward\r\nto figure out a bit about how well our different measures of polarity\r\nagree across the different approaches by looking at their correlation\r\nusing the “cor()” function.\r\nIt seems like the polarity of the headlines are more similar in this\r\nexpanded analysis.\r\nFor Main Headlines\r\n\r\n\r\nShow code\r\n\r\ncor(main_sent$nrc_polarity, main_sent$lsd2015_polarity)\r\n\r\n\r\n[1] 0.4619394\r\n\r\nShow code\r\n\r\ncor(main_sent$nrc_polarity, main_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4702233\r\n\r\nShow code\r\n\r\ncor(main_sent$lsd2015_polarity, main_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.5321535\r\n\r\nFor Print Headlines\r\n\r\n\r\nShow code\r\n\r\ncor(print_sent$nrc_polarity, print_sent$lsd2015_polarity)\r\n\r\n\r\n[1] 0.4700115\r\n\r\nShow code\r\n\r\ncor(print_sent$nrc_polarity, print_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4824528\r\n\r\nShow code\r\n\r\ncor(print_sent$lsd2015_polarity, print_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4933739\r\n\r\nCorrelation of NRC\r\nSentiments\r\nI can take a quick visual look at the correlation between sentiments\r\ndetected in both sets of headlines using the “GGally” package. There\r\nseems to be very little difference in that regard.\r\nMain Headlines\r\n\r\n\r\nShow code\r\n\r\nlibrary(GGally)\r\n\r\nmain_nrc_only<- read.csv(\"main_sent_nrc_only.csv\")\r\nggcorr(main_nrc_only, method = c(\"everything\", \"pearson\"))\r\n\r\n\r\n\r\n\r\nPrint Headlines\r\n\r\n\r\nShow code\r\n\r\nprint_nrc_only<- read.csv(\"print_sent_nrc_only.csv\")\r\nggcorr(print_nrc_only, method = c(\"everything\", \"pearson\"))\r\n\r\n\r\n\r\n\r\nLinear Model Testing\r\nFinally, I want to visually look at the correlations or positive and\r\nnegative sentiments as my starting point for understanding relationships\r\nbetween both my sentiment analyses and dictionaries. I’ll start by\r\ndividing the sentiment scores for positive and negative from each text\r\nsource into its own object and change column names to make them unique\r\nexcept for ‘doc_id’ for joining them into one data frame.\r\n\r\n\r\nShow code\r\n\r\ncorr_main <- main_sent %>%\r\n  select(nrc_doc_id, nrc_polarity, lsd2015_polarity, geninq_polarity )\r\ncolnames(corr_main) <- c(\"doc_id\", \"main_nrc\", \"main_lsd\", \"main_geninq\")\r\ncorr_print <- print_sent %>%\r\n  select(nrc_doc_id, nrc_polarity, lsd2015_polarity, geninq_polarity )\r\ncolnames(corr_print) <- c(\"doc_id\", \"print_nrc\", \"print_lsd\", \"print_geninq\")\r\n\r\ncorr_matrix <- join(corr_main, corr_print, by = \"doc_id\")\r\nhead(corr_matrix)\r\n\r\n\r\n  doc_id main_nrc main_lsd main_geninq  print_nrc  print_lsd\r\n1      1       -1       -1           0 -1.0000000 -1.0000000\r\n2     10       -1       -1          -1 -1.0000000 -1.0000000\r\n3    100       -1       -1          -1 -1.0000000 -1.0000000\r\n4    101        1        0           1  0.3333333 -0.3333333\r\n5    102        1        1           1  1.0000000  1.0000000\r\n6    103        0        0           0 -1.0000000 -1.0000000\r\n  print_geninq\r\n1    0.0000000\r\n2   -1.0000000\r\n3   -1.0000000\r\n4    0.3333333\r\n5    1.0000000\r\n6   -1.0000000\r\n\r\nThen I can look at the model for each relationship\r\nNRC\r\n\r\n\r\nShow code\r\n\r\n#run the linear model of main vs. print correlation in the NRC dictionary\r\nlm_nrc <- lm(main_nrc~print_nrc, data = corr_matrix)\r\nsummary(lm_nrc)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = main_nrc ~ print_nrc, data = corr_matrix)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.36631 -0.44844  0.09262  0.55156  1.55156 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -0.09262    0.02090  -4.432 1.05e-05 ***\r\nprint_nrc    0.45893    0.02838  16.173  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.6348 on 934 degrees of freedom\r\nMultiple R-squared:  0.2188,    Adjusted R-squared:  0.2179 \r\nF-statistic: 261.6 on 1 and 934 DF,  p-value: < 2.2e-16\r\n\r\nLSD\r\n\r\n\r\nShow code\r\n\r\n#run the linear model of main vs. print correlation in the LSD dictionary\r\nlm_lsd <- lm(main_lsd~print_lsd, data = corr_matrix)\r\nsummary(lm_lsd)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = main_lsd ~ print_lsd, data = corr_matrix)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1.3257 -0.3098 -0.3098  0.3516  1.6903 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -0.18226    0.02113  -8.627   <2e-16 ***\r\nprint_lsd    0.50799    0.02739  18.549   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.5908 on 934 degrees of freedom\r\nMultiple R-squared:  0.2692,    Adjusted R-squared:  0.2684 \r\nF-statistic: 344.1 on 1 and 934 DF,  p-value: < 2.2e-16\r\n\r\nGeneral Inquiry\r\n\r\n\r\nShow code\r\n\r\n#run the linear model of main vs. print correlation in the General Inquiry dictionary\r\nlm_geninq <- lm(main_geninq~print_geninq, data = corr_matrix)\r\nsummary(lm_geninq)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = main_geninq ~ print_geninq, data = corr_matrix)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1.2563 -0.4979  0.1229  0.5021  1.5021 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -0.12292    0.02168  -5.669 1.91e-08 ***\r\nprint_geninq  0.37920    0.02979  12.730  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.6409 on 934 degrees of freedom\r\nMultiple R-squared:  0.1478,    Adjusted R-squared:  0.1469 \r\nF-statistic:   162 on 1 and 934 DF,  p-value: < 2.2e-16\r\n\r\nAnd try to look at if there is any meaningful difference between the\r\nmodels. Despite the differences in the expanded dataset from the primary\r\none, there stil does not seem to be any meaningful difference between\r\nmain and print headlines based on these analyses.\r\n\r\n\r\nShow code\r\n\r\n#create a data frame from the NRC model results\r\ntidynrc <- tidy(lm_nrc, conf.int = FALSE) \r\n#round the results to 3 decimal points\r\ntidynrc <- tidynrc %>%\r\n  mutate_if(is.numeric, round, 3)\r\ntidynrc$model <- c(\"nrc\")\r\n\r\n#create a data frame from the LSD model results\r\ntidylsd <- tidy(lm_lsd, conf.int = FALSE) \r\n#round the results to 3 decimal points\r\ntidylsd <- tidylsd %>%\r\n  mutate_if(is.numeric, round, 3)\r\ntidylsd$model <- c(\"lsd\")\r\n\r\n#create a data frame from the Gen Inq model results\r\ntidygeninq <- tidy(lm_geninq, conf.int = FALSE) \r\n#round the results to 3 decimal points\r\ntidygeninq <- tidygeninq %>%\r\n  mutate_if(is.numeric, round, 3)\r\ntidygeninq$model <- c(\"geninq\")\r\n\r\ntidy_all <- do.call(\"rbind\", list(tidynrc, tidylsd, tidygeninq))\r\n\r\ntidy_all\r\n\r\n\r\n# A tibble: 6 x 6\r\n  term         estimate std.error statistic p.value model \r\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl> <chr> \r\n1 (Intercept)    -0.093     0.021     -4.43       0 nrc   \r\n2 print_nrc       0.459     0.028     16.2        0 nrc   \r\n3 (Intercept)    -0.182     0.021     -8.63       0 lsd   \r\n4 print_lsd       0.508     0.027     18.5        0 lsd   \r\n5 (Intercept)    -0.123     0.022     -5.67       0 geninq\r\n6 print_geninq    0.379     0.03      12.7        0 geninq\r\n\r\nVisualizing NRC Sentiment\r\nMain Headlines\r\n\r\n\r\nShow code\r\n\r\n#main headlines\r\nhead(main_nrc)\r\n\r\n\r\nDocument-feature matrix of: 6 documents, 10 features (65.00% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            0       0    0   0        2        0       0\r\n   2     0            0       0    1   0        1        1       0\r\n   3     1            0       0    1   0        1        0       0\r\n   4     0            0       0    1   0        2        0       0\r\n   5     1            0       1    1   0        1        0       1\r\n   6     0            1       1    0   2        0        2       0\r\n    features\r\ndocs surprise trust\r\n   1        0     0\r\n   2        0     1\r\n   3        0     0\r\n   4        0     0\r\n   5        0     0\r\n   6        1     3\r\n\r\nShow code\r\n\r\n#transpose\r\nmain_df <-data.frame(t(main_nrc))\r\n#The function rowSums computes column sums across rows for each level of a grouping variable.\r\ndf_new <- data.frame(rowSums(main_df[2:937]))\r\n#Transformation and cleaning\r\nnames(df_new)[1] <- \"count\"\r\ndf_new <- cbind(\"sentiment\" = rownames(df_new), df_new)\r\nrownames(df_new) <- NULL\r\ndf_new2<-df_new[1:10,]\r\ndf_new2 <- read.csv(\"df_new2.csv\")\r\n#Plot One - count of words associated with each sentiment\r\nquickplot(sentiment, data=df_new2, weight=count, geom=\"bar\", fill=sentiment, ylab=\"count\")+ggtitle(\"Main Headline Sentiments\")\r\n\r\n\r\n\r\n\r\nPrint Headlines\r\n\r\n\r\nShow code\r\n\r\n#print headlines\r\nhead(print_nrc)\r\n\r\n\r\nDocument-feature matrix of: 6 documents, 10 features (71.67% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            0       0    0   0        2        0       0\r\n   2     0            0       0    2   0        1        0       1\r\n   3     0            0       0    1   0        0        0       0\r\n   4     1            0       1    1   0        4        0       1\r\n   5     0            0       0    0   0        0        0       0\r\n   6     0            1       1    0   2        0        2       0\r\n    features\r\ndocs surprise trust\r\n   1        0     0\r\n   2        0     0\r\n   3        0     0\r\n   4        0     1\r\n   5        0     0\r\n   6        1     3\r\n\r\nShow code\r\n\r\n#transpose\r\nmain_df2 <-data.frame(t(print_nrc))\r\n#The function rowSums computes column sums across rows for each level of a grouping variable.\r\ndf_new3 <- data.frame(rowSums(main_df2[2:937]))\r\n#Transformation and cleaning\r\nnames(df_new3)[1] <- \"count\"\r\ndf_new3 <- cbind(\"sentiment\" = rownames(df_new3), df_new3)\r\nrownames(df_new3) <- NULL\r\ndf_new4<-df_new3[1:10,]\r\ndf_new4 <- read.csv(\"df_new4.csv\")\r\n#Plot One - count of words associated with each sentiment\r\nquickplot(sentiment, data=df_new4, weight=count, geom=\"bar\", fill=sentiment, ylab=\"count\")+ggtitle(\"Print Headline Sentiments\")\r\n\r\n\r\n\r\n\r\nClustered Bar Chart\r\n\r\n\r\nShow code\r\n\r\n#load data frame with both headline sentiments from NRC\r\ndf_dual <- read.csv(\"df_dual.csv\")\r\n\r\nggplot(df_dual,\r\n       aes(x = sentiment,\r\n           y = count,\r\n           fill = headline)) +\r\n  geom_bar(stat = \"identity\",\r\n           position = \"dodge\") +\r\n  scale_fill_manual(values=c(\"#993333\", \"#336699\")) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nSummary\r\nAlthough I was not able to find any statistically meaningful,\r\nmeasurable difference between the sentiments of print vs. main headlines\r\nanalyzed in this project, it is still a valid observation that there is\r\nan overall pattern to be observed.\r\nSpecifically, there is a pattern that the print headlines carry a\r\nlower level of emotionally weighted words than the online headlines. I\r\ncan’t make a factual observation, but I would like to do further,\r\nexpanded studies in articles in this research path to investigate the\r\nhypothesis.\r\nCitations\r\nCitations:\r\nThis research makes use of the NRC\r\nWord-Emotion Association Lexicon, created by Saif Mohammad and Peter\r\nTurney at the National Research Council Canada.\r\nThis research makes use of the LSD Lexicoder Sentiment\r\nDictionary. This dataset was first published in Young, L. &\r\nSoroka, S. (2012). Affective News: The Automated Coding of Sentiment in\r\nPolitical Texts]. doi: 10.1080/10584609.2012.671234 . Political\r\nCommunication, 29(2), 205–231.\r\nThis research makes use of the General\r\nInquirer Lexicon, Stone, P. J. (1962). The general inquirer: A\r\ncomputer system for content analysis and retrieval based on the sentence\r\nas a unit of information. Harvard: Laboratory of Social Relations,\r\nHarvard University.\r\n\r\n\r\n\r\n",
    "preview": "posts/headline-expanded/000010.png",
    "last_modified": "2022-05-02T02:56:04-04:00",
    "input_file": {}
  },
  {
    "path": "posts/headline-analysis/",
    "title": "Analysis of Main vs. Print Headlines: Phase 1",
    "description": "Text as Data Project Headline Comparison Research Using API Query \"Afghanistan Withdrawal\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/NYT-Analysis/"
      }
    ],
    "date": "2022-04-17",
    "categories": [
      "text as data",
      "NYT text analysis project"
    ],
    "contents": "\r\n\r\nContents\r\nComparing\r\nMain vs. Print Headlines in the New York Times\r\nResearch Background\r\nPrior Project Details\r\nCurrent Project Initial\r\nPlan\r\nCurrent Project New\r\nPath\r\n\r\nMaking\r\nChoices on Inclusion of Observations\r\nGathering Data\r\nAPI Process Not Run\r\nLoad Data\r\nCreate Corpus\r\nAssign Type to Docvars\r\nTokenization &\r\nPre-Processing\r\n\r\nDocument Feature Matrix\r\nCreating the DFM\r\nCreating Word Frequency\r\nRankings\r\n\r\nFeature Co-Occurrence\r\nMatrix\r\nDictionary Analysis\r\nNRC\r\nNRC as DFM\r\nNRC Polarity Plot\r\nNRC Sample Results\r\nLSD 2015\r\nLSD Sample Results\r\nLSD Polarity Plot\r\nGeneral Inquirer\r\nGeneral Inquirer Sample\r\nResults\r\nGeneral Inquirer Polarity\r\nPlot\r\n\r\nComparison Study\r\nCreate Data Frame of All\r\nResults\r\nCorrelation\r\nCorrelation of NRC\r\nSentiments\r\nLinear Model Testing\r\n\r\n\r\nCitations\r\n\r\nComparing\r\nMain vs. Print Headlines in the New York Times\r\nResearch Background\r\nPrior Project Details\r\nDuring the Fall 2021 semester, my research group hand coded PDF\r\ncopies of articles resulting from a simple search on the websites of the\r\nNew York Times and Wall Street Journal from February 29, 2020 through\r\nSeptember 30, 2021 using the term “Afghanistan withdrawal”. The basic\r\nprocess was as follows: Utilizing the basic New York Times search\r\ninterface and printing to PDF was step 1, and then saving the page in a\r\nZotero bibiography for reference was step 2 for each article in the\r\nresult that fell into the world or U.S. news sections.\r\nOne thing I noticed was that when loading the PDF articles into\r\nNVivo12 for coding was that it was difficult to match the New York Times\r\nPDF titles generated by the site to the article citation information in\r\nZotero for many of the articles because they did not match. I realized\r\nthat in the process of saving the articles in Zotero, the headline/title\r\nwas saved from on the web version of the article; however, once the\r\narticle had been preserved by using the site’s “Print to PDF” function,\r\nthe article title that it used as a default file name was different than\r\nthe web version.\r\nCurrent Project Initial Plan\r\nThis semester, I initially began this project to be one expanding on\r\nlast semester’s research and looking to expand a machine analysis of\r\narticles pulling articles from a larger time period than the one used in\r\nour previous research. For my initial text collection, I collected\r\narticles using the New York Times API for the search query\r\n“Afghanistan”, and hoped to be able to analyze the full text of a larger\r\nrange of articles. However, I found that I am limited in that the\r\narticle search API for the New York Times does not pull the entire\r\narticle. Rather, I was able to pull for each article the\r\nabstract/summary, lead paragraph, and snippet as well as the keywords,\r\nauthors, sections, and url (with other various metadata). In an\r\nunexpected turn of events, I found that the API also returned the\r\narticle title text for both the print and online versions of each\r\narticle.\r\nThe API’s lack of full article text was not optimal for my purpose,\r\nwhich was ultimately to examine sentiment and co-occurence of various\r\nsources. As sources are not necessarily detailed in the lead paragraph\r\nor abstract of an article, I knew I needed to move to a different\r\nresearch path.\r\nCurrent Project New Path\r\nRemembering the differences in headlines from our manual coding\r\nresearch and noting that the API provides both headlines in the article\r\nsearch API, I turned my attention to analyzing the differences in the\r\nmain vs. print headlines for articles from the same research period as\r\nour first examination. Ideally, this will be something I can then expand\r\nto the entirety of the available data form the New York Times API from\r\nSeptember 11, 2001 onward. But I want to start with the same time frame\r\nas the last project for my initial research because as part of that\r\nproject, we engaged in a random, stratified sample of the articles\r\nobtained through the same search term and hand-coded them for sentiment\r\nafter reaching inter-coder reliability of 83.92%. It seems worthwhile to\r\ntake a look at how that manual sentiment analysis corresponds to\r\nstatistical methods we have been researching throughout this course.\r\nThis way, I can potentially use a sample of the full articles\r\ncollected in our previous research and take the additional step of\r\nanalyzing the sentiment of full articles and how they may or may not\r\nrelate to the differing headlines.\r\nMaking Choices on\r\nInclusion of Observations\r\nIn my initial look at the headline data, it was clear that not all of\r\nthe articles had different headlines; some are the same entries, and\r\nsome have “N/A” in the “print” version only, indicating they were\r\nonline-only stories. Although I initially felt inclined to leave the\r\n“N/A” observations in the analysis, I removed those observations as they\r\nwould not be relevant to my new research questions comparing the framing\r\nfor different audiences.\r\nI also removed whole sections where the API returned an observation\r\nas there was apparently use of the term “Afghanistan withdrawal”\r\nsomewhere in the article/entry, but the type of entry was clearly not\r\nbeing represented in the headline. For example, “Corrections” entries\r\nhave headlines consisting only of the term “Corrections” and the\r\ncorresponding date. Similar choices were made on the “Arts”, Books”, and\r\n“Podcasts” sections when entries are primarily the names of the things\r\nbeing reviewed that may have a reference to the Afghanistan withdrawal\r\nsomewhere in the text, but it is not relevant specifically to the\r\nwithdrawal time period being analyzed.\r\nWith few exceptions, this left the entirety of the “U.S.” and “World”\r\nnews sections, even if the content related to Afghanistan is not readily\r\nobservable by reading only the headline.\r\nGathering Data\r\nAPI Process Not Run\r\nTo pull the data, I had to reduce the queries into more workable\r\ngroups that would not time out, given the NYT API limits. I was able to\r\npull the articles by chunk, and bind them together.\r\n\r\n\r\nShow code\r\n\r\n# For articles from February 29, 2020 through April 30, 2021\r\n#url1 <- ('https://api.nytimes.com/svc/search/v2/articlesearch.json?begin_date=20200229&end_date=20210430&q=afghanistan&withdrawal&api-key=XXXXX')\r\n\r\n#query1 <- fromJSON(url1)\r\n\r\n#max.pages1 <- ceiling((query1$response$meta$hits[1] / 10)-1) \r\n\r\n#pages1 <- list()\r\n#for(i in 0:max.pages1){\r\n  #search1 <- fromJSON(paste0(url1, \"&page=\", i), flatten = TRUE) %>% data.frame() \r\n  #message(\"Retrieving page \", i)\r\n  #pages1[[i+1]] <- search1\r\n  #Sys.sleep(10)\r\n  #}\r\n\r\n#pages1[[i+1]] <- search1 \r\n#afghanistan_withdrawal_articles1 <- rbind_pages(pages1)\r\n\r\n#save(afghanistan_withdrawal_articles1,file=\"afghanistan_withdrawal_articles1.Rdata\")\r\n\r\n#For May 1 through September 30, 2021\r\n\r\n#url2 <- ('https://api.nytimes.com/svc/search/v2/articlesearch.json?begin_date=20210501&end_date=20210930&q=afghanistan&withdrawal&api-key=XXXXX')\r\n\r\n#query2 <- fromJSON(url2)\r\n\r\n#max.pages2 <- ceiling((query2$response$meta$hits[1] / 10)-1) \r\n\r\n#pages2 <- list()\r\n#for(i in 0:max.pages2){\r\n  #search2 <- fromJSON(paste0(url2, \"&page=\", i), flatten = TRUE) %>% data.frame() \r\n  #message(\"Retrieving page \", i)\r\n  #pages2[[i+1]] <- search2\r\n  #Sys.sleep(10)\r\n  #}\r\n\r\n#pages2[[i+1]] <- search2\r\n#afghanistan_withdrawal_articles2 <- rbind_pages(pages2)\r\n\r\n#save(afghanistan_withdrawal_articles2,file=\"afghanistan_withdrawal_articles2.Rdata\")\r\n\r\n\r\n# Create shell for data\r\n\r\n#afghanistan_withdrawal_articles <- c()\r\n#afghanistan_withdrawal_articles <- rbind_pages(c(pages1, pages2))\r\n#saveRDS(afghanistan_withdrawal_articles,file=\"afghanistan_withdrawal_articles_all.Rdata\")\r\n\r\n\r\n\r\nAfter compiling the data, I re-formatted the date column and saving\r\nthe formatted tibble for offline access.\r\n\r\n\r\nShow code\r\n\r\n#afghanistan_withdrawal_table<- as_tibble(cbind(\r\n  #date=afghanistan_withdrawal_articles$response.docs.pub_date,\r\n  #abstract=afghanistan_withdrawal_articles$response.docs.abstract,\r\n  #lead.paragraph=afghanistan_withdrawal_articles$response.docs.lead_paragraph,\r\n  #snippet=afghanistan_withdrawal_articles$response.docs.snippet,\r\n  #section.name=afghanistan_withdrawal_articles$response.docs.section_name,\r\n  #subsection.name=afghanistan_withdrawal_articles$response.docs.subsection_name,\r\n  #news.desk=afghanistan_withdrawal_articles$response.docs.news_desk,\r\n  #byline=afghanistan_withdrawal_articles$response.docs.byline.original,\r\n  #headline.main=afghanistan_withdrawal_articles$response.docs.headline.main,\r\n  #headline.print=afghanistan_withdrawal_articles$response.docs.headline.print_headline,\r\n  #headline.kicker=afghanistan_withdrawal_articles$response.docs.headline.kicker,\r\n  #material=afghanistan_withdrawal_articles$response.docs.type_of_material,\r\n  #url=afghanistan_withdrawal_articles$response.docs.web_url\r\n  #))\r\n\r\n#afghanistan_withdrawal_table$date <- substr(afghanistan_withdrawal_table$date, 1, nchar(afghanistan_withdrawal_table$date)-14)\r\n\r\n#afghanistan_withdrawal_table$date <- as.Date(afghanistan_withdrawal_table$date, \"%Y-%m-%d\")\r\n\r\n#save(afghanistan_withdrawal_table,file=\"afghanistan_withdrawal_table.Rdata\")\r\n\r\n#write.table(afghanistan_withdrawal_table, file = \"~/GitHub/DACSS.697D/Text as Data Spring22/afghanistan_withdrawal_table.csv\", sep=\",\", row.names=FALSE)\r\n\r\n\r\n\r\nLoad Data\r\nNow to the active review of the data. Loading the data from my\r\ncollection phase.\r\n\r\n\r\nShow code\r\n\r\n#load data\r\nmain_headlines <- read.csv(\"afghanistan_withdrawal_main.csv\")\r\nmain_headlines <- as.data.frame(main_headlines)\r\n#turn into data frame\r\nprint_headlines <- read.csv(\"afghanistan_withdrawal_print.csv\")\r\nprint_headlines <- as.data.frame(print_headlines)\r\n#inspect data\r\nhead(main_headlines)\r\n\r\n\r\n  article_id      date\r\n1          1 2/29/2020\r\n2          2 2/29/2020\r\n3          3  3/1/2020\r\n4          4  3/2/2020\r\n5          5  3/2/2020\r\n6          6  3/3/2020\r\n                                                                 headline_main\r\n1                              4 Takeaways From the U.S. Deal With the Taliban\r\n2    Taliban and U.S. Strike Deal to Withdraw American Troops From Afghanistan\r\n3           Afghanistan War Enters New Stage as U.S. Military Prepares to Exit\r\n4                 At Center of Taliban Deal, a U.S. Envoy Who Made It Personal\r\n5 U.S. Announces Troop Withdrawal in Afghanistan as Respite From Violence Ends\r\n6                                           Trump Speaks With a Taliban Leader\r\n\r\nShow code\r\n\r\nhead(print_headlines)\r\n\r\n\r\n  article_id      date\r\n1          1 2/29/2020\r\n2          2 2/29/2020\r\n3          3  3/1/2020\r\n4          4  3/2/2020\r\n5          5  3/2/2020\r\n6          6  3/3/2020\r\n                                                        headline_print\r\n1                                 Table Is Set For a Pullout And Talks\r\n2                              U.S. and Taliban Sign Withdrawal Accord\r\n3                                      A Mission Shift for Afghanistan\r\n4 At the Center of the Taliban Deal, a U.S. Envoy Who Made It Personal\r\n5                           U.S. Troop Reduction Begins in Afghanistan\r\n6               Pursuing Exit, Trump Talks  To a Leader Of the Taliban\r\n\r\nNotably, the number of observations has been significantly reduced\r\nfrom the API pull of ~700 to 346 due to the fact that I am only\r\nexamining headlines that are different and have eliminated incidences\r\nwhere there is no ‘alternative’ print article headline.\r\nCreate Corpus\r\n\r\n\r\nShow code\r\n\r\nmain_corpus <- corpus(main_headlines, docid_field = \"article_id\", text_field = \"headline_main\")\r\nprint_corpus <- corpus(print_headlines, docid_field = \"article_id\", text_field = \"headline_print\")\r\n\r\n\r\n\r\nAssign Type to Docvars\r\n\r\n\r\nShow code\r\n\r\nmain_corpus$type <- \"Main Headline\"\r\nprint_corpus$type <- \"Print Headline\"\r\ndocvars(main_corpus, field = \"type\") <- main_corpus$type\r\ndocvars(print_corpus, field = \"type\") <- print_corpus$type\r\n\r\n\r\n\r\nTokenization &\r\nPre-Processing\r\nNext, I need to take the corpus and create tokens, which are lists of\r\ncharacter vectors where each element of the list corresponds to an input\r\ndocument. This is where the pre-processing takes place.\r\nAfter many process posts, I finally realized how to remove the “�”\r\nsymbol that has plagued me since starting working with this API by using\r\n“remove_symbols=TRUE” in addition to removing the punctuation when\r\ntokenizing. I also want to remove stopwords. Finally, since I am\r\nremoving symbols, the first run of this process left me with many\r\nabandoned “s” characters, so I am going to remove those specifically as\r\nwell.\r\nI have decided NOT to engage in the stemming of words on this initial\r\nanalysis.\r\nMain Headlines\r\n\r\n\r\nShow code\r\n\r\nmain_tokens <- tokens(main_corpus) %>%\r\n  tokens(main_corpus, remove_punct = TRUE) %>%\r\n  tokens(main_corpus, remove_numbers = TRUE) %>%\r\n  tokens(main_corpus, remove_symbols = TRUE) %>%\r\n  tokens_remove(stopwords(\"english\")) %>%\r\n  tokens_remove(c(\"s\"))\r\n\r\nmain_dfm <- dfm(main_tokens)\r\n\r\nlength(main_tokens)\r\n\r\n\r\n[1] 346\r\n\r\nShow code\r\n\r\nprint(main_tokens)\r\n\r\n\r\nTokens consisting of 346 documents and 2 docvars.\r\n1 :\r\n[1] \"Takeaways\" \"U.S\"       \"Deal\"      \"Taliban\"  \r\n\r\n2 :\r\n[1] \"Taliban\"     \"U.S\"         \"Strike\"      \"Deal\"       \r\n[5] \"Withdraw\"    \"American\"    \"Troops\"      \"Afghanistan\"\r\n\r\n3 :\r\n[1] \"Afghanistan\" \"War\"         \"Enters\"      \"New\"        \r\n[5] \"Stage\"       \"U.S\"         \"Military\"    \"Prepares\"   \r\n[9] \"Exit\"       \r\n\r\n4 :\r\n[1] \"Center\"   \"Taliban\"  \"Deal\"     \"U.S\"      \"Envoy\"    \"Made\"    \r\n[7] \"Personal\"\r\n\r\n5 :\r\n[1] \"U.S\"         \"Announces\"   \"Troop\"       \"Withdrawal\" \r\n[5] \"Afghanistan\" \"Respite\"     \"Violence\"    \"Ends\"       \r\n\r\n6 :\r\n[1] \"Trump\"   \"Speaks\"  \"Taliban\" \"Leader\" \r\n\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\nPrint Headlines\r\n\r\n\r\nShow code\r\n\r\nprint_tokens <- tokens(print_corpus) %>%\r\n  tokens(print_corpus, remove_punct = TRUE) %>%\r\n  tokens(print_corpus, remove_numbers = TRUE) %>%\r\n  tokens(print_corpus, remove_symbols = TRUE) %>%\r\n  tokens_remove(stopwords(\"english\")) %>%\r\n  tokens_remove(c(\"s\"))\r\n\r\nmain_dfm <- dfm(print_tokens)\r\n\r\nlength(print_tokens)\r\n\r\n\r\n[1] 346\r\n\r\nShow code\r\n\r\nprint(print_tokens)\r\n\r\n\r\nTokens consisting of 346 documents and 2 docvars.\r\n1 :\r\n[1] \"Table\"   \"Set\"     \"Pullout\" \"Talks\"  \r\n\r\n2 :\r\n[1] \"U.S\"        \"Taliban\"    \"Sign\"       \"Withdrawal\" \"Accord\"    \r\n\r\n3 :\r\n[1] \"Mission\"     \"Shift\"       \"Afghanistan\"\r\n\r\n4 :\r\n[1] \"Center\"   \"Taliban\"  \"Deal\"     \"U.S\"      \"Envoy\"    \"Made\"    \r\n[7] \"Personal\"\r\n\r\n5 :\r\n[1] \"U.S\"         \"Troop\"       \"Reduction\"   \"Begins\"     \r\n[5] \"Afghanistan\"\r\n\r\n6 :\r\n[1] \"Pursuing\" \"Exit\"     \"Trump\"    \"Talks\"    \"Leader\"   \"Taliban\" \r\n\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\nDocument Feature Matrix\r\nCreating the DFM\r\nIn order to perform statistical analysis, I first have to extract a\r\nmatrix that will associate values for certain features of each document\r\nusing the quanteda “dfm()” function. This will show me the occurrence of\r\nwords within each ‘doc’ or headline observation.\r\n\r\n\r\nShow code\r\n\r\n#print dfm\r\nprint_dfm <- dfm(print_tokens)\r\n#main dfm\r\nmain_dfm <- dfm(main_tokens)\r\n#look at each dfm\r\nprint_dfm\r\n\r\n\r\nDocument-feature matrix of: 346 documents, 1,155 features (99.43% sparse) and 2 docvars.\r\n    features\r\ndocs table set pullout talks u.s taliban sign withdrawal accord\r\n   1     1   1       1     1   0       0    0          0      0\r\n   2     0   0       0     0   1       1    1          1      1\r\n   3     0   0       0     0   0       0    0          0      0\r\n   4     0   0       0     0   1       1    0          0      0\r\n   5     0   0       0     0   1       0    0          0      0\r\n   6     0   0       0     1   0       1    0          0      0\r\n    features\r\ndocs mission\r\n   1       0\r\n   2       0\r\n   3       1\r\n   4       0\r\n   5       0\r\n   6       0\r\n[ reached max_ndoc ... 340 more documents, reached max_nfeat ... 1,145 more features ]\r\n\r\nShow code\r\n\r\nmain_dfm\r\n\r\n\r\nDocument-feature matrix of: 346 documents, 1,208 features (99.40% sparse) and 2 docvars.\r\n    features\r\ndocs takeaways u.s deal taliban strike withdraw american troops\r\n   1         1   1    1       1      0        0        0      0\r\n   2         0   1    1       1      1        1        1      1\r\n   3         0   1    0       0      0        0        0      0\r\n   4         0   1    1       1      0        0        0      0\r\n   5         0   1    0       0      0        0        0      0\r\n   6         0   0    0       1      0        0        0      0\r\n    features\r\ndocs afghanistan war\r\n   1           0   0\r\n   2           1   0\r\n   3           1   1\r\n   4           0   0\r\n   5           1   0\r\n   6           0   0\r\n[ reached max_ndoc ... 340 more documents, reached max_nfeat ... 1,198 more features ]\r\n\r\nI can come back to this function and use it to further pre-process my\r\ndata, for example, to concatenate multi-word expressions if I see words\r\nhave been tokenized that should be together. For example:\r\ntokens(“New York City is located in the United States.”) %>%\r\ntokens_compound(pattern = phrase(c(“New York City”, “United\r\nStates”)))\r\nI can also come back and remove any word that may not have been a\r\nstopword but I later note to be non-significant to my analysis. This is\r\nalso where I can come back to employ stemming, if I feel it is\r\nappropriate. Basically, any pre-processing done during tokenization can\r\nalso be done in this process, if needed. For example:\r\ndfmat_inaug_post1990 <- dfm(dfmat_inaug_post1990, remove =\r\nstopwords(“unnecessary”), stem = TRUE)\r\nCreating Word Frequency\r\nRankings\r\nI can take a preliminary look at the data frame from each of the\r\nheadlines to see the most frequent words after pre-processing.\r\n\r\n\r\nShow code\r\n\r\n#create a word frequency variable and the rankings\r\n#main headlines\r\nmain_counts <- as.data.frame(sort(colSums(main_dfm),dec=T))\r\ncolnames(main_counts) <- c(\"Frequency\")\r\nmain_counts$Rank <- c(1:ncol(main_dfm))\r\nhead(main_counts)\r\n\r\n\r\n            Frequency Rank\r\nu.s               100    1\r\nafghanistan        87    2\r\nafghan             85    3\r\ntaliban            65    4\r\nbiden              53    5\r\nwar                30    6\r\n\r\nShow code\r\n\r\n#print headlines\r\nprint_counts <- as.data.frame(sort(colSums(print_dfm),dec=T))\r\ncolnames(print_counts) <- c(\"Frequency\")\r\nprint_counts$Rank <- c(1:ncol(print_dfm))\r\nhead(print_counts)\r\n\r\n\r\n            Frequency Rank\r\nu.s               100    1\r\ntaliban            66    2\r\nafghan             64    3\r\nafghanistan        56    4\r\nbiden              41    5\r\nexit               27    6\r\n\r\nFeature Co-Occurrence Matrix\r\nNow I can take a look at this network of feature co-occurrences for\r\nthe main headlines (FCM):\r\n\r\n\r\nShow code\r\n\r\n#create fcm from dfm\r\nmain_fcm <- fcm(main_dfm)\r\n#check the dimensions (i.e., the number of rows and the number of columns of the matrix we created\r\ndim(main_fcm)\r\n\r\n\r\n[1] 1208 1208\r\n\r\nShow code\r\n\r\n#pull the top features\r\nmyFeatures <- names(topfeatures(main_fcm, 20))\r\n#retain only those top features as part of our matrix\r\nsmaller_main_fcm <- fcm_select(main_fcm, pattern = myFeatures, selection = \"keep\")\r\n#check dimensions\r\ndim(smaller_main_fcm)\r\n\r\n\r\n[1] 20 20\r\n\r\nShow code\r\n\r\n#compute size weight for vertices in network\r\nsize <- log(colSums(smaller_main_fcm))\r\n#create plot\r\ntextplot_network(smaller_main_fcm, vertex_size = size / max(size) * 3)\r\n\r\n\r\n\r\n\r\nand for the print headlines:\r\n\r\n\r\nShow code\r\n\r\n# create fcm from dfm\r\nprint_fcm <- fcm(print_dfm)\r\n# check the dimensions (i.e., the number of rows and the number of columnns)\r\n# of the matrix we created\r\ndim(print_fcm)\r\n\r\n\r\n[1] 1155 1155\r\n\r\nShow code\r\n\r\n# pull the top features\r\nmyFeatures <- names(topfeatures(print_fcm, 20))\r\n# retain only those top features as part of our matrix\r\nsmaller_print_fcm <- fcm_select(print_fcm, pattern = myFeatures, selection = \"keep\")\r\n# check dimensions\r\ndim(smaller_print_fcm)\r\n\r\n\r\n[1] 20 20\r\n\r\nShow code\r\n\r\n# compute size weight for vertices in network\r\nsize <- log(colSums(smaller_print_fcm))\r\n# create plot\r\ntextplot_network(smaller_print_fcm, vertex_size = size / max(size) * 3)\r\n\r\n\r\n\r\n\r\nThis brings me to where I had previously stopped in my comparison and\r\nanalysis, and now that I’m able to produce a cleaner result, I’ll move\r\non to further analysis using the quanteda dictionary.\r\nDictionary Analysis\r\nFor my initial sentiment analysis, I am going to use the three\r\ndictionaries we used in the course tutorial, the NRC, LSD(2015) and\r\nGeneral Inquiry dictionaries.\r\nNRC\r\nI am first using the “liwcalike()” function from the\r\nquanteda.dictionaries package to apply the NRC dictionary. I can take a\r\nlook at the head or tail and choose to look at a snapshot of the\r\nsentiments that have been applied to the corpus for each text group.\r\nJust at first glance, I can see some differences in the scoring.\r\n\r\n\r\nShow code\r\n\r\n#use liwcalike() to estimate sentiment using NRC dictionary\r\n#for main headlines\r\nmain_sentiment_nrc <- liwcalike(as.character(main_corpus), data_dictionary_NRC)\r\nhead(main_sentiment_nrc)[7:12]\r\n\r\n\r\n  anger anticipation disgust  fear   joy negative\r\n1  0.00        10.00       0  0.00 10.00     0.00\r\n2  8.33         8.33       0  0.00  8.33    16.67\r\n3  0.00         0.00       0 16.67  0.00     8.33\r\n4  0.00         7.14       0  0.00  7.14     0.00\r\n5  8.33         0.00       0  8.33  8.33     8.33\r\n6  0.00         0.00       0  0.00  0.00     0.00\r\n\r\nShow code\r\n\r\n#and print headlines\r\nprint_sentiment_nrc <- liwcalike(as.character(print_corpus), data_dictionary_NRC)\r\nhead(print_sentiment_nrc)[11:16]\r\n\r\n\r\n   joy negative positive sadness surprise trust\r\n1 0.00        0     0.00       0     0.00  0.00\r\n2 0.00        0    14.29       0     0.00 14.29\r\n3 0.00        0     0.00       0     0.00  0.00\r\n4 6.25        0    12.50       0     6.25 18.75\r\n5 0.00        0     0.00       0     0.00  0.00\r\n6 0.00        0     9.09       0     9.09  9.09\r\n\r\nNRC as DFM\r\nI can also put the results into a document feature matrix for each\r\ntext group:\r\n\r\n\r\nShow code\r\n\r\n# convert tokens from each headline data set to DFM using the dictionary \"NRC\"\r\nmain_nrc <- dfm(main_tokens) %>%\r\n  dfm_lookup(data_dictionary_NRC)\r\nprint_nrc <- dfm(print_tokens) %>%\r\n  dfm_lookup(data_dictionary_NRC)\r\n\r\ndim(main_nrc)\r\n\r\n\r\n[1] 346  10\r\n\r\nShow code\r\n\r\nmain_nrc\r\n\r\n\r\nDocument-feature matrix of: 346 documents, 10 features (69.36% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            1       0    0   1        0        1       0\r\n   2     1            1       0    0   1        2        1       1\r\n   3     0            0       0    2   0        1        0       0\r\n   4     0            1       0    0   1        0        2       0\r\n   5     1            0       0    1   1        1        1       1\r\n   6     0            0       0    0   0        0        1       0\r\n    features\r\ndocs surprise trust\r\n   1        1     1\r\n   2        1     1\r\n   3        0     0\r\n   4        1     3\r\n   5        0     1\r\n   6        1     1\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\nShow code\r\n\r\ndim(print_nrc)\r\n\r\n\r\n[1] 346  10\r\n\r\nShow code\r\n\r\nprint_nrc\r\n\r\n\r\nDocument-feature matrix of: 346 documents, 10 features (71.47% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            0       0    0   0        0        0       0\r\n   2     0            0       0    0   0        0        1       0\r\n   3     0            0       0    0   0        0        0       0\r\n   4     0            1       0    0   1        0        2       0\r\n   5     0            0       0    0   0        0        0       0\r\n   6     0            0       0    0   0        0        1       0\r\n    features\r\ndocs surprise trust\r\n   1        0     0\r\n   2        0     1\r\n   3        0     0\r\n   4        1     3\r\n   5        0     0\r\n   6        1     1\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\nNRC Polarity Plot\r\nAnd use the information in a data frame to plot the output as\r\nrepresented by a calculation for polarity:\r\n\r\n\r\nShow code\r\n\r\nlibrary(cowplot)\r\n#for the main headlines\r\ndf_main_nrc <- convert(main_nrc, to = \"data.frame\")\r\ndf_main_nrc$polarity <- (df_main_nrc$positive - df_main_nrc$negative)/(df_main_nrc$positive + df_main_nrc$negative)\r\ndf_main_nrc$polarity[which((df_main_nrc$positive + df_main_nrc$negative) == 0)] <- 0\r\n\r\nggplot(df_main_nrc) + \r\n  geom_histogram(aes(x=polarity), bins = 15) + \r\n  theme_minimal_hgrid()\r\n\r\n\r\n\r\nShow code\r\n\r\n#and the print headlines\r\ndf_print_nrc <- convert(print_nrc, to = \"data.frame\")\r\ndf_print_nrc$polarity <- (df_print_nrc$positive - df_print_nrc$negative)/(df_print_nrc$positive + df_print_nrc$negative)\r\ndf_print_nrc$polarity[which((df_print_nrc$positive + df_print_nrc$negative) == 0)] <- 0\r\n\r\nggplot(df_print_nrc) + \r\n  geom_histogram(aes(x=polarity), bins = 15) + \r\n  theme_minimal_hgrid()\r\n\r\n\r\n\r\n\r\nNRC Sample Results\r\nLooking at the headlines that are indicated as “1”, or positive in\r\nsentiment, I can see why these specific headlines are being evaluated as\r\n‘positive’. Some of the aberrations are likely due to the word ‘peace’\r\nin a headline even if it is speaking of it in the past tense.\r\n\r\n\r\nShow code\r\n\r\nhead(main_corpus[which(df_main_nrc$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n1 :\r\n\"4 Takeaways From the U.S. Deal With the Taliban\"\r\n\r\n4 :\r\n\"At Center of Taliban Deal, a U.S. Envoy Who Made It Personal\"\r\n\r\n6 :\r\n\"Trump Speaks With a Taliban Leader\"\r\n\r\n8 :\r\n\"After Tours in Afghanistan, U.S. Veterans Weigh Peace With t...\"\r\n\r\n9 :\r\n\"Javier Pérez de Cuéllar Dies at 100; U.N. Chief Brokered Pea...\"\r\n\r\n10 :\r\n\"From the Afghan Peace Deal, a Weak and Pliable Neighbor for ...\"\r\n\r\nShow code\r\n\r\nhead(print_corpus[which(df_print_nrc$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n2 :\r\n\"U.S. and Taliban Sign Withdrawal Accord\"\r\n\r\n4 :\r\n\"At the Center of the Taliban Deal, a U.S. Envoy Who Made It ...\"\r\n\r\n6 :\r\n\"Pursuing Exit, Trump Talks  To a Leader Of the Taliban\"\r\n\r\n7 :\r\n\"Attacks on Afghans by Taliban Rise After Signing of Peace De...\"\r\n\r\n8 :\r\n\"After Afghanistan Tours,  U.S. Veterans Appraise  Peace Deal...\"\r\n\r\n9 :\r\n\"Javier Pérez de Cuéllar, U.N. Chief  Behind Vital Peace Pact...\"\r\n\r\nLSD 2015\r\nI am going to want to look at multiple dictionaries to see if one can\r\nbest apply to this data. Next, the LSD 2015 dictionary:\r\n\r\n\r\nShow code\r\n\r\n# convert main corpus to DFM using the LSD2015 dictionary\r\nmain_lsd2015 <- dfm(main_tokens) %>%\r\n  dfm_lookup(data_dictionary_LSD2015)\r\n# create main polarity measure for LSD2015\r\nmain_lsd2015 <- convert(main_lsd2015, to = \"data.frame\")\r\nmain_lsd2015$polarity <- (main_lsd2015$positive - main_lsd2015$negative)/(main_lsd2015$positive + main_lsd2015$negative)\r\nmain_lsd2015$polarity[which((main_lsd2015$positive + main_lsd2015$negative) == 0)] <- 0\r\n# convert print corpus to DFM using the LSD2015 dictionary\r\nprint_lsd2015 <- dfm(print_tokens) %>%\r\n  dfm_lookup(data_dictionary_LSD2015)\r\n# create print polarity measure for LSD2015\r\nprint_lsd2015 <- convert(print_lsd2015, to = \"data.frame\")\r\nprint_lsd2015$polarity <- (print_lsd2015$positive - print_lsd2015$negative)/(print_lsd2015$positive + print_lsd2015$negative)\r\nprint_lsd2015$polarity[which((print_lsd2015$positive + print_lsd2015$negative) == 0)] <- 0\r\n\r\n\r\n\r\nLSD Sample Results\r\nLooking at the headlines that are indicated as “1”, or positive in\r\nsentiment, I can again see why these specific headlines are being\r\nevaluated as ‘positive’. At least one of the aberrations is likely due\r\nto a headline referencing someone being set free, though it also\r\nreferences him having shot someone. So it’s a mixed bag, as usually\r\nseems to be the case.\r\n\r\n\r\nShow code\r\n\r\nhead(main_corpus[which(main_lsd2015$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n8 :\r\n\"After Tours in Afghanistan, U.S. Veterans Weigh Peace With t...\"\r\n\r\n11 :\r\n\"A Secret Accord With the Taliban: When and How the U.S. Woul...\"\r\n\r\n18 :\r\n\"To Save Afghan Peace Deal, U.S. May Scale Back C.I.A. Presen...\"\r\n\r\n22 :\r\n\"Afghan Sides Agree to Rare Cease-Fire During Eid al-Fitr\"\r\n\r\n37 :\r\n\"Taliban Announce Brief Cease-Fire, as Afghan Peace Talks Loo...\"\r\n\r\n51 :\r\n\"Afghan Peace Talks Begin This Week. Here’s What to Know.\"\r\n\r\nShow code\r\n\r\nhead(print_corpus[which(print_lsd2015$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n2 :\r\n\"U.S. and Taliban Sign Withdrawal Accord\"\r\n\r\n8 :\r\n\"After Afghanistan Tours,  U.S. Veterans Appraise  Peace Deal...\"\r\n\r\n18 :\r\n\"To Save Peace Deal With Taliban, U.S. May Reduce C.I.A. Pres...\"\r\n\r\n19 :\r\n\"Prominent Retired General Joins Taliban, Stunning the Afghan...\"\r\n\r\n22 :\r\n\"For the First Time Since 2018, the Taliban Agree to a Cease-...\"\r\n\r\n25 :\r\n\"Man Who Shot U.S. Advisers Is Set Free In Afghanistan\"\r\n\r\nLSD Polarity Plot\r\nAnd use the information in a data frame to plot the output as\r\nrepresented by a calculation for polarity:\r\n\r\n\r\nShow code\r\n\r\n#for the main headlines\r\nggplot(main_lsd2015) + \r\n  geom_histogram(aes(x=polarity), bins = 15) + \r\n  theme_minimal_hgrid()\r\n\r\n\r\n\r\nShow code\r\n\r\n#and the print headlines\r\nggplot(print_lsd2015) + \r\n  geom_histogram(aes(x=polarity), bins = 15) + \r\n  theme_minimal_hgrid()\r\n\r\n\r\n\r\n\r\nGeneral Inquirer\r\nand the General Inquirer dictionary:\r\n\r\n\r\nShow code\r\n\r\n# convert main corpus to DFM using the General Inquirer dictionary\r\nmain_geninq <- dfm(main_tokens) %>%\r\n                    dfm_lookup(data_dictionary_geninqposneg)\r\n# create main polarity measure for GenInq\r\nmain_geninq <- convert(main_geninq, to = \"data.frame\")\r\nmain_geninq$polarity <- (main_geninq$positive - main_geninq$negative)/(main_geninq$positive + main_geninq$negative)\r\nmain_geninq$polarity[which((main_geninq$positive + main_geninq$negative) == 0)] <- 0\r\n# convert print corpus to DFM using the General Inquirer dictionary\r\nprint_geninq <- dfm(print_tokens) %>%\r\n                    dfm_lookup(data_dictionary_geninqposneg)\r\n# create print polarity measure for GenInq\r\nprint_geninq <- convert(print_geninq, to = \"data.frame\")\r\nprint_geninq$polarity <- (print_geninq$positive - print_geninq$negative)/(print_geninq$positive + print_geninq $negative)\r\nprint_geninq$polarity[which((print_geninq$positive + print_geninq$negative) == 0)] <- 0\r\n\r\n\r\n\r\nGeneral Inquirer Sample\r\nResults\r\nLooking at the headlines that are indicated as “1”, or positive in\r\nsentiment, I can again see why these specific headlines are being\r\nevaluated as ‘positive’. This one is even more of a mixed bag, with the\r\nsentiment rationale clear. However, it is also clear why the rationale\r\nis being used at the expense of subtle subject matter knowledge.\r\n\r\n\r\nShow code\r\n\r\nhead(main_corpus[which(main_geninq$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n8 :\r\n\"After Tours in Afghanistan, U.S. Veterans Weigh Peace With t...\"\r\n\r\n9 :\r\n\"Javier Pérez de Cuéllar Dies at 100; U.N. Chief Brokered Pea...\"\r\n\r\n14 :\r\n\"As U.S. Troops Leave Afghanistan, Diplomats Are Left to Fill...\"\r\n\r\n22 :\r\n\"Afghan Sides Agree to Rare Cease-Fire During Eid al-Fitr\"\r\n\r\n23 :\r\n\"How the Taliban Outlasted a Superpower: Tenacity and Carnage\"\r\n\r\n24 :\r\n\"Trump Wants Troops in Afghanistan Home by Election Day. The ...\"\r\n\r\nShow code\r\n\r\nhead(print_corpus[which(print_geninq$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n2 :\r\n\"U.S. and Taliban Sign Withdrawal Accord\"\r\n\r\n9 :\r\n\"Javier Pérez de Cuéllar, U.N. Chief  Behind Vital Peace Pact...\"\r\n\r\n19 :\r\n\"Prominent Retired General Joins Taliban, Stunning the Afghan...\"\r\n\r\n20 :\r\n\"Afghans’ Power-Sharing Accord  Honors Official Accused in Ra...\"\r\n\r\n22 :\r\n\"For the First Time Since 2018, the Taliban Agree to a Cease-...\"\r\n\r\n24 :\r\n\"Trump Wants Troops in Afghanistan Home by Election Day\"\r\n\r\nGeneral Inquirer Polarity\r\nPlot\r\nAnd use the information in a data frame to plot the output as\r\nrepresented by a calculation for polarity:\r\n\r\n\r\nShow code\r\n\r\n#for the main headlines\r\nggplot(main_geninq) + \r\n  geom_histogram(aes(x=polarity), bins = 15) + \r\n  theme_minimal_hgrid()\r\n\r\n\r\n\r\nShow code\r\n\r\n#and the print headlines\r\nggplot(print_geninq) + \r\n  geom_histogram(aes(x=polarity), bins = 15) + \r\n  theme_minimal_hgrid()\r\n\r\n\r\n\r\n\r\nComparison Study\r\nCreate Data Frame of All\r\nResults\r\nNow I’m going to be able to compare the different dictionary scores\r\nin one data frame for each type of headline.\r\nMain Headlines\r\n\r\n\r\nShow code\r\n\r\n# create unique names for each main headline dataframe\r\ncolnames(df_main_nrc) <- paste(\"nrc\", colnames(df_main_nrc), sep = \"_\")\r\ncolnames(main_lsd2015) <- paste(\"lsd2015\", colnames(main_lsd2015), sep = \"_\")\r\ncolnames(main_geninq) <- paste(\"geninq\", colnames(main_geninq), sep = \"_\")\r\n# now let's compare our estimates\r\nmain_sent <- merge(df_main_nrc, main_lsd2015, by.x = \"nrc_doc_id\", by.y = \"lsd2015_doc_id\")\r\nmain_sent <- merge(main_sent, main_geninq, by.x = \"nrc_doc_id\", by.y = \"geninq_doc_id\")\r\nhead(main_sent)[1:5]\r\n\r\n\r\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear\r\n1          1         0                1           0        0\r\n2         10         0                3           0        0\r\n3        100         0                0           0        0\r\n4        101         0                2           0        0\r\n5        102         1                0           0        2\r\n6        103         1                1           0        1\r\n\r\nPrint Headlines\r\n\r\n\r\nShow code\r\n\r\n# create unique names for each print headline dataframe\r\ncolnames(df_print_nrc) <- paste(\"nrc\", colnames(df_print_nrc), sep = \"_\")\r\ncolnames(print_lsd2015) <- paste(\"lsd2015\", colnames(print_lsd2015), sep = \"_\")\r\ncolnames(print_geninq) <- paste(\"geninq\", colnames(print_geninq), sep = \"_\")\r\n# now let's compare our estimates\r\nprint_sent <- merge(df_print_nrc, print_lsd2015, by.x = \"nrc_doc_id\", by.y = \"lsd2015_doc_id\")\r\nprint_sent <- merge(print_sent, print_geninq, by.x = \"nrc_doc_id\", by.y = \"geninq_doc_id\")\r\nhead(print_sent)[1:5]\r\n\r\n\r\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear\r\n1          1         0                0           0        0\r\n2         10         0                2           0        0\r\n3        100         0                0           0        0\r\n4        101         0                2           0        0\r\n5        102         1                0           0        4\r\n6        103         1                1           0        1\r\n\r\nCorrelation\r\nNow that I have them all in a single data frame, it’s straightforward\r\nto figure out a bit about how well our different measures of polarity\r\nagree across the different approaches by looking at their correlation\r\nusing the “cor()” function.\r\nIt seems like the polarity of the headlines are more highly\r\ncorrelated between dictionaries slightly for the main headlines than the\r\nprint headlines.\r\nFor Main Headlines\r\n\r\n\r\nShow code\r\n\r\ncor(main_sent$nrc_polarity, main_sent$lsd2015_polarity)\r\n\r\n\r\n[1] 0.512629\r\n\r\nShow code\r\n\r\ncor(main_sent$nrc_polarity, main_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4939498\r\n\r\nShow code\r\n\r\ncor(main_sent$lsd2015_polarity, main_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.5273237\r\n\r\nFor Print Headlines\r\n\r\n\r\nShow code\r\n\r\ncor(print_sent$nrc_polarity, print_sent$lsd2015_polarity)\r\n\r\n\r\n[1] 0.4332694\r\n\r\nShow code\r\n\r\ncor(print_sent$nrc_polarity, print_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4983176\r\n\r\nShow code\r\n\r\ncor(print_sent$lsd2015_polarity, print_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4879879\r\n\r\nCorrelation of NRC\r\nSentiments\r\nI can take a quick visual look at the correlation between sentiments\r\ndetected in both sets of headlines using the “GGally” package. There\r\nseems to be very little difference in that regard.\r\nMain Headlines\r\n\r\n\r\nShow code\r\n\r\nlibrary(GGally)\r\n\r\nmain_nrc_only<- read.csv(\"main_sent_nrc_only.csv\")\r\nggcorr(main_nrc_only, method = c(\"everything\", \"pearson\"))\r\n\r\n\r\n\r\n\r\nPrint Headlines\r\n\r\n\r\nShow code\r\n\r\nprint_nrc_only<- read.csv(\"print_sent_nrc_only.csv\")\r\nggcorr(print_nrc_only, method = c(\"everything\", \"pearson\"))\r\n\r\n\r\n\r\n\r\nLinear Model Testing\r\nFinally, I want to visually look at the correlations or positive and\r\nnegative sentiments as my starting point for understanding relationships\r\nbetween both my sentiment analyses and dictionaries. I’ll start by\r\ndividing the sentiment scores for positive and negative from each text\r\nsource into its own object and change column names to make them unique\r\nexcept for ‘doc_id’ for joining them into one data frame.\r\n\r\n\r\nShow code\r\n\r\ncorr_main <- main_sent %>%\r\n  select(nrc_doc_id, nrc_polarity, lsd2015_polarity, geninq_polarity )\r\ncolnames(corr_main) <- c(\"doc_id\", \"main_nrc\", \"main_lsd\", \"main_geninq\")\r\ncorr_print <- print_sent %>%\r\n  select(nrc_doc_id, nrc_polarity, lsd2015_polarity, geninq_polarity )\r\ncolnames(corr_print) <- c(\"doc_id\", \"print_nrc\", \"print_lsd\", \"print_geninq\")\r\n\r\ncorr_matrix <- join(corr_main, corr_print, by = \"doc_id\")\r\nhead(corr_matrix)\r\n\r\n\r\n  doc_id   main_nrc main_lsd main_geninq  print_nrc print_lsd\r\n1      1  1.0000000        0   0.0000000  0.0000000         0\r\n2     10  1.0000000        0   0.3333333  1.0000000        -1\r\n3    100 -1.0000000       -1   0.0000000 -1.0000000        -1\r\n4    101 -0.3333333       -1  -1.0000000 -0.3333333        -1\r\n5    102 -1.0000000       -1   0.0000000 -1.0000000        -1\r\n6    103  1.0000000        0   0.0000000  1.0000000         0\r\n  print_geninq\r\n1    0.0000000\r\n2    0.3333333\r\n3   -1.0000000\r\n4   -1.0000000\r\n5    1.0000000\r\n6    1.0000000\r\n\r\nThen I can look at the model for each relationship\r\nNRC\r\n\r\n\r\nShow code\r\n\r\n#run the linear model of main vs. print correlation in the NRC dictionary\r\nlm_nrc <- lm(main_nrc~print_nrc, data = corr_matrix)\r\nsummary(lm_nrc)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = main_nrc ~ print_nrc, data = corr_matrix)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1.3637 -0.4190  0.1087  0.5810  1.5810 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -0.10868    0.03398  -3.198  0.00151 ** \r\nprint_nrc    0.47236    0.04668  10.119  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.6304 on 344 degrees of freedom\r\nMultiple R-squared:  0.2294,    Adjusted R-squared:  0.2272 \r\nF-statistic: 102.4 on 1 and 344 DF,  p-value: < 2.2e-16\r\n\r\nLSD\r\n\r\n\r\nShow code\r\n\r\n#run the linear model of main vs. print correlation in the LSD dictionary\r\nlm_lsd <- lm(main_lsd~print_lsd, data = corr_matrix)\r\nsummary(lm_lsd)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = main_lsd ~ print_lsd, data = corr_matrix)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.31105 -0.33699  0.01796  0.50931  1.66301 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -0.17598    0.03452  -5.097  5.7e-07 ***\r\nprint_lsd    0.48703    0.04767  10.218  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.6117 on 344 degrees of freedom\r\nMultiple R-squared:  0.2328,    Adjusted R-squared:  0.2306 \r\nF-statistic: 104.4 on 1 and 344 DF,  p-value: < 2.2e-16\r\n\r\nGeneral Inquiry\r\n\r\n\r\nShow code\r\n\r\n#run the linear model of main vs. print correlation in the General Inquiry dictionary\r\nlm_geninq <- lm(main_geninq~print_geninq, data = corr_matrix)\r\nsummary(lm_geninq)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = main_geninq ~ print_geninq, data = corr_matrix)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1.1599 -0.5267  0.1567  0.4733  1.4733 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -0.15671    0.03540  -4.426 1.29e-05 ***\r\nprint_geninq  0.31660    0.04799   6.597 1.59e-10 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.6479 on 344 degrees of freedom\r\nMultiple R-squared:  0.1123,    Adjusted R-squared:  0.1097 \r\nF-statistic: 43.52 on 1 and 344 DF,  p-value: 1.586e-10\r\n\r\nAnd try to look at if there is any meaningful difference between the\r\nmodels. There does not seem to be!\r\n\r\n\r\nShow code\r\n\r\n#create a data frame from the NRC model results\r\ntidynrc <- tidy(lm_nrc, conf.int = FALSE) \r\n#round the results to 3 decimal points\r\ntidynrc <- tidynrc %>%\r\n  mutate_if(is.numeric, round, 3)\r\ntidynrc$model <- c(\"nrc\")\r\n\r\n#create a data frame from the LSD model results\r\ntidylsd <- tidy(lm_lsd, conf.int = FALSE) \r\n#round the results to 3 decimal points\r\ntidylsd <- tidylsd %>%\r\n  mutate_if(is.numeric, round, 3)\r\ntidylsd$model <- c(\"lsd\")\r\n\r\n#create a data frame from the Gen Inq model results\r\ntidygeninq <- tidy(lm_geninq, conf.int = FALSE) \r\n#round the results to 3 decimal points\r\ntidygeninq <- tidygeninq %>%\r\n  mutate_if(is.numeric, round, 3)\r\ntidygeninq$model <- c(\"geninq\")\r\n\r\ntidy_all <- do.call(\"rbind\", list(tidynrc, tidylsd, tidygeninq))\r\n\r\ntidy_all\r\n\r\n\r\n# A tibble: 6 x 6\r\n  term         estimate std.error statistic p.value model \r\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl> <chr> \r\n1 (Intercept)    -0.109     0.034     -3.20   0.002 nrc   \r\n2 print_nrc       0.472     0.047     10.1    0     nrc   \r\n3 (Intercept)    -0.176     0.035     -5.10   0     lsd   \r\n4 print_lsd       0.487     0.048     10.2    0     lsd   \r\n5 (Intercept)    -0.157     0.035     -4.43   0     geninq\r\n6 print_geninq    0.317     0.048      6.60   0     geninq\r\n\r\nCitations\r\nCitations:\r\nThis research makes use of the NRC\r\nWord-Emotion Association Lexicon, created by Saif Mohammad and Peter\r\nTurney at the National Research Council Canada.\r\nThis research makes use of the LSD Lexicoder Sentiment\r\nDictionary. This dataset was first published in Young, L. &\r\nSoroka, S. (2012). Affective News: The Automated Coding of Sentiment in\r\nPolitical Texts]. doi: 10.1080/10584609.2012.671234 . Political\r\nCommunication, 29(2), 205–231.\r\nThis research makes use of the General\r\nInquirer Lexicon, Stone, P. J. (1962). The general inquirer: A\r\ncomputer system for content analysis and retrieval based on the sentence\r\nas a unit of information. Harvard: Laboratory of Social Relations,\r\nHarvard University.\r\n\r\n\r\n\r\n",
    "preview": "posts/headline-analysis/000012.png",
    "last_modified": "2022-04-29T18:40:56-04:00",
    "input_file": {}
  },
  {
    "path": "posts/pdf-analysis/",
    "title": "Analysis of PDF Articles",
    "description": "Text as Data Project-Article Sentiment Research",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/NYT-Analysis/"
      }
    ],
    "date": "2022-04-17",
    "categories": [
      "text as data",
      "NYT text analysis project"
    ],
    "contents": "\r\n\r\nContents\r\nGetting Started\r\nPulling in the PDF docs\r\nExtracting\r\nPDF Files being examined (random at this time - exploratory)\r\nInspect the first\r\narticle\r\nInspecting Individual\r\nArticles\r\nUnlist\r\n\r\nConclusion\r\n\r\nGetting Started\r\nThe primary goal of this aspect of research is to refine the process\r\nfor examining the content of the full articles for which the main\r\nvs. print headlines are the most different from each other in the\r\nprimary project analysis.\r\nPulling in the PDF docs\r\nI have the PDF files in my working directory. Using the\r\n“list.files()” function from the “pdftools” package, I can create a\r\nvector of PDF file names, specifying only files that end in “.pdf”.\r\n\r\n\r\nShow code\r\n\r\n#load libraries\r\nlibrary(pdftools)\r\nlibrary(readtext)\r\nlibrary(readr)\r\nlibrary(tm)\r\nlibrary(tidytext)\r\nlibrary(stringr)\r\nlibrary(MASS)\r\nlibrary(tidyverse)\r\nlibrary(plyr); library(dplyr)\r\nlibrary(quanteda)\r\nlibrary(purrr)\r\nlibrary(here)\r\n\r\n\r\n\r\nExtracting\r\nPDF Files being examined (random at this time - exploratory)\r\n\r\n\r\nShow code\r\n\r\n#create file names\r\nfiles <- list.files(pattern = \"pdf$\")\r\n\r\n#extract the pdf file data\r\nnyt_articles <- lapply(files, pdf_text)\r\n\r\n#apply length functions\r\nlapply(nyt_articles, length)\r\n\r\n\r\n[[1]]\r\n[1] 4\r\n\r\n[[2]]\r\n[1] 2\r\n\r\n[[3]]\r\n[1] 3\r\n\r\n[[4]]\r\n[1] 4\r\n\r\n[[5]]\r\n[1] 10\r\n\r\n[[6]]\r\n[1] 6\r\n\r\n[[7]]\r\n[1] 2\r\n\r\n[[8]]\r\n[1] 5\r\n\r\n[[9]]\r\n[1] 5\r\n\r\n[[10]]\r\n[1] 2\r\n\r\nShow code\r\n\r\n#view the structure of the list\r\nstr(nyt_articles)\r\n\r\n\r\nList of 10\r\n $ : chr [1:4] \"                                 https://www.nytimes.com/2021/04/13/us/politics/samantha-power-biden.html\\n\\n\\n\"| __truncated__ \"                                Secretary of State Antony J. Blinken at the opening session of talks with China\"| __truncated__ \"The mostly benign prodding by Democrats and Republicans during the hearing signaled how countering China has be\"| __truncated__ \"“There is so much that can be done between bombing and nothing,” Mr. Prendergast said, paraphrasing Luis Moreno\"| __truncated__\r\n $ : chr [1:2] \"                            https://www.nytimes.com/2021/04/29/world/asia/central-asia-border-\\n               \"| __truncated__ \"In announcing the cease-fire, the Kyrgyz Ministry of Interior said that it “does not have\\ndesigns on foreign t\"| __truncated__\r\n $ : chr [1:3] \"                                https://www.nytimes.com/2021/08/05/us/politics/taliban-afghanistan-peace-deal.h\"| __truncated__ \"The statement came as Taliban representatives met with Afghan government officials, including Mr. Abdullah, for\"| __truncated__ \"“The Taliban is not interested in negotiating seriously right now because of what’s happening on the battlefiel\"| __truncated__\r\n $ : chr [1:4] \"                                https://www.nytimes.com/2021/08/08/us/politics/taliban-afghanistan-united-state\"| __truncated__ \"Over the past week, Taliban fighters have moved swiftly to retake cities around Afghanistan, assassinated gover\"| __truncated__ \"                                 Ms. Psaki speaking to reporters at the White House, on Friday. Tom Brenner for\"| __truncated__ \"Mr. Biden, declaring that the United States had long ago accomplished its mission of denying terrorists a haven\"| __truncated__\r\n $ : chr [1:10] \"                                https://www.nytimes.com/2021/08/30/world/asia/us-withdrawal-afghanistan-kabul.h\"| __truncated__ \"Old Soviet tanks litter the grounds of Bala Hissar, outside Kunduz. Jim Huylebroek for The New York Times\\n\" \"  Khalil Haqqani, a Taliban leader, appeared at Friday prayers in Kabul this month with an American-made M-4 ri\"| __truncated__ \"The Taliban’s leverage, earned after years of fighting the world’s most advanced military, multiplied as they c\"| __truncated__ ...\r\n $ : chr [1:6] \"                                 https://www.nytimes.com/2021/09/01/world/asia/afghanistan-taliban-government-l\"| __truncated__ \"  Internally displaced Afghans fleeing the fighting in the north still live at a camp in the Sarawi Shomali par\"| __truncated__ \"  A vendor selling Taliban flags in Kabul on Friday near posters of the senior Taliban officials Amir Khan Mutt\"| __truncated__ \"The Taliban are also fighting stubborn opposition forces led by National Resistance Front leaders in Panjshir P\"| __truncated__ ...\r\n $ : chr [1:2] \"                               https://www.nytimes.com/2021/09/02/us/politics/congress-pentagon-budget-biden.ht\"| __truncated__ \"The lopsided vote underscored another reality: Even as the hard-charging liberal bloc of lawmakers pledging to \"| __truncated__\r\n $ : chr [1:5] \"                                 https://www.nytimes.com/2021/09/07/us/politics/afghan-war-iraq-veterans.html\\n\"| __truncated__ \"                                Jen Burch said the doctors who examined her in 2014 found ground glass nodules \"| __truncated__ \"                                 Melissa Gauntner has dealt with dual traumas and has at times been gripped wit\"| __truncated__ \"In military families, scholars find what they call secondary traumatic distress, symptoms of anxiety stemming f\"| __truncated__ ...\r\n $ : chr [1:5] \"                                 https://www.nytimes.com/2020/10/05/world/asia/afghan-peace-talks-children.html\"| __truncated__ \"                                   Fatima Gailani, whose father was one of the leaders of the mujahedeen resist\"| __truncated__ \"                                Anas Haqqani, the youngest son of the insurgent chief Jalaluddin Haqqani, is pa\"| __truncated__ \"                                 Jalaluddin Haqqani in an undated photo from a video released by the Taliban on\"| __truncated__ ...\r\n $ : chr [1:2] \"                                https://www.nytimes.com/2020/03/04/world/asia/afghanistan-taliban-violence.html\"| __truncated__ \"     Understand the Taliban Takeover in Afghanistan\\n\\n     Who are the Taliban? The Taliban arose in 1994 amid\"| __truncated__\r\n\r\nInspect the first article\r\n\r\n\r\nShow code\r\n\r\nhead(nyt_articles[1])\r\n\r\n\r\n[[1]]\r\n[1] \"                                 https://www.nytimes.com/2021/04/13/us/politics/samantha-power-biden.html\\n\\n\\n\\nAfter Backing Military Force in Past, U.S.A.I.D. Nominee Focuses on Deploying Soft\\nPower\\nIf confirmed to oversee the U.S. Agency for International Development, Samantha Power will confront adversaries by bolstering\\ndemocracy and human rights. China is an early focus.\\n\\n\\n          By Lara Jakes\\n\\nPublished April 13, 2021   Updated April 14, 2021\\n\\n\\nWASHINGTON — Near the end of the 2014 documentary “Watchers of the Sky,” which chronicles the origins of the legal definition\\nof genocide, Samantha Power grows emotional. At the time, Ms. Power was President Barack Obama’s ambassador to the United\\nNations, and, she said, had “great visibility into a lot of the pain” in the world.\\n\\nFrom that perch, preventing mass atrocities abroad required “thinking through what we can do about it, to exhaust the tools at your\\ndisposal,” Ms. Power said in the film. “And I always think about the privilege of, you know, of getting to try — just to try.”\\n\\nFew doubt Ms. Power’s zeal — given her career as a war correspondent, human rights activist, academic expert and foreign policy\\nadviser — even if it has meant advocating military force to stop widespread killings.\\n\\nNow, as President Biden’s nominee to lead the United States Agency for International Development, she is preparing to rejoin the\\ngovernment as an administrator of soft power, and resist using weapons as a means of deterrence and punishment that she has\\npushed for in the past.\\n\\nA Senate committee is expected to vote Thursday on her nomination to lead one of the world’s largest distributors of humanitarian\\naid.\\n\\nIf she is confirmed, Mr. Biden will also seat her on the National Security Council, where during the Obama administration she\\npressed for military intervention to protect civilians from state-sponsored attacks in Libya in 2011 and Syria in 2013. (However, she\\nalso opposed the 2003 invasion of Iraq.)\\n\\nThat she will be back at the table at the council — and again almost certain to be debating whether to entangle American forces in\\nenduring conflicts — has concerned some officials, analysts and think tank experts who demand military restraint from the Biden\\nadministration. Mr. Biden appears to be leaning that way: He has embraced economic sanctions as a tool of hard power and is\\nexpected to announce a full withdrawal of American troops from Afghanistan by Sept. 11, ending the United States’ longest war.\\n\\n“If you’re talking about humanitarianism, famine, the wars — really, other than natural causes, war is the No. 1 cause of famine\\naround the world,” Senator Rand Paul, Republican of Kentucky, told Ms. Power last month during her Senate confirmation hearing.\\n“Are you willing to admit that the Libyan and Syrian interventions that you advocated for were a mistake?”\\n\\nMs. Power did not. “When these situations arise, it’s a question almost of lesser evils — that the choices are very challenging,” she\\nsaid.\\n\\nBy its very nature, the U.S. aid agency takes a long-term view of the world compared with the immediacy of military action. Beyond\\nthe roughly $6 billion in humanitarian aid it is delivering this year to disaster-ridden nations, the agency seeks to prevent conflict at\\nits roots, largely bolstering economies, countering state corruption and fostering democracy and human rights.\\n\\nThat mission is central to Mr. Biden’s foreign policy, and will perhaps prove nowhere more pivotal than in his global competition\\nwith China.\\n\\nLast month, Secretary of State Antony J. Blinken assured allies that they would not be backed into an “‘us-or-them’ choice with\\nChina” as the two superpowers vie for economic, diplomatic and military advantage.\\n\"\r\n[2] \"                                Secretary of State Antony J. Blinken at the opening session of talks with China at the\\n                                Captain Cook hotel in Anchorage. Pool photo by Frederic J. Brown\\n\\n\\n\\nInstead, the United States is highlighting what officials call China’s malign ideology and self-interests as it expands an influence\\ncampaign across Africa, Europe and South America with financial loans, infrastructure funds, coronavirus vaccines and advanced\\ntechnology.\\n\\nThe Trump administration also seized on China’s human rights abuses — particularly against ethnic Uyghurs in the country’s\\nwestern region of Xinjiang — to persuade allies to turn against Beijing. On the Trump administration’s final day in office, Mike\\nPompeo, the secretary of state, declared China’s oppression against Uyghurs as an act of genocide, and he criticized Beijing’s\\nviolent suppression of dissidents in Hong Kong and military harassment of Taiwan.\\n\\n\\n                                Sign Up for On Politics A guide to the political news cycle, cutting\\n                                through the spin and delivering clarity from the chaos. Get it sent to your\\n                                inbox.\\n\\n\\nOfficials said China’s much-debated Belt and Road Initiative was a prime battleground for U.S.A.I.D. to challenge Beijing.\\n\\nRepresentative Tom Malinowski, Democrat of New Jersey and a former assistant secretary of state for democracy and human\\nrights for Mr. Obama, described a “perception that China is exporting corruption” with its loans and development projects.\\n\\nFor example, a study in February by the International Republican Institute, a private nonprofit group that receives government\\nfunding and promotes democracy, concluded that Panama’s decision in 2017 to sever diplomatic ties with Taiwan “appears to have\\nbeen driven by payoffs” from China. It also noted that Nepal regularly revoked the legal status of Tibetan refugees after becoming\\neconomically reliant on Beijing.\\n\\nThe American aid agency alone cannot match the funds that China has seeded in developing countries. But Mr. Malinowski said its\\nsupport to journalists, legal advisers and legitimate opposition groups could “expose and combat” corrosive foreign leaders who\\nhad benefited from Beijing’s financial backing and playbook for how to remain in power.\\n\\n“There is one issue that has risen to the top in this administration that I know she is very focused on, and that’s fighting corruption,”\\nMr. Malinowski said of Ms. Power. “And U.S.A.I.D. has a very important role to play there, potentially.”\\n\\nAt her confirmation hearing in March, Ms. Power told senators she was moved to pursue a career in foreign policy after the 1989\\nmassacre of protesters in Tiananmen Square in Beijing. She described China’s “coercive and predatory approach, which is so\\ntransactional” in its dealings with developing countries that ultimately become dependent on Beijing through what she called “debt-\\ntrap diplomacy.”\\n\\n“I think it’s not going over that well, and that creates an opening for the United States,” Ms. Power told Senator Todd Young,\\nRepublican of Indiana.\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \r\n[3] \"The mostly benign prodding by Democrats and Republicans during the hearing signaled how countering China has become a rare,\\nif reliable, issue of bipartisanship in Congress. “It’s absolutely essential that our development dollars, I think, be used to advance\\nour geostrategic priorities,” Mr. Young said.\\n\\nThe aid agency and the State Department have budgeted about $2 billion on programs to foster democracy, human rights and open\\ngovernance abroad in the 2021 fiscal year — one-third as much as funding for humanitarian assistance.\\n\\nIt is an area that Ms. Power is expected to expand. The Biden administration’s first budget blueprint, released on Friday, asserted it\\nwould commit an unspecified but “significant increase in resources” to advance human rights and democracy while thwarting\\ncorruption and authoritarianism.\\n\\n\\n\\n\\n                               Asylum seekers from Central America crossing the Paso del Norte International Bridge,\\n                               in Ciudad Juarez, Mexico. One of Ms. Power’s priorities will be to target corruption,\\n                               violence and poverty in the region. Jose Luis Gonzalez/Reuters\\n\\n\\n\\nThe spending plan also will support another of Ms. Power’s priorities: targeting corruption, violence and poverty in Central\\nAmerica as a means to curb the flow of thousands of migrants who head to the southwestern border each year. The Biden\\nadministration is banking on a $4 billion strategy through 2025 — including an initial tranche of $861 million proposed this year — to\\nhelp stabilize the region.\\n\\nIn El Salvador, for example, homicides dropped 61 percent after a U.S.A.I.D. effort to reduce violence from 2015 to 2017, Ms. Power\\ntold the senators, and the agency’s programs in Honduras have yielded similar results. The programs not only supported local\\nprosecutors but also brought together government officials, businesses and church and community leaders to divert young people\\nfrom gangs through job training, tutoring and artistic activities.\\n\\nShe was met with some skepticism.\\n\\nSenator Rob Portman, Republican of Ohio, noted that the number of children from Central America at the border had steadily\\nincreased since January, even though the United States spent $3.6 billion over the past five years on similar efforts.\\n\\n“The results are not impressive,” Mr. Portman said. “It’s an economic issue, primarily,” and “people will still be looking to come to\\nthe United States.”\\n\\nExplaining foreign policy decisions to the American people, and making it relevant to their lives, is a driving theme of the State\\nDepartment under Mr. Biden. Ms. Power can reach back to her own experiences as both an immigrant from Ireland and a\\nstoryteller to make the case for easing the border crisis by attacking its root causes.\\n\\n“That’s part of the job, too — you’ve got to be a salesperson, you’ve got to go out there and explain to people, ‘Here’s why we need\\nmore resources to do this work, and here’s where U.S.A.I.D. can be an incredibly important partner,’” said John Prendergast, a\\nlongtime human rights and anticorruption activist and close friend to Ms. Power.\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \r\n[4] \"“There is so much that can be done between bombing and nothing,” Mr. Prendergast said, paraphrasing Luis Moreno Ocampo, the\\nformer prosector of the International Criminal Court who was featured in the same documentary about genocide as Ms. Power.\\n“And Samantha’s whole work and life has been between those two extremes.”\\n\\nGayle Smith, who ran the aid agency for Mr. Obama and is now the State Department’s coronavirus vaccine envoy, put it more\\nbluntly.\\n\\n“It’s not like U.S.A.I.D. is going to invade somebody,” she said.\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \r\n\r\nInspecting Individual\r\nArticles\r\nNow I’m going to use “purrr” to “pluck()” each of the articles as\r\nits’ own vector and create a corpus of each article to examine.\r\n\r\n\r\nShow code\r\n\r\narticle_111 <- nyt_articles %>% \r\n  pluck(1)\r\narticle_111 <- as_vector(article_111)\r\n\r\narticle_111_corpus <- corpus(article_111)\r\narticle_111_summary <- summary(article_111_corpus)\r\narticle_111_summary\r\n\r\n\r\nCorpus consisting of 4 documents, showing 4 documents:\r\n\r\n  Text Types Tokens Sentences\r\n text1   357    688        24\r\n text2   289    523        19\r\n text3   304    562        18\r\n text4    76    105         4\r\n\r\nI also found a very interesting way to pull the text and save them as\r\nindividual .txt files, but for now I’m just going to note that as an\r\nalternative process. I’ve struggled quite a bit to get the PDF text read\r\ncompared to the headlines.\r\n\r\n\r\nShow code\r\n\r\nconvertpdf2txt <- function(dirpath){\r\n  files <- list.files(dirpath, full.names = T)\r\n  x <- sapply(files, function(x){\r\n  x <- pdftools::pdf_text(x) %>%\r\n  paste(sep = \" \") %>%\r\n  stringr::str_replace_all(fixed(\"\\n\"), \" \") %>%\r\n  stringr::str_replace_all(fixed(\"\\r\"), \" \") %>%\r\n  stringr::str_replace_all(fixed(\"\\t\"), \" \") %>%\r\n  stringr::str_replace_all(fixed(\"\\\"\"), \" \") %>%\r\n  paste(sep = \" \", collapse = \" \") %>%\r\n  stringr::str_squish() %>%\r\n  stringr::str_replace_all(\"- \", \"\") \r\n  return(x)\r\n    })\r\n}\r\n# apply function\r\ntxts <- convertpdf2txt(\"./files\")\r\n# inspect the structure of the txts element\r\nstr(txts)\r\n\r\n\r\n Named chr [1:10] \"https://www.nytimes.com/2021/04/13/us/politics/samantha-power-biden.html After Backing Military Force in Past, \"| __truncated__ ...\r\n - attr(*, \"names\")= chr [1:10] \"./files/article_111.pdf\" \"./files/article_132.pdf\" \"./files/article_193.pdf\" \"./files/article_196.pdf\" ...\r\n\r\n\r\n\r\nShow code\r\n\r\n#apply length functions\r\nlapply(txts, length)\r\n\r\n\r\n$`./files/article_111.pdf`\r\n[1] 1\r\n\r\n$`./files/article_132.pdf`\r\n[1] 1\r\n\r\n$`./files/article_193.pdf`\r\n[1] 1\r\n\r\n$`./files/article_196.pdf`\r\n[1] 1\r\n\r\n$`./files/article_278.pdf`\r\n[1] 1\r\n\r\n$`./files/article_288.pdf`\r\n[1] 1\r\n\r\n$`./files/article_293.pdf`\r\n[1] 1\r\n\r\n$`./files/article_300.pdf`\r\n[1] 1\r\n\r\n$`./files/article_56.pdf`\r\n[1] 1\r\n\r\n$`./files/article_7.pdf`\r\n[1] 1\r\n\r\nShow code\r\n\r\n#view the structure of the list\r\nstr(txts)\r\n\r\n\r\n Named chr [1:10] \"https://www.nytimes.com/2021/04/13/us/politics/samantha-power-biden.html After Backing Military Force in Past, \"| __truncated__ ...\r\n - attr(*, \"names\")= chr [1:10] \"./files/article_111.pdf\" \"./files/article_132.pdf\" \"./files/article_193.pdf\" \"./files/article_196.pdf\" ...\r\n\r\nShow code\r\n\r\n# add names to txt files\r\nnames(txts) <- paste(\"nyt\", 1:length(txts), sep = \"\")\r\n# save result to disc\r\nlapply(seq_along(txts), function(i)writeLines(text = unlist(txts[i]),\r\n    con = paste(\"./txts\", names(txts)[i],\".txt\", sep = \"\")))\r\n\r\n\r\n[[1]]\r\nNULL\r\n\r\n[[2]]\r\nNULL\r\n\r\n[[3]]\r\nNULL\r\n\r\n[[4]]\r\nNULL\r\n\r\n[[5]]\r\nNULL\r\n\r\n[[6]]\r\nNULL\r\n\r\n[[7]]\r\nNULL\r\n\r\n[[8]]\r\nNULL\r\n\r\n[[9]]\r\nNULL\r\n\r\n[[10]]\r\nNULL\r\n\r\nUnlist\r\nDocumenting, for now, the ways I’m struggling with so I can find out\r\nwhy. Primarily, I’m struggling with the ‘unlist’ command as it applies\r\nto documents originating as PDF files, though this is not an issue when\r\nI use it in other types of situations.\r\n\r\n\r\nShow code\r\n\r\n#convert list to vector\r\n#nyt_vector <- unlist(nyt_articles, recursive = TRUE)\r\n#put articles into data frame\r\n#nyt_df <- as.data.frame(nyt_vector, row.names = NULL, stringsAsFactors = FALSE)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n#create corpus\r\n#nyt_corpus <- corpus(txts)\r\n#confirming class of corpus\r\n#class(nyt_corpus)\r\n#confirm length of corpus\r\n#length(nyt_corpus)\r\n\r\n\r\n\r\nConclusion\r\nI will not be able to fit this type of analysis into the scope of my\r\ncurrent project. I will use this in further studies.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-29T14:43:59-04:00",
    "input_file": {}
  },
  {
    "path": "posts/lit-review/",
    "title": "Literature Review",
    "description": "Text as Data Project-Literature Review",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/NYT-Analysis/"
      }
    ],
    "date": "2022-04-15",
    "categories": [
      "text as data",
      "NYT text analysis project",
      "literature review"
    ],
    "contents": "\r\n\r\nContents\r\nAnnotated\r\nBibliography (Starting with Abstracts Only - In Development)\r\nLiu & Huang (2022)\r\nChan, et al. (2021)\r\nVan Atteveldt, et\r\nal. (2021)\r\nBurggraff & Trilling\r\n(2020)\r\nBoukes, et al. (2020)\r\nSong, et al. (2020)\r\nRudkowsky, et al. (2018)\r\nSilva (2017)\r\nGottlieb (2015)\r\nDiakopoulos (2015)\r\nGrimmer & Stewart\r\n(2013)\r\nKothari (2010)\r\nKiousis (2004)\r\nPeng (2004)\r\nAlthaus & Tewksbury\r\n(2002)\r\nPan & Kosicki (1993)\r\n\r\n\r\nAnnotated\r\nBibliography (Starting with Abstracts Only - In Development)\r\nLiu & Huang (2022)\r\nLiu, M., & Huang, J. (2022). “Climate change” versus “global\r\nwarming”: A corpus-assisted discourse analysis of two popular terms in\r\nthe New York Times. Journal of World Languages. https://doi.org/10.1515/jwl-2022-0004\r\nAbstract\r\n“Climate change” and “global warming” are two popular terms that may\r\nbe often used interchangeably in news media. This study proposes to give\r\na corpusassisted discourse study of the representations of climate\r\nchange and global warming in the New York Times (2000–2019) in order to\r\nexamine how they are actually used in the newspaper. The findings show\r\nboth similarities and differences in their representations in terms of\r\nthe associated topics/themes, the particular ways of framing, and the\r\nperspectivization strategy employed. It is argued that a corpus-assisted\r\ndiscourse study of a large sample of news articles presents a more\r\naccurate picture of the actual use of the two terms in news media.\r\nChan, et al. (2021)\r\nChan, C., Bajjalieh, J., Auvil, L., Wessler, H., Althaus, S.,\r\nWelbers, K., Atteveldt, W. van, & Jungblut, M. (2021). Four best\r\npractices for measuring news sentiment using ‘off-the-shelf’\r\ndictionaries: A large-scale p-hacking experiment. Computational\r\nCommunication Research, 3(1), 1–27.\r\nAbstract\r\nWe examined the validity of 37 sentiment scores based on\r\ndictionary-based methods using a large news corpus and demonstrated the\r\nrisk of generating a spectrum of results with different levels of\r\nstatistical significance by presenting an analysis of relationships\r\nbetween news sentiment and U.S. presidential approval. We summarize our\r\nfindings into four best practices: 1) use a suitable sentiment\r\ndictionary; 2) do not assume that the validity and reliability of the\r\ndictionary is ‘built-in’; 3) check for the influence of content length\r\nand 4) do not use multiple dictionaries to test the same statistical\r\nhypothesis.\r\nVan Atteveldt, et al. (2021)\r\nvan Atteveldt, W., van der Velden, M. A. C. G., & Boukes, M.\r\n(2021). The Validity of Sentiment Analysis: Comparing Manual Annotation,\r\nCrowd-Coding, Dictionary Approaches, and Machine Learning Algorithms.\r\nCommunication Methods and Measures, 15(2), 121–140. https://doi.org/10.1080/19312458.2020.1869198\r\nAbstract\r\nSentiment is central to many studies of communication science, from\r\nnegativity and polarization in political communication to analyzing\r\nproduct reviews and social media comments in other sub-fields. This\r\nstudy provides an exhaustive comparison of sentiment analysis methods,\r\nusing a validation set of Dutch economic headlines to compare the\r\nperformance of manual annotation, crowd coding, numerous dictionaries\r\nand machine learning using both traditional and deep learning\r\nalgorithms. The three main conclusions of this article are that: (1) The\r\nbest performance is still attained with trained human or crowd coding;\r\n(2) None of the used dictionaries come close to acceptable levels of\r\nvalidity; and (3) machine learning, especially deep learning,\r\nsubstantially outperforms dictionary-based methods but falls short of\r\nhuman performance. From these findings, we stress the importance of\r\nalways validating automatic text analysis methods before usage.\r\nMoreover, we provide a recommended step-bystep approach for (automated)\r\ntext analysis projects to ensure both efficiency and validity.\r\nBurggraff & Trilling (2020)\r\nBurggraaff, C., & Trilling, D. (2020). Through a different\r\ngate: An automated content analysis of how online news and print news\r\ndiffer. Journalism, 21(1), 112–129. https://doi.org/10.1177/1464884917716699\r\nAbstract\r\nWe investigate how news values differ between online and print news\r\narticles. We hypothesize that print and online articles differ in terms\r\nof news values because of differences in the routines used to produce\r\nthem. Based on a quantitative automated content analysis of N = 762,095\r\nDutch news items, we show that online news items are more likely to be\r\nfollow-up items than print items, and that there are further differences\r\nregarding news values like references to persons, the power elite,\r\nnegativity, and positivity. In order to conduct this large-scale\r\nanalysis, we developed innovative methods to automatically code a wide\r\nrange of news values. In particular, this article demonstrates how\r\ntechniques such as sentiment analysis, named entity recognition,\r\nsupervised machine learning, and automated queries of external databases\r\ncan be combined and used to study journalistic content. Possible\r\nexplanations for the difference found between online and offline news\r\nare discussed.\r\nBoukes, et al. (2020)\r\nBoukes, M., van de Velde, B., Araujo, T., & Vliegenthart, R.\r\n(2020). What’s the Tone? Easy Doesn’t Do It: Analyzing Performance and\r\nAgreement Between Off-the-Shelf Sentiment Analysis Tools. Communication\r\nMethods and Measures, 14(2), 83–104. https://doi.org/10.1080/19312458.2019.1671966\r\nAbstract\r\nThis article scrutinizes the method of automated content analysis to\r\nmeasure the tone of news coverage. We compare a range of off-the-shelf\r\nsentiment analysis tools to manually coded economic news as well as\r\nexamine the agreement between these dictionary approaches themselves. We\r\nassess the performance of five off-the-shelf sentiment analysis tools\r\nand two tailor-made dictionary-based approaches. The analyses result in\r\nfive conclusions. First, there is little overlap between the\r\noff-the-shelf tools; causing wide divergence in terms of tone\r\nmeasurement. Second, there is no stronger overlap with manual coding for\r\nshort texts (i.e., headlines) than for long texts (i.e., full articles).\r\nThird, an approach that combines individual dictionaries achieves a\r\ncomparably good performance. Fourth, precision may increase to\r\nacceptable levels at higher levels of granularity. Fifth, performance of\r\ndictionary approaches depends more on the number of relevant keywords in\r\nthe dictionary than on the number of valenced words as such; a small\r\ntailor-made lexicon was not inferior to large established dictionaries.\r\nAltogether, we conclude that off-the-shelf sentiment analysis tools are\r\nmostly unreliable and unsuitable for research purposes – at least in the\r\ncontext of Dutch economic news – and manual validation for the specific\r\nlanguage, domain, and genre of the research project at hand is always\r\nwarranted.\r\nSong, et al. (2020)\r\nSong, H., Tolochko, P., Eberl, J.-M., Eisele, O., Greussing, E.,\r\nHeidenreich, T., Lind, F., Galyga, S., & Boomgaarden, H. G. (2020).\r\nIn Validations We Trust? The Impact of Imperfect Human Annotations as a\r\nGold Standard on the Quality of Validation of Automated Content\r\nAnalysis. Political Communication, 37(4), 550–572. https://doi.org/10.1080/10584609.2020.1723752\r\nAbstract\r\nPolitical communication has become one of the central arenas of\r\ninnovation in the application of automated analysis approaches to\r\never-growing quantities of digitized texts. However, although\r\nresearchers routinely and conveniently resort to certain forms of human\r\ncoding to validate the results derived from automated procedures, in\r\npractice the actual “quality assurance” of such a “gold standard” often\r\ngoes unchecked. Contemporary practices of validation via manual\r\nannotations are far from being acknowledged as best practices in the\r\nliterature, and the reporting and interpretation of validation\r\nprocedures differ greatly. We systematically assess the connection\r\nbetween the quality of human judgment in manual annotations and the\r\nrelative performance evaluations of automated procedures against true\r\nstandards by relying on large-scale Monte Carlo simulations. The results\r\nfrom the simulations confirm that there is a substantially greater risk\r\nof a researcher reaching an incorrect conclusion regarding the\r\nperformance of automated procedures when the quality of manual\r\nannotations used for validation is not properly ensured. Our\r\ncontribution should therefore be regarded as a call for the systematic\r\napplication of high-quality manual validation materials in any political\r\ncommunication study, drawing on automated text analysis procedures.\r\nRudkowsky, et al. (2018)\r\nRudkowsky, E., Haselmayer, M., Wastian, M., Jenny, M., Emrich,\r\nŠ., & Sedlmair, M. (2018). More than Bags of Words: Sentiment\r\nAnalysis with Word Embeddings. Communication Methods and Measures,\r\n12(2–3), 140–157. https://doi.org/10.1080/19312458.2018.1455817\r\nAbstract\r\nMoving beyond the dominant bag-of-words approach to sentiment\r\nanalysis we introduce an alternative procedure based on distributed word\r\nembeddings. The strength of word embeddings is the ability to capture\r\nsimilarities in word meaning. We use word embeddings as part of a\r\nsupervised machine learning procedure which estimates levels of\r\nnegativity in parliamentary speeches. The procedure’s accuracy is\r\nevaluated with crowdcoded training sentences; its external validity\r\nthrough a study of patterns of negativity in Austrian parliamentary\r\nspeeches. The results show the potential of the word embeddings approach\r\nfor sentiment analysis in the social sciences.\r\nSilva (2017)\r\nSilva, D. M. D. (2017). The Othering of Muslims: Discourses of\r\nRadicalization in the New York Times, 1969–2014. Sociological Forum,\r\n32(1), 138–161. https://doi.org/10.1111/socf.12321\r\nAbstract\r\nIn this article, I engage with Edward Said’s Orientalism and various\r\nperspectives within the othering paradigm to analyze the emergence and\r\ntransformation of radicalization discourses in the news media. Employing\r\ndiscourse analysis of 607 New York Times articles from 1969 to 2014,\r\nthis article demonstrates that radicalization discourses are not new but\r\nare the result of complex sociolinguistic and historical developments\r\nthat cannot be reduced to dominant contemporary understandings of the\r\nconcept or to singular events or crises. The news articles were then\r\ncompared to 850 government documents, speeches, and other official\r\ncommunications. The analysis of the data indicates that media\r\nconceptualizations of radicalization, which once denoted political and\r\neconomic differences, have now shifted to overwhelmingly focus on Islam.\r\nAs such, radicalization discourse now evokes the construct\r\nradicalization as symbolic marker of conflict between the West and the\r\nEast. I also advanced the established notion that the news media employ\r\nstrategic discursive strategies that contribute to conceptual\r\ndistinctions that are used to construct Muslims as an “alien other” to\r\nthe West.\r\nGottlieb (2015)\r\nGottlieb, J. (2015). Protest News Framing Cycle: How The New York\r\nTimes Covered Occupy Wall Street. International Journal of\r\nCommunication, 9(0), 23.\r\nAbstract\r\nThis article introduces a protest news framing cycle and presents the\r\nresults of a longitudinal analysis of news attention and framing of\r\nprotest movements. To identify the frame-changing dynamic occurring over\r\ntime, a content analysis of the news coverage of Occupy Wall Street was\r\nconducted on 228 articles and 37 editorials in The New York Times from\r\nthe start of the protest in September 2011 until long after the protest\r\nhad subsided in July 2014. The article identifies longitudinal changes\r\nin news frames about the economic substance of the protest and the\r\nensuing conflict between protesters and city officials during the\r\noccupation. Findings suggest that conflict had a significant impact on\r\nthe number of news stories about the protest. Further, the results\r\ndemonstrate how news framing opportunities changed as the movement\r\nreached different stages of the news attention cycle. As the movement\r\ngrew, journalists focused on the movement’s economic grievances,\r\nincluding economic inequality, bank bailouts, and foreclosures. As the\r\nmovement peaked, news attention shifted to the intensifying conflict\r\nbetween city officials and protesters.\r\nDiakopoulos (2015)\r\nDiakopoulos, N. A. (2015). The Editor’s Eye: Curation and Comment\r\nRelevance on the New York Times. Proceedings of the 18th ACM Conference\r\non Computer Supported Cooperative Work & Social Computing,\r\n1153–1157. https://doi.org/10.1145/2675133.2675160\r\nAbstract\r\nThe journalistic curation of social media content from platforms like\r\nFacebook and YouTube or from commenting systems is underscored by an\r\nimperative for publishing accurate and quality content. This work\r\nexplores the manifestation of editorial quality criteria in comments\r\nthat have been curated and selected on the New York Times website as\r\n“NYT Picks.” The relationship between comment selection and comment\r\nrelevance is examined through the analysis of 331,785 comments,\r\nincluding 12,542 editor’s selections. A robust association between\r\neditorial selection and article relevance or conversational relevance\r\nwas found. The results are discussed in terms of their implications for\r\nreducing journalistic curatorial work load, or scaling the ability to\r\nexamine more comments for editorial selection , as well as how end-user\r\ncommenting experiences might be improved.\r\nGrimmer & Stewart (2013)\r\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The\r\nPromise and Pitfalls of Automatic Content Analysis Methods for Political\r\nTexts. Political Analysis, 21(3), 267–297.\r\nAbstract\r\nPolitics and political conflict often occur in the written and spoken\r\nword. Scholars have long recognized this, but the massive costs of\r\nanalyzing even moderately sized collections of texts have hindered their\r\nuse in political science research. Here lies the promise of automated\r\ntext analysis: it substantially reduces the costs of analyzing large\r\ncollections of text. We provide a guide to this exciting new area of\r\nresearch and show how, in many instances, the methods have already\r\nobtained part of their promise. But there are pitfalls to using\r\nautomated methods–they are no substitute for careful thought and close\r\nreading and require extensive and problem-specific validation. We survey\r\na wide range of new methods, provide guidance on how to validate the\r\noutput of the models, and clarify misconceptions and errors in the\r\nliterature. To conclude, we argue that for automated text methods to\r\nbecome a standard tool for political scientists, methodologists must\r\ncontribute new methods and new methods of validation.\r\nKothari (2010)\r\nKothari, A. (2010). The Framing of the Darfur Conflict in the New\r\nYork Times: 2003–2006. Journalism Studies, 11(2), 209–224. https://doi.org/10.1080/14616700903481978\r\nAbstract\r\nThis multi-method study examines how the New York Times reported the\r\nDarfur conflict in the Sudan, which has led to an estimated 300,000\r\ndeaths and over 2.3 million people displaced by the fighting. Drawing on\r\nnormative media theories and prior studies of Africa’s representation,\r\nthe role of sources in the frame-building process was analyzed, together\r\nwith the impact of news-making processes on journalists’ reporting about\r\nDarfur. The textual analysis largely supports results of prior studies\r\non news framing of Africa. However, interviews with four New York Times\r\njournalists reveal that the individual biases and motives of the\r\njournalists and their sources significantly influenced the coverage.\r\nWhile the journalists participated in news-making processes\r\ndistinguishable by journalist goal, source availability, and source\r\ncredibility, their sources also provided information that reinforced\r\ncertain media frames.\r\nKiousis (2004)\r\nKiousis, S. (2004). Explicating Media Salience: A Factor Analysis\r\nof New York Times Issue Coverage During the 2000 U.S. Presidential\r\nElection. Journal of Communication, 54(1), 71–87. https://doi.org/10.1111/j.1460-2466.2004.tb02614.x\r\nAbstract\r\nMedia salience—the key independent variable in agenda-setting\r\nresearch—has traditionally been explicated as a singular construct.\r\nNevertheless, scholars have defined and measured it using a number of\r\ndifferent conceptualizations and empirical indicators. To address this\r\nlimitation in research, this study introduced a conceptual model of\r\nmedia salience, suggesting it is a multidimensional construct consisting\r\nof 3 core elements: attention, prominence, and valence. Furthermore, the\r\nmodel was tested through an exploratory factor analysis of The New York\r\nTimes news coverage of 8 major political issues during the 2000\r\npresidential election as a case study. The data revealed that 2\r\ndimensions of media salience emerge: visibility and valence. Based on\r\nthe factor analysis, 2 indices are created to measure the construct,\r\nwhich are intended for use in future investigations.\r\nPeng (2004)\r\nPeng, Z. (2004). Representation of China: An across time analysis\r\nof coverage in the New York Times and Los Angeles Times. Asian Journal\r\nof Communication, 14(1), 53–67. https://doi.org/10.1080/0129298042000195170\r\nAbstract\r\nThis study examined the coverage of China in the New York Times and\r\nLos Angeles Times between 1992 and 2001. Across time comparison were\r\nmade both within and between the two newspapers in terms of total number\r\nof stories, media frames used and favourability differences. Findings\r\nshow that coverage of China has increased significantly over time, but\r\nthe overall tone remained negative. Stories presented in political\r\nframes and ideological frames were more likely to be unfavourable. No\r\nsignificant differences were found between the two newspapers.\r\nAlthaus & Tewksbury (2002)\r\nAlthaus, S. L., & Tewksbury, D. (2002). Agenda Setting and\r\nthe “New” News: Patterns of Issue Importance Among Readers of the Paper\r\nand Online Versions of the New York Times. Communication Research,\r\n29(2), 180–207. https://doi.org/10.1177/0093650202029002004\r\nAbstract\r\nThis study examines whether readers of the paper and online versions\r\nof a national newspaper acquire different perceptions of the importance\r\nof political issues. Using data from a weeklong experiment in which\r\nsubjects either readthe print version of the New York Times, the online\r\nversion of that paper, or received no special exposure, this study finds\r\nevidence that people exposed to the Times for 5 days adjusted their\r\nagendas in response to that exposure and that print readers modified\r\ntheir agendas differently than did online readers.\r\nPan & Kosicki (1993)\r\nPan, Z., & Kosicki, G. (1993). Framing analysis: An approach\r\nto news discourse. Political Communication, 10(1), 55–75. https://doi.org/10.1080/10584609.1993.9962963\r\nAbstract\r\nIn the American political process, news discourse concerning public\r\npolicy issues is carefully constructed. This occurs in part because both\r\npoliticians and interest groups take an increasingly proactive approach\r\nto amplify their views of what an issue is about. However, news media\r\nalso play an active role in framing public policy issues. Thus, in this\r\narticle, news discourse is conceived as a sociocognitive process\r\ninvolving all three players: sources, journalists, and audience members\r\noperating in the universe of shared culture and on the basis of socially\r\ndefined roles. Framing analysis is presented as a constructivist\r\napproach to examine news discourse with the primary focus on\r\nconceptualizing news texts into empirically operationalizable\r\ndimensions—syntactical, script, thematic, and rhetorical structures—so\r\nthat evidence of the news media’s framing of issues in news texts may be\r\ngathered. This is considered an initial step toward analyzing the news\r\ndiscourse process as a whole. Finally, an extended empirical example is\r\nprovided to illustrate the applications of this conceptual framework of\r\nnews texts.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-29T14:41:55-04:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Analysis of New York Times Headlines",
    "description": "This is the project page for the analysis of differences between main and print headlines for New York Times articles published surrounding the U.S. withdrawal of the military in Afghanistan.",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/Grateful-Network/"
      }
    ],
    "date": "2022-04-13",
    "categories": [],
    "contents": "\r\n\r\nFor this project, I am using some data gathered in the DACSS 602\r\ncourse “Research Design”.\r\nI continued down the same path but with new data and a new direction\r\nthrough the DACSS 697D course “Text as Data”.\r\nMore background data can be found in this series of posts from my academic blog.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-29T14:35:54-04:00",
    "input_file": {}
  }
]
