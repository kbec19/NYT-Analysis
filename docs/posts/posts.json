[
  {
    "path": "posts/headline-analysis/",
    "title": "Analysis of Main vs. Print Headlines",
    "description": "Text as Data Project Headline Comparison Research",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/NYT-Analysis/"
      }
    ],
    "date": "2022-04-17",
    "categories": [
      "text as data",
      "NYT text analysis project"
    ],
    "contents": "\r\n\r\nContents\r\nComparing\r\nMain vs. Print Headlines in the New York Times\r\nResearch Background\r\nMaking\r\nChoices on Inclusion of Observations\r\nGathering Data\r\nPrevious Process\r\nLoad Data\r\nLoad Data\r\nCreate Corpus\r\nAssign Type to Docvars\r\nTokenization\r\nDocument Feature Matrix\r\n\r\nDictionary Analysis\r\nliwcalike()\r\nNRC\r\nLSD 2015\r\nGeneral Inquirer\r\nAnnotation\r\n\r\n\r\n\r\nComparing\r\nMain vs. Print Headlines in the New York Times\r\nResearch Background\r\nDuring the Fall 2021 semester, my research group hand coded PDF\r\ncopies of articles resulting from a simple search on the websites of the\r\nNew York Times and Wall Street Journal from Feburary 29, 2020 through\r\nSeptember 30, 2021 using the term “Afghanistan withdrawal”. One thing I\r\nnoticed was that when loading the PDF articles into NVivo for coding was\r\nthat it was difficult to match the New York Times articles to the\r\ncitation information in Zotero for many of the articles because the\r\narticle titles did not match. I realized that in the process of saving\r\nthe articles in Zotero, they were saved with the title viewable on the\r\nweb version of the article; however, once the article had been preserved\r\nby using the site’s “Print to PDF” function, the article title that it\r\nused as a default file name was different than the web version.\r\nThis semester, I began this project to be one expanding on last\r\nsemester’s research and looking to expand a machine analysis of articles\r\npulling articles beginning January 2020 through December 2021. For my\r\ninitial text collection, I collected articles using the New York Times\r\nAPI for the search query “Afghanistan”, and hoped to be able to analyze\r\nthe full text of a larger range of articles.\r\nHowever, I found that I am limited in that the article search API for\r\nthe New York Times does not pull the entire article; rather, I was able\r\nto pull the abstract/summary, lead paragraph, and snippet for each\r\narticle as well as the keywords, authors, sections, and url. In\r\naddition, I was able to get the article titles for both the print and\r\nonline versions of the article.\r\nThe API’s lack of full article text was not optimal for my purposes;\r\nto examine sentiment and co-occurence of various sources. Sources are\r\nnot necessarily detailed in the lead paragraph or abstract of an\r\narticle, so I moved to a different research path.\r\nRemembering the differences in headlines from our manual coding\r\nresearch and noting that the API provides both headlines in the article\r\nsearch API, I turned to analyzing the differences in the main vs. print\r\nheadlines for articles from the same research period as our first\r\nexamination. This way, I can potentially use a sample of the full\r\narticles collected in our previous research and take the additional step\r\nof analyzing the sentiment of full articles and how they may or may not\r\nrelate to the differing headlines.\r\nMaking Choices on\r\nInclusion of Observations\r\nIn my initial look at the headline data, it was clear that not all of\r\nthe articles had different headlines; some are the same entries, and\r\nsome have “N/A” in the “print” version only, indicating they were\r\nonline-only stories. Although I initially felt inclined to leave the\r\n“N/A” observations in the analysis, I removed those observations as they\r\nwould not be relevant to my new research questions comparing the framing\r\nfor different audiences.\r\nI also removed whole sections where the API returned an observation\r\nas there was apparently use of the term “Afghanistan withdrawal”\r\nsomewhere in the article/entry, but the type of entry was clearly not\r\nbeing represented in the headline. For example, “Corrections” entries\r\nhave headlines consisting only of the term “Corrections” and the\r\ncorresponding date. Similar choices were made on the “Arts”, Books”, and\r\n“Podcasts” sections when entries are primarily the names of the things\r\nbeing reviewed that may have a reference to the Afghanistan withdrawal\r\nsomewhere in the text, but it is not relevant specifically to the\r\nwithdrawal time period being analyzed.\r\nWith few exceptions, this left the entirety of the “U.S.” and “World”\r\nnews sections, even if the content related to Afghanistan is not readily\r\nobservable. The count (~650) matched the number of articles pulled for\r\nthe hand coding research as well.\r\nGathering Data\r\nPrevious Process\r\nTo pull the data, I had to reduce the queries into more workable\r\ngroups that would not time out, given the NYT API limits. I was able to\r\npull the ~700 articles by chunk, then assemble them into a dataframe\r\nafter cleaning. I will not run the code in this post, as it was already\r\nrun and is an exhaustive process.\r\n\r\n\r\n\r\nAfter compiling the data, I re-formatted the date column and saving\r\nthe formatted tibble for offline access.\r\n\r\n\r\n\r\nLoad Data\r\nNow to the active review of the data. Loading the data from my\r\ncollection phase:\r\nLoad Data\r\n\r\n  article_id      date\r\n1          1 2/29/2020\r\n2          2 2/29/2020\r\n3          3  3/1/2020\r\n4          4  3/2/2020\r\n5          5  3/2/2020\r\n6          6  3/3/2020\r\n                                                                 headline_main\r\n1                              4 Takeaways From the U.S. Deal With the Taliban\r\n2    Taliban and U.S. Strike Deal to Withdraw American Troops From Afghanistan\r\n3           Afghanistan War Enters New Stage as U.S. Military Prepares to Exit\r\n4                 At Center of Taliban Deal, a U.S. Envoy Who Made It Personal\r\n5 U.S. Announces Troop Withdrawal in Afghanistan as Respite From Violence Ends\r\n6                                           Trump Speaks With a Taliban Leader\r\n  article_id      date\r\n1          1 2/29/2020\r\n2          2 2/29/2020\r\n3          3  3/1/2020\r\n4          4  3/2/2020\r\n5          5  3/2/2020\r\n6          6  3/3/2020\r\n                                                        headline_print\r\n1                                 Table Is Set For a Pullout And Talks\r\n2                              U.S. and Taliban Sign Withdrawal Accord\r\n3                                      A Mission Shift for Afghanistan\r\n4 At the Center of the Taliban Deal, a U.S. Envoy Who Made It Personal\r\n5                           U.S. Troop Reduction Begins in Afghanistan\r\n6               Pursuing Exit, Trump Talks  To a Leader Of the Taliban\r\n\r\nCreate Corpus\r\n\r\n\r\nmain_corpus <- corpus(main_headlines, docid_field = \"article_id\", text_field = \"headline_main\")\r\nprint_corpus <- corpus(print_headlines, docid_field = \"article_id\", text_field = \"headline_print\")\r\n\r\n\r\n\r\nAssign Type to Docvars\r\n\r\n\r\nmain_corpus$type <- \"Main Headline\"\r\nprint_corpus$type <- \"Print Headline\"\r\ndocvars(main_corpus, field = \"type\") <- main_corpus$type\r\ndocvars(print_corpus, field = \"type\") <- print_corpus$type\r\n\r\n\r\n\r\nTokenization\r\nAfter many process posts, I finally realized how to remove the “�”\r\nsymbol that has plagued me since starting working with this API by using\r\n“remove_symbols=TRUE” in addition to removing the punctuation when\r\ntokenizing. I also want to remove stopwords.\r\nMain Headlines\r\n\r\n[1] 346\r\nTokens consisting of 346 documents and 2 docvars.\r\n1 :\r\n[1] \"4\"         \"Takeaways\" \"U.S\"       \"Deal\"      \"Taliban\"  \r\n\r\n2 :\r\n[1] \"Taliban\"     \"U.S\"         \"Strike\"      \"Deal\"       \r\n[5] \"Withdraw\"    \"American\"    \"Troops\"      \"Afghanistan\"\r\n\r\n3 :\r\n[1] \"Afghanistan\" \"War\"         \"Enters\"      \"New\"        \r\n[5] \"Stage\"       \"U.S\"         \"Military\"    \"Prepares\"   \r\n[9] \"Exit\"       \r\n\r\n4 :\r\n[1] \"Center\"   \"Taliban\"  \"Deal\"     \"U.S\"      \"Envoy\"    \"Made\"    \r\n[7] \"Personal\"\r\n\r\n5 :\r\n[1] \"U.S\"         \"Announces\"   \"Troop\"       \"Withdrawal\" \r\n[5] \"Afghanistan\" \"Respite\"     \"Violence\"    \"Ends\"       \r\n\r\n6 :\r\n[1] \"Trump\"   \"Speaks\"  \"Taliban\" \"Leader\" \r\n\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\nPrint Headlines\r\n\r\n[1] 346\r\nTokens consisting of 346 documents and 2 docvars.\r\n1 :\r\n[1] \"Table\"   \"Set\"     \"Pullout\" \"Talks\"  \r\n\r\n2 :\r\n[1] \"U.S\"        \"Taliban\"    \"Sign\"       \"Withdrawal\" \"Accord\"    \r\n\r\n3 :\r\n[1] \"Mission\"     \"Shift\"       \"Afghanistan\"\r\n\r\n4 :\r\n[1] \"Center\"   \"Taliban\"  \"Deal\"     \"U.S\"      \"Envoy\"    \"Made\"    \r\n[7] \"Personal\"\r\n\r\n5 :\r\n[1] \"U.S\"         \"Troop\"       \"Reduction\"   \"Begins\"     \r\n[5] \"Afghanistan\"\r\n\r\n6 :\r\n[1] \"Pursuing\" \"Exit\"     \"Trump\"    \"Talks\"    \"Leader\"   \"Taliban\" \r\n\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\nDocument Feature Matrix\r\n\r\n\r\n\r\n\r\n\r\n#create a word frequency variable and the rankings\r\nmain_counts <- as.data.frame(sort(colSums(main_dfm),dec=T))\r\ncolnames(main_counts) <- c(\"Frequency\")\r\nmain_counts$Rank <- c(1:ncol(main_dfm))\r\nhead(main_counts)\r\n\r\n\r\n            Frequency Rank\r\nu.s               100    1\r\nafghanistan        87    2\r\nafghan             85    3\r\ntaliban            65    4\r\nbiden              53    5\r\nwar                30    6\r\n\r\nprint_counts <- as.data.frame(sort(colSums(print_dfm),dec=T))\r\ncolnames(print_counts) <- c(\"Frequency\")\r\nprint_counts$Rank <- c(1:ncol(print_dfm))\r\nhead(print_counts)\r\n\r\n\r\n            Frequency Rank\r\nu.s               100    1\r\ntaliban            66    2\r\nafghan             64    3\r\nafghanistan        56    4\r\nbiden              41    5\r\nexit               27    6\r\n\r\nNow I can take a look at this network of feature co-occurrences for\r\nthe main headlines:\r\n\r\n[1] 1232 1232\r\n[1] 20 20\r\n\r\n\r\nand for the print headlines:\r\n\r\n[1] 1178 1178\r\n[1] 20 20\r\n\r\n\r\nThis brings me to where I had previously stopped in my comparison and\r\nanalysis, and now that I’m able to produce a cleaner result, I’ll move\r\non to further analysis using the quanteda dictionary.\r\nDictionary Analysis\r\nliwcalike()\r\n\r\n [1] \"docname\"      \"Segment\"      \"WPS\"          \"WC\"          \r\n [5] \"Sixltr\"       \"Dic\"          \"anger\"        \"anticipation\"\r\n [9] \"disgust\"      \"fear\"         \"joy\"          \"negative\"    \r\n[13] \"positive\"     \"sadness\"      \"surprise\"     \"trust\"       \r\n[17] \"AllPunc\"      \"Period\"       \"Comma\"        \"Colon\"       \r\n[21] \"SemiC\"        \"QMark\"        \"Exclam\"       \"Dash\"        \r\n[25] \"Quote\"        \"Apostro\"      \"Parenth\"      \"OtherP\"      \r\n\r\nNRC\r\n\r\n\r\n# convert tokens from each headline data set to DFM using the dictionary \"NRC\"\r\nmain_nrc <- dfm(main_tokens) %>%\r\n  dfm_lookup(data_dictionary_NRC)\r\nprint_nrc <- dfm(print_tokens) %>%\r\n  dfm_lookup(data_dictionary_NRC)\r\n\r\ndim(main_nrc)\r\n\r\n\r\n[1] 346  10\r\n\r\nmain_nrc\r\n\r\n\r\nDocument-feature matrix of: 346 documents, 10 features (69.36% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            1       0    0   1        0        1       0\r\n   2     1            1       0    0   1        2        1       1\r\n   3     0            0       0    2   0        1        0       0\r\n   4     0            1       0    0   1        0        2       0\r\n   5     1            0       0    1   1        1        1       1\r\n   6     0            0       0    0   0        0        1       0\r\n    features\r\ndocs surprise trust\r\n   1        1     1\r\n   2        1     1\r\n   3        0     0\r\n   4        1     3\r\n   5        0     1\r\n   6        1     1\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\ndim(print_nrc)\r\n\r\n\r\n[1] 346  10\r\n\r\nprint_nrc\r\n\r\n\r\nDocument-feature matrix of: 346 documents, 10 features (71.47% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            0       0    0   0        0        0       0\r\n   2     0            0       0    0   0        0        1       0\r\n   3     0            0       0    0   0        0        0       0\r\n   4     0            1       0    0   1        0        2       0\r\n   5     0            0       0    0   0        0        0       0\r\n   6     0            0       0    0   0        0        1       0\r\n    features\r\ndocs surprise trust\r\n   1        0     0\r\n   2        0     1\r\n   3        0     0\r\n   4        1     3\r\n   5        0     0\r\n   6        1     1\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\nAnd use the information in a data frame to plot the output:\r\n\r\n\r\n#for the main headlines\r\ndf_main_nrc <- convert(main_nrc, to = \"data.frame\")\r\ndf_main_nrc$polarity <- (df_main_nrc$positive - df_main_nrc$negative)/(df_main_nrc$positive + df_main_nrc$negative)\r\ndf_main_nrc$polarity[which((df_main_nrc$positive + df_main_nrc$negative) == 0)] <- 0\r\n\r\nggplot(df_main_nrc) + \r\n  geom_histogram(aes(x=polarity)) + \r\n  theme_bw()\r\n\r\n\r\n\r\n#and the print headlines\r\ndf_print_nrc <- convert(print_nrc, to = \"data.frame\")\r\ndf_print_nrc$polarity <- (df_print_nrc$positive - df_print_nrc$negative)/(df_print_nrc$positive + df_print_nrc$negative)\r\ndf_print_nrc$polarity[which((df_print_nrc$positive + df_print_nrc$negative) == 0)] <- 0\r\n\r\nggplot(df_print_nrc) + \r\n  geom_histogram(aes(x=polarity)) + \r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nLooking at the headlines that are indicated as “1”, or positive in\r\nsentiment, it’s clear that this dictionary is not capturing the\r\nsentiment accurately.\r\n\r\n\r\nhead(main_corpus[which(df_main_nrc$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n1 :\r\n\"4 Takeaways From the U.S. Deal With the Taliban\"\r\n\r\n4 :\r\n\"At Center of Taliban Deal, a U.S. Envoy Who Made It Personal\"\r\n\r\n6 :\r\n\"Trump Speaks With a Taliban Leader\"\r\n\r\n8 :\r\n\"After Tours in Afghanistan, U.S. Veterans Weigh Peace With t...\"\r\n\r\n9 :\r\n\"Javier Pérez de Cuéllar Dies at 100; U.N. Chief Brokered Pea...\"\r\n\r\n10 :\r\n\"From the Afghan Peace Deal, a Weak and Pliable Neighbor for ...\"\r\n\r\nhead(print_corpus[which(df_print_nrc$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n2 :\r\n\"U.S. and Taliban Sign Withdrawal Accord\"\r\n\r\n4 :\r\n\"At the Center of the Taliban Deal, a U.S. Envoy Who Made It ...\"\r\n\r\n6 :\r\n\"Pursuing Exit, Trump Talks  To a Leader Of the Taliban\"\r\n\r\n7 :\r\n\"Attacks on Afghans by Taliban Rise After Signing of Peace De...\"\r\n\r\n8 :\r\n\"After Afghanistan Tours,  U.S. Veterans Appraise  Peace Deal...\"\r\n\r\n9 :\r\n\"Javier Pérez de Cuéllar, U.N. Chief  Behind Vital Peace Pact...\"\r\n\r\nLSD 2015\r\nI am going to want to look at multiple dictionaries to see if one can\r\nbest apply to this data. First, the LSD 2015 dictionary:\r\n\r\n\r\n# convert main corpus to DFM using the LSD2015 dictionary\r\nmain_lsd2015 <- dfm(tokens(main_corpus, remove_punct = TRUE),\r\n                              tolower = TRUE) %>%\r\n                          dfm_lookup(data_dictionary_LSD2015)\r\n# create main polarity measure for LSD2015\r\nmain_lsd2015 <- convert(main_lsd2015, to = \"data.frame\")\r\nmain_lsd2015$polarity <- (main_lsd2015$positive - main_lsd2015$negative)/(main_lsd2015$positive + main_lsd2015$negative)\r\nmain_lsd2015$polarity[which((main_lsd2015$positive + main_lsd2015$negative) == 0)] <- 0\r\n# convert print corpus to DFM using the LSD2015 dictionary\r\nprint_lsd2015 <- dfm(tokens(print_corpus, remove_punct = TRUE),\r\n                              tolower = TRUE) %>%\r\n                          dfm_lookup(data_dictionary_LSD2015)\r\n# create print polarity measure for LSD2015\r\nprint_lsd2015 <- convert(print_lsd2015, to = \"data.frame\")\r\nprint_lsd2015$polarity <- (print_lsd2015$positive - print_lsd2015$negative)/(print_lsd2015$positive + print_lsd2015$negative)\r\nprint_lsd2015$polarity[which((print_lsd2015$positive + print_lsd2015$negative) == 0)] <- 0\r\n\r\n\r\n\r\nGeneral Inquirer\r\nand the General Inquirer dictionary:\r\n\r\n\r\n# convert main corpus to DFM using the General Inquirer dictionary\r\nmain_geninq <- dfm(tokens(main_corpus, remove_punct = TRUE),\r\n                             tolower = TRUE) %>%\r\n                    dfm_lookup(data_dictionary_geninqposneg)\r\n# create main polarity measure for GenInq\r\nmain_geninq <- convert(main_geninq, to = \"data.frame\")\r\nmain_geninq$polarity <- (main_geninq$positive - main_geninq$negative)/(main_geninq$positive + main_geninq$negative)\r\nmain_geninq$polarity[which((main_geninq$positive + main_geninq$negative) == 0)] <- 0\r\n# convert print corpus to DFM using the General Inquirer dictionary\r\nprint_geninq <- dfm(tokens(print_corpus, remove_punct = TRUE),\r\n                             tolower = TRUE) %>%\r\n                    dfm_lookup(data_dictionary_geninqposneg)\r\n# create print polarity measure for GenInq\r\nprint_geninq <- convert(print_geninq, to = \"data.frame\")\r\nprint_geninq$polarity <- (print_geninq$positive - print_geninq$negative)/(print_geninq$positive + print_geninq $negative)\r\nprint_geninq$polarity[which((print_geninq$positive + print_geninq$negative) == 0)] <- 0\r\n\r\n\r\n\r\nNow I’m going to be able to compare the different dictionary scores\r\nin one data frame for each type of headline.\r\n\r\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy\r\n1          1         0                1           0        0       1\r\n2         10         0                3           0        0       2\r\n3        100         0                0           0        0       0\r\n4        101         0                2           0        0       1\r\n5        102         1                0           0        2       0\r\n6        103         1                1           0        1       0\r\n  nrc_negative nrc_positive nrc_sadness nrc_surprise nrc_trust\r\n1            0            1           0            1         1\r\n2            0            3           0            1         3\r\n3            1            0           1            0         0\r\n4            2            1           0            0         2\r\n5            2            0           2            1         0\r\n6            0            2           0            0         1\r\n  nrc_polarity lsd2015_negative lsd2015_positive lsd2015_neg_positive\r\n1    1.0000000                0                0                    0\r\n2    1.0000000                1                1                    0\r\n3   -1.0000000                1                0                    0\r\n4   -0.3333333                1                0                    0\r\n5   -1.0000000                2                0                    0\r\n6    1.0000000                0                0                    0\r\n  lsd2015_neg_negative lsd2015_polarity geninq_positive\r\n1                    0                0               1\r\n2                    0                0               2\r\n3                    0               -1               0\r\n4                    0               -1               0\r\n5                    0               -1               1\r\n6                    0                0               1\r\n  geninq_negative geninq_polarity\r\n1               1       0.0000000\r\n2               1       0.3333333\r\n3               0       0.0000000\r\n4               2      -1.0000000\r\n5               1       0.0000000\r\n6               1       0.0000000\r\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy\r\n1          1         0                0           0        0       0\r\n2         10         0                2           0        0       1\r\n3        100         0                0           0        0       0\r\n4        101         0                2           0        0       1\r\n5        102         1                0           0        4       0\r\n6        103         1                1           0        1       0\r\n  nrc_negative nrc_positive nrc_sadness nrc_surprise nrc_trust\r\n1            0            0           0            0         0\r\n2            0            2           0            1         2\r\n3            1            0           1            0         0\r\n4            2            1           0            0         2\r\n5            3            0           3            1         0\r\n6            0            1           0            0         0\r\n  nrc_polarity lsd2015_negative lsd2015_positive lsd2015_neg_positive\r\n1    0.0000000                0                0                    0\r\n2    1.0000000                1                0                    0\r\n3   -1.0000000                1                0                    0\r\n4   -0.3333333                1                0                    0\r\n5   -1.0000000                2                0                    0\r\n6    1.0000000                0                0                    0\r\n  lsd2015_neg_negative lsd2015_polarity geninq_positive\r\n1                    0                0               0\r\n2                    0               -1               2\r\n3                    0               -1               0\r\n4                    0               -1               0\r\n5                    0               -1               1\r\n6                    0                0               1\r\n  geninq_negative geninq_polarity\r\n1               0       0.0000000\r\n2               1       0.3333333\r\n3               1      -1.0000000\r\n4               2      -1.0000000\r\n5               0       1.0000000\r\n6               0       1.0000000\r\n\r\nNow that we have them all in a single data frame, it’s\r\nstraightforward to figure out a bit about how well our different\r\nmeasures of polarity agree across the different approaches by looking at\r\ntheir correlation using the “cor()” function.\r\n\r\n\r\ncor(main_sent$nrc_polarity, main_sent$lsd2015_polarity)\r\n\r\n\r\n[1] 0.4968335\r\n\r\ncor(main_sent$nrc_polarity, main_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4813359\r\n\r\ncor(main_sent$lsd2015_polarity, main_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.5327856\r\n\r\n\r\n\r\ncor(print_sent$nrc_polarity, print_sent$lsd2015_polarity)\r\n\r\n\r\n[1] 0.4197161\r\n\r\ncor(print_sent$nrc_polarity, print_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4917256\r\n\r\ncor(print_sent$lsd2015_polarity, print_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4921922\r\n\r\nAnnotation\r\n\r\n\r\nlibrary(cleanNLP)\r\ncnlp_init_udpipe()\r\n\r\n\r\n\r\n\r\n\r\nlibrary(tidyr)\r\n#amain <- as_tibble(headlines_main)\r\n#annotated_main <- cnlp_annotate(main_corpus)\r\n#annotated_main\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/headline-analysis/unnamed-chunk-12-1.png",
    "last_modified": "2022-04-23T00:12:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/pdf-analysis/",
    "title": "Analysis of PDF Articles",
    "description": "Text as Data Project-Article Sentiment Research",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/NYT-Analysis/"
      }
    ],
    "date": "2022-04-17",
    "categories": [
      "text as data",
      "NYT text analysis project"
    ],
    "contents": "\r\n\r\nContents\r\nGetting Started\r\nPulling in the PDF docs\r\nExtracting\r\nPDF Files being examined (random at this time - exploratory)\r\nInspect the first\r\narticle\r\nInspecting Individual\r\nArticles\r\nUnlist\r\n\r\nConclusion\r\n\r\nGetting Started\r\nThe primary goal of this aspect of research is to refine the process\r\nfor examining the content of the full articles for which the main\r\nvs. print headlines are the most different from each other in the\r\nprimary project analysis.\r\nPulling in the PDF docs\r\nI have the PDF files in my working directory. Using the\r\n“list.files()” function from the “pdftools” package, I can create a\r\nvector of PDF file names, specifying only files that end in “.pdf”.\r\n\r\n\r\nShow code\r\n\r\n#load libraries\r\nlibrary(pdftools)\r\nlibrary(readtext)\r\nlibrary(readr)\r\nlibrary(tm)\r\nlibrary(tidytext)\r\nlibrary(stringr)\r\nlibrary(MASS)\r\nlibrary(tidyverse)\r\nlibrary(plyr); library(dplyr)\r\nlibrary(quanteda)\r\nlibrary(purrr)\r\nlibrary(here)\r\n\r\n\r\n\r\nExtracting\r\nPDF Files being examined (random at this time - exploratory)\r\n\r\n\r\n#create file names\r\nfiles <- list.files(pattern = \"pdf$\")\r\n\r\n#extract the pdf file data\r\nnyt_articles <- lapply(files, pdf_text)\r\n\r\n#apply length functions\r\nlapply(nyt_articles, length)\r\n\r\n\r\n[[1]]\r\n[1] 4\r\n\r\n[[2]]\r\n[1] 2\r\n\r\n[[3]]\r\n[1] 3\r\n\r\n[[4]]\r\n[1] 4\r\n\r\n[[5]]\r\n[1] 10\r\n\r\n[[6]]\r\n[1] 6\r\n\r\n[[7]]\r\n[1] 2\r\n\r\n[[8]]\r\n[1] 5\r\n\r\n[[9]]\r\n[1] 5\r\n\r\n[[10]]\r\n[1] 2\r\n\r\n#view the structure of the list\r\nstr(nyt_articles)\r\n\r\n\r\nList of 10\r\n $ : chr [1:4] \"                                 https://www.nytimes.com/2021/04/13/us/politics/samantha-power-biden.html\\n\\n\\n\"| __truncated__ \"                                Secretary of State Antony J. Blinken at the opening session of talks with China\"| __truncated__ \"The mostly benign prodding by Democrats and Republicans during the hearing signaled how countering China has be\"| __truncated__ \"“There is so much that can be done between bombing and nothing,” Mr. Prendergast said, paraphrasing Luis Moreno\"| __truncated__\r\n $ : chr [1:2] \"                            https://www.nytimes.com/2021/04/29/world/asia/central-asia-border-\\n               \"| __truncated__ \"In announcing the cease-fire, the Kyrgyz Ministry of Interior said that it “does not have\\ndesigns on foreign t\"| __truncated__\r\n $ : chr [1:3] \"                                https://www.nytimes.com/2021/08/05/us/politics/taliban-afghanistan-peace-deal.h\"| __truncated__ \"The statement came as Taliban representatives met with Afghan government officials, including Mr. Abdullah, for\"| __truncated__ \"“The Taliban is not interested in negotiating seriously right now because of what’s happening on the battlefiel\"| __truncated__\r\n $ : chr [1:4] \"                                https://www.nytimes.com/2021/08/08/us/politics/taliban-afghanistan-united-state\"| __truncated__ \"Over the past week, Taliban fighters have moved swiftly to retake cities around Afghanistan, assassinated gover\"| __truncated__ \"                                 Ms. Psaki speaking to reporters at the White House, on Friday. Tom Brenner for\"| __truncated__ \"Mr. Biden, declaring that the United States had long ago accomplished its mission of denying terrorists a haven\"| __truncated__\r\n $ : chr [1:10] \"                                https://www.nytimes.com/2021/08/30/world/asia/us-withdrawal-afghanistan-kabul.h\"| __truncated__ \"Old Soviet tanks litter the grounds of Bala Hissar, outside Kunduz. Jim Huylebroek for The New York Times\\n\" \"  Khalil Haqqani, a Taliban leader, appeared at Friday prayers in Kabul this month with an American-made M-4 ri\"| __truncated__ \"The Taliban’s leverage, earned after years of fighting the world’s most advanced military, multiplied as they c\"| __truncated__ ...\r\n $ : chr [1:6] \"                                 https://www.nytimes.com/2021/09/01/world/asia/afghanistan-taliban-government-l\"| __truncated__ \"  Internally displaced Afghans fleeing the fighting in the north still live at a camp in the Sarawi Shomali par\"| __truncated__ \"  A vendor selling Taliban flags in Kabul on Friday near posters of the senior Taliban officials Amir Khan Mutt\"| __truncated__ \"The Taliban are also fighting stubborn opposition forces led by National Resistance Front leaders in Panjshir P\"| __truncated__ ...\r\n $ : chr [1:2] \"                               https://www.nytimes.com/2021/09/02/us/politics/congress-pentagon-budget-biden.ht\"| __truncated__ \"The lopsided vote underscored another reality: Even as the hard-charging liberal bloc of lawmakers pledging to \"| __truncated__\r\n $ : chr [1:5] \"                                 https://www.nytimes.com/2021/09/07/us/politics/afghan-war-iraq-veterans.html\\n\"| __truncated__ \"                                Jen Burch said the doctors who examined her in 2014 found ground glass nodules \"| __truncated__ \"                                 Melissa Gauntner has dealt with dual traumas and has at times been gripped wit\"| __truncated__ \"In military families, scholars find what they call secondary traumatic distress, symptoms of anxiety stemming f\"| __truncated__ ...\r\n $ : chr [1:5] \"                                 https://www.nytimes.com/2020/10/05/world/asia/afghan-peace-talks-children.html\"| __truncated__ \"                                   Fatima Gailani, whose father was one of the leaders of the mujahedeen resist\"| __truncated__ \"                                Anas Haqqani, the youngest son of the insurgent chief Jalaluddin Haqqani, is pa\"| __truncated__ \"                                 Jalaluddin Haqqani in an undated photo from a video released by the Taliban on\"| __truncated__ ...\r\n $ : chr [1:2] \"                                https://www.nytimes.com/2020/03/04/world/asia/afghanistan-taliban-violence.html\"| __truncated__ \"     Understand the Taliban Takeover in Afghanistan\\n\\n     Who are the Taliban? The Taliban arose in 1994 amid\"| __truncated__\r\n\r\nInspect the first article\r\n\r\n\r\nhead(nyt_articles[1])\r\n\r\n\r\n[[1]]\r\n[1] \"                                 https://www.nytimes.com/2021/04/13/us/politics/samantha-power-biden.html\\n\\n\\n\\nAfter Backing Military Force in Past, U.S.A.I.D. Nominee Focuses on Deploying Soft\\nPower\\nIf confirmed to oversee the U.S. Agency for International Development, Samantha Power will confront adversaries by bolstering\\ndemocracy and human rights. China is an early focus.\\n\\n\\n          By Lara Jakes\\n\\nPublished April 13, 2021   Updated April 14, 2021\\n\\n\\nWASHINGTON — Near the end of the 2014 documentary “Watchers of the Sky,” which chronicles the origins of the legal definition\\nof genocide, Samantha Power grows emotional. At the time, Ms. Power was President Barack Obama’s ambassador to the United\\nNations, and, she said, had “great visibility into a lot of the pain” in the world.\\n\\nFrom that perch, preventing mass atrocities abroad required “thinking through what we can do about it, to exhaust the tools at your\\ndisposal,” Ms. Power said in the film. “And I always think about the privilege of, you know, of getting to try — just to try.”\\n\\nFew doubt Ms. Power’s zeal — given her career as a war correspondent, human rights activist, academic expert and foreign policy\\nadviser — even if it has meant advocating military force to stop widespread killings.\\n\\nNow, as President Biden’s nominee to lead the United States Agency for International Development, she is preparing to rejoin the\\ngovernment as an administrator of soft power, and resist using weapons as a means of deterrence and punishment that she has\\npushed for in the past.\\n\\nA Senate committee is expected to vote Thursday on her nomination to lead one of the world’s largest distributors of humanitarian\\naid.\\n\\nIf she is confirmed, Mr. Biden will also seat her on the National Security Council, where during the Obama administration she\\npressed for military intervention to protect civilians from state-sponsored attacks in Libya in 2011 and Syria in 2013. (However, she\\nalso opposed the 2003 invasion of Iraq.)\\n\\nThat she will be back at the table at the council — and again almost certain to be debating whether to entangle American forces in\\nenduring conflicts — has concerned some officials, analysts and think tank experts who demand military restraint from the Biden\\nadministration. Mr. Biden appears to be leaning that way: He has embraced economic sanctions as a tool of hard power and is\\nexpected to announce a full withdrawal of American troops from Afghanistan by Sept. 11, ending the United States’ longest war.\\n\\n“If you’re talking about humanitarianism, famine, the wars — really, other than natural causes, war is the No. 1 cause of famine\\naround the world,” Senator Rand Paul, Republican of Kentucky, told Ms. Power last month during her Senate confirmation hearing.\\n“Are you willing to admit that the Libyan and Syrian interventions that you advocated for were a mistake?”\\n\\nMs. Power did not. “When these situations arise, it’s a question almost of lesser evils — that the choices are very challenging,” she\\nsaid.\\n\\nBy its very nature, the U.S. aid agency takes a long-term view of the world compared with the immediacy of military action. Beyond\\nthe roughly $6 billion in humanitarian aid it is delivering this year to disaster-ridden nations, the agency seeks to prevent conflict at\\nits roots, largely bolstering economies, countering state corruption and fostering democracy and human rights.\\n\\nThat mission is central to Mr. Biden’s foreign policy, and will perhaps prove nowhere more pivotal than in his global competition\\nwith China.\\n\\nLast month, Secretary of State Antony J. Blinken assured allies that they would not be backed into an “‘us-or-them’ choice with\\nChina” as the two superpowers vie for economic, diplomatic and military advantage.\\n\"\r\n[2] \"                                Secretary of State Antony J. Blinken at the opening session of talks with China at the\\n                                Captain Cook hotel in Anchorage. Pool photo by Frederic J. Brown\\n\\n\\n\\nInstead, the United States is highlighting what officials call China’s malign ideology and self-interests as it expands an influence\\ncampaign across Africa, Europe and South America with financial loans, infrastructure funds, coronavirus vaccines and advanced\\ntechnology.\\n\\nThe Trump administration also seized on China’s human rights abuses — particularly against ethnic Uyghurs in the country’s\\nwestern region of Xinjiang — to persuade allies to turn against Beijing. On the Trump administration’s final day in office, Mike\\nPompeo, the secretary of state, declared China’s oppression against Uyghurs as an act of genocide, and he criticized Beijing’s\\nviolent suppression of dissidents in Hong Kong and military harassment of Taiwan.\\n\\n\\n                                Sign Up for On Politics A guide to the political news cycle, cutting\\n                                through the spin and delivering clarity from the chaos. Get it sent to your\\n                                inbox.\\n\\n\\nOfficials said China’s much-debated Belt and Road Initiative was a prime battleground for U.S.A.I.D. to challenge Beijing.\\n\\nRepresentative Tom Malinowski, Democrat of New Jersey and a former assistant secretary of state for democracy and human\\nrights for Mr. Obama, described a “perception that China is exporting corruption” with its loans and development projects.\\n\\nFor example, a study in February by the International Republican Institute, a private nonprofit group that receives government\\nfunding and promotes democracy, concluded that Panama’s decision in 2017 to sever diplomatic ties with Taiwan “appears to have\\nbeen driven by payoffs” from China. It also noted that Nepal regularly revoked the legal status of Tibetan refugees after becoming\\neconomically reliant on Beijing.\\n\\nThe American aid agency alone cannot match the funds that China has seeded in developing countries. But Mr. Malinowski said its\\nsupport to journalists, legal advisers and legitimate opposition groups could “expose and combat” corrosive foreign leaders who\\nhad benefited from Beijing’s financial backing and playbook for how to remain in power.\\n\\n“There is one issue that has risen to the top in this administration that I know she is very focused on, and that’s fighting corruption,”\\nMr. Malinowski said of Ms. Power. “And U.S.A.I.D. has a very important role to play there, potentially.”\\n\\nAt her confirmation hearing in March, Ms. Power told senators she was moved to pursue a career in foreign policy after the 1989\\nmassacre of protesters in Tiananmen Square in Beijing. She described China’s “coercive and predatory approach, which is so\\ntransactional” in its dealings with developing countries that ultimately become dependent on Beijing through what she called “debt-\\ntrap diplomacy.”\\n\\n“I think it’s not going over that well, and that creates an opening for the United States,” Ms. Power told Senator Todd Young,\\nRepublican of Indiana.\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \r\n[3] \"The mostly benign prodding by Democrats and Republicans during the hearing signaled how countering China has become a rare,\\nif reliable, issue of bipartisanship in Congress. “It’s absolutely essential that our development dollars, I think, be used to advance\\nour geostrategic priorities,” Mr. Young said.\\n\\nThe aid agency and the State Department have budgeted about $2 billion on programs to foster democracy, human rights and open\\ngovernance abroad in the 2021 fiscal year — one-third as much as funding for humanitarian assistance.\\n\\nIt is an area that Ms. Power is expected to expand. The Biden administration’s first budget blueprint, released on Friday, asserted it\\nwould commit an unspecified but “significant increase in resources” to advance human rights and democracy while thwarting\\ncorruption and authoritarianism.\\n\\n\\n\\n\\n                               Asylum seekers from Central America crossing the Paso del Norte International Bridge,\\n                               in Ciudad Juarez, Mexico. One of Ms. Power’s priorities will be to target corruption,\\n                               violence and poverty in the region. Jose Luis Gonzalez/Reuters\\n\\n\\n\\nThe spending plan also will support another of Ms. Power’s priorities: targeting corruption, violence and poverty in Central\\nAmerica as a means to curb the flow of thousands of migrants who head to the southwestern border each year. The Biden\\nadministration is banking on a $4 billion strategy through 2025 — including an initial tranche of $861 million proposed this year — to\\nhelp stabilize the region.\\n\\nIn El Salvador, for example, homicides dropped 61 percent after a U.S.A.I.D. effort to reduce violence from 2015 to 2017, Ms. Power\\ntold the senators, and the agency’s programs in Honduras have yielded similar results. The programs not only supported local\\nprosecutors but also brought together government officials, businesses and church and community leaders to divert young people\\nfrom gangs through job training, tutoring and artistic activities.\\n\\nShe was met with some skepticism.\\n\\nSenator Rob Portman, Republican of Ohio, noted that the number of children from Central America at the border had steadily\\nincreased since January, even though the United States spent $3.6 billion over the past five years on similar efforts.\\n\\n“The results are not impressive,” Mr. Portman said. “It’s an economic issue, primarily,” and “people will still be looking to come to\\nthe United States.”\\n\\nExplaining foreign policy decisions to the American people, and making it relevant to their lives, is a driving theme of the State\\nDepartment under Mr. Biden. Ms. Power can reach back to her own experiences as both an immigrant from Ireland and a\\nstoryteller to make the case for easing the border crisis by attacking its root causes.\\n\\n“That’s part of the job, too — you’ve got to be a salesperson, you’ve got to go out there and explain to people, ‘Here’s why we need\\nmore resources to do this work, and here’s where U.S.A.I.D. can be an incredibly important partner,’” said John Prendergast, a\\nlongtime human rights and anticorruption activist and close friend to Ms. Power.\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \r\n[4] \"“There is so much that can be done between bombing and nothing,” Mr. Prendergast said, paraphrasing Luis Moreno Ocampo, the\\nformer prosector of the International Criminal Court who was featured in the same documentary about genocide as Ms. Power.\\n“And Samantha’s whole work and life has been between those two extremes.”\\n\\nGayle Smith, who ran the aid agency for Mr. Obama and is now the State Department’s coronavirus vaccine envoy, put it more\\nbluntly.\\n\\n“It’s not like U.S.A.I.D. is going to invade somebody,” she said.\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \r\n\r\nInspecting Individual\r\nArticles\r\nNow I’m going to use “purrr” to “pluck()” each of the articles as\r\nits’ own vector and create a corpus of each article to examine.\r\n\r\n\r\narticle_111 <- nyt_articles %>% \r\n  pluck(1)\r\narticle_111 <- as_vector(article_111)\r\n\r\narticle_111_corpus <- corpus(article_111)\r\narticle_111_summary <- summary(article_111_corpus)\r\narticle_111_summary\r\n\r\n\r\nCorpus consisting of 4 documents, showing 4 documents:\r\n\r\n  Text Types Tokens Sentences\r\n text1   357    688        24\r\n text2   289    523        19\r\n text3   304    562        18\r\n text4    76    105         4\r\n\r\nI also found a very interesting way to pul the text and save them as\r\nindividual .txt files, but for now I’m just going to note that as an\r\nalternative process. I’ve struggled quite a bit to get the PDF text read\r\ncompared to the headlines.\r\n\r\n\r\nShow code\r\n\r\nconvertpdf2txt <- function(dirpath){\r\n  files <- list.files(dirpath, full.names = T)\r\n  x <- sapply(files, function(x){\r\n  x <- pdftools::pdf_text(x) %>%\r\n  paste(sep = \" \") %>%\r\n  stringr::str_replace_all(fixed(\"\\n\"), \" \") %>%\r\n  stringr::str_replace_all(fixed(\"\\r\"), \" \") %>%\r\n  stringr::str_replace_all(fixed(\"\\t\"), \" \") %>%\r\n  stringr::str_replace_all(fixed(\"\\\"\"), \" \") %>%\r\n  paste(sep = \" \", collapse = \" \") %>%\r\n  stringr::str_squish() %>%\r\n  stringr::str_replace_all(\"- \", \"\") \r\n  return(x)\r\n    })\r\n}\r\n# apply function\r\ntxts <- convertpdf2txt(\"./files\")\r\n# inspect the structure of the txts element\r\nstr(txts)\r\n\r\n\r\n Named chr [1:10] \"https://www.nytimes.com/2021/04/13/us/politics/samantha-power-biden.html After Backing Military Force in Past, \"| __truncated__ ...\r\n - attr(*, \"names\")= chr [1:10] \"./files/article_111.pdf\" \"./files/article_132.pdf\" \"./files/article_193.pdf\" \"./files/article_196.pdf\" ...\r\n\r\n\r\n\r\nShow code\r\n\r\n#apply length functions\r\nlapply(txts, length)\r\n\r\n\r\n$`./files/article_111.pdf`\r\n[1] 1\r\n\r\n$`./files/article_132.pdf`\r\n[1] 1\r\n\r\n$`./files/article_193.pdf`\r\n[1] 1\r\n\r\n$`./files/article_196.pdf`\r\n[1] 1\r\n\r\n$`./files/article_278.pdf`\r\n[1] 1\r\n\r\n$`./files/article_288.pdf`\r\n[1] 1\r\n\r\n$`./files/article_293.pdf`\r\n[1] 1\r\n\r\n$`./files/article_300.pdf`\r\n[1] 1\r\n\r\n$`./files/article_56.pdf`\r\n[1] 1\r\n\r\n$`./files/article_7.pdf`\r\n[1] 1\r\n\r\nShow code\r\n\r\n#view the structure of the list\r\nstr(txts)\r\n\r\n\r\n Named chr [1:10] \"https://www.nytimes.com/2021/04/13/us/politics/samantha-power-biden.html After Backing Military Force in Past, \"| __truncated__ ...\r\n - attr(*, \"names\")= chr [1:10] \"./files/article_111.pdf\" \"./files/article_132.pdf\" \"./files/article_193.pdf\" \"./files/article_196.pdf\" ...\r\n\r\nShow code\r\n\r\n# add names to txt files\r\nnames(txts) <- paste(\"nyt\", 1:length(txts), sep = \"\")\r\n# save result to disc\r\nlapply(seq_along(txts), function(i)writeLines(text = unlist(txts[i]),\r\n    con = paste(\"./txts\", names(txts)[i],\".txt\", sep = \"\")))\r\n\r\n\r\n[[1]]\r\nNULL\r\n\r\n[[2]]\r\nNULL\r\n\r\n[[3]]\r\nNULL\r\n\r\n[[4]]\r\nNULL\r\n\r\n[[5]]\r\nNULL\r\n\r\n[[6]]\r\nNULL\r\n\r\n[[7]]\r\nNULL\r\n\r\n[[8]]\r\nNULL\r\n\r\n[[9]]\r\nNULL\r\n\r\n[[10]]\r\nNULL\r\n\r\nUnlist\r\nDocumenting, for now, the ways I’m struggling with so I can find out\r\nwhy.\r\n\r\n\r\n#convert list to vector\r\n#nyt_vector <- unlist(nyt_articles, recursive = TRUE)\r\n#put articles into data frame\r\n#nyt_df <- as.data.frame(nyt_vector, row.names = NULL, stringsAsFactors = FALSE)\r\n\r\n\r\n\r\n\r\n\r\n#create corpus\r\n#nyt_corpus <- corpus(txts)\r\n#confirming class of corpus\r\n#class(nyt_corpus)\r\n#confirm length of corpus\r\n#length(nyt_corpus)\r\n\r\n\r\n\r\nConclusion\r\nI will not be able to fit this type of analysis into the scope of my\r\ncurrent project. I will use this in further studies.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-23T13:23:57-05:00",
    "input_file": "pdf-analysis.knit.md"
  },
  {
    "path": "posts/lit-review/",
    "title": "Literature Review",
    "description": "Text as Data Project-Literature Review",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/NYT-Analysis/"
      }
    ],
    "date": "2022-04-15",
    "categories": [
      "text as data",
      "NYT text analysis project",
      "literature review"
    ],
    "contents": "\r\n\r\nContents\r\nAnnotated\r\nBibliography (Starting with Abstracts Only - In Development)\r\nLiu & Huang (2022)\r\nChan, et al. (2021)\r\nVan Atteveldt, et\r\nal. (2021)\r\nBurggraff & Trilling\r\n(2020)\r\nBoukes, et al. (2020)\r\nSong, et al. (2020)\r\nRudkowsky, et al. (2018)\r\nSilva (2017)\r\nGottlieb (2015)\r\nDiakopoulos (2015)\r\nGrimmer & Stewart\r\n(2013)\r\nKothari (2010)\r\nKiousis (2004)\r\nAlthaus & Tewksbury\r\n(2002)\r\n\r\n\r\nAnnotated\r\nBibliography (Starting with Abstracts Only - In Development)\r\nLiu & Huang (2022)\r\nLiu, M., & Huang, J. (2022). “Climate change” versus “global\r\nwarming”: A corpus-assisted discourse analysis of two popular terms in\r\nthe New York Times. Journal of World Languages. https://doi.org/10.1515/jwl-2022-0004\r\nAbstract\r\n“Climate change” and “global warming” are two popular terms that may\r\nbe often used interchangeably in news media. This study proposes to give\r\na corpusassisted discourse study of the representations of climate\r\nchange and global warming in the New York Times (2000–2019) in order to\r\nexamine how they are actually used in the newspaper. The findings show\r\nboth similarities and differences in their representations in terms of\r\nthe associated topics/themes, the particular ways of framing, and the\r\nperspectivization strategy employed. It is argued that a corpus-assisted\r\ndiscourse study of a large sample of news articles presents a more\r\naccurate picture of the actual use of the two terms in news media.\r\nChan, et al. (2021)\r\nChan, C., Bajjalieh, J., Auvil, L., Wessler, H., Althaus, S.,\r\nWelbers, K., Atteveldt, W. van, & Jungblut, M. (2021). Four best\r\npractices for measuring news sentiment using ‘off-the-shelf’\r\ndictionaries: A large-scale p-hacking experiment. Computational\r\nCommunication Research, 3(1), 1–27.\r\nAbstract\r\nWe examined the validity of 37 sentiment scores based on\r\ndictionary-based methods using a large news corpus and demonstrated the\r\nrisk of generating a spectrum of results with different levels of\r\nstatistical significance by presenting an analysis of relationships\r\nbetween news sentiment and U.S. presidential approval. We summarize our\r\nfindings into four best practices: 1) use a suitable sentiment\r\ndictionary; 2) do not assume that the validity and reliability of the\r\ndictionary is ‘built-in’; 3) check for the influence of content length\r\nand 4) do not use multiple dictionaries to test the same statistical\r\nhypothesis.\r\nVan Atteveldt, et al. (2021)\r\nvan Atteveldt, W., van der Velden, M. A. C. G., & Boukes, M.\r\n(2021). The Validity of Sentiment Analysis: Comparing Manual Annotation,\r\nCrowd-Coding, Dictionary Approaches, and Machine Learning Algorithms.\r\nCommunication Methods and Measures, 15(2), 121–140. https://doi.org/10.1080/19312458.2020.1869198\r\nAbstract\r\nSentiment is central to many studies of communication science, from\r\nnegativity and polarization in political communication to analyzing\r\nproduct reviews and social media comments in other sub-fields. This\r\nstudy provides an exhaustive comparison of sentiment analysis methods,\r\nusing a validation set of Dutch economic headlines to compare the\r\nperformance of manual annotation, crowd coding, numerous dictionaries\r\nand machine learning using both traditional and deep learning\r\nalgorithms. The three main conclusions of this article are that: (1) The\r\nbest performance is still attained with trained human or crowd coding;\r\n(2) None of the used dictionaries come close to acceptable levels of\r\nvalidity; and (3) machine learning, especially deep learning,\r\nsubstantially outperforms dictionary-based methods but falls short of\r\nhuman performance. From these findings, we stress the importance of\r\nalways validating automatic text analysis methods before usage.\r\nMoreover, we provide a recommended step-bystep approach for (automated)\r\ntext analysis projects to ensure both efficiency and validity.\r\nBurggraff & Trilling (2020)\r\nBurggraaff, C., & Trilling, D. (2020). Through a different\r\ngate: An automated content analysis of how online news and print news\r\ndiffer. Journalism, 21(1), 112–129. https://doi.org/10.1177/1464884917716699\r\nAbstract\r\nWe investigate how news values differ between online and print news\r\narticles. We hypothesize that print and online articles differ in terms\r\nof news values because of differences in the routines used to produce\r\nthem. Based on a quantitative automated content analysis of N = 762,095\r\nDutch news items, we show that online news items are more likely to be\r\nfollow-up items than print items, and that there are further differences\r\nregarding news values like references to persons, the power elite,\r\nnegativity, and positivity. In order to conduct this large-scale\r\nanalysis, we developed innovative methods to automatically code a wide\r\nrange of news values. In particular, this article demonstrates how\r\ntechniques such as sentiment analysis, named entity recognition,\r\nsupervised machine learning, and automated queries of external databases\r\ncan be combined and used to study journalistic content. Possible\r\nexplanations for the difference found between online and offline news\r\nare discussed.\r\nBoukes, et al. (2020)\r\nBoukes, M., van de Velde, B., Araujo, T., & Vliegenthart, R.\r\n(2020). What’s the Tone? Easy Doesn’t Do It: Analyzing Performance and\r\nAgreement Between Off-the-Shelf Sentiment Analysis Tools. Communication\r\nMethods and Measures, 14(2), 83–104. https://doi.org/10.1080/19312458.2019.1671966\r\nAbstract\r\nThis article scrutinizes the method of automated content analysis to\r\nmeasure the tone of news coverage. We compare a range of off-the-shelf\r\nsentiment analysis tools to manually coded economic news as well as\r\nexamine the agreement between these dictionary approaches themselves. We\r\nassess the performance of five off-the-shelf sentiment analysis tools\r\nand two tailor-made dictionary-based approaches. The analyses result in\r\nfive conclusions. First, there is little overlap between the\r\noff-the-shelf tools; causing wide divergence in terms of tone\r\nmeasurement. Second, there is no stronger overlap with manual coding for\r\nshort texts (i.e., headlines) than for long texts (i.e., full articles).\r\nThird, an approach that combines individual dictionaries achieves a\r\ncomparably good performance. Fourth, precision may increase to\r\nacceptable levels at higher levels of granularity. Fifth, performance of\r\ndictionary approaches depends more on the number of relevant keywords in\r\nthe dictionary than on the number of valenced words as such; a small\r\ntailor-made lexicon was not inferior to large established dictionaries.\r\nAltogether, we conclude that off-the-shelf sentiment analysis tools are\r\nmostly unreliable and unsuitable for research purposes – at least in the\r\ncontext of Dutch economic news – and manual validation for the specific\r\nlanguage, domain, and genre of the research project at hand is always\r\nwarranted.\r\nSong, et al. (2020)\r\nSong, H., Tolochko, P., Eberl, J.-M., Eisele, O., Greussing, E.,\r\nHeidenreich, T., Lind, F., Galyga, S., & Boomgaarden, H. G. (2020).\r\nIn Validations We Trust? The Impact of Imperfect Human Annotations as a\r\nGold Standard on the Quality of Validation of Automated Content\r\nAnalysis. Political Communication, 37(4), 550–572. https://doi.org/10.1080/10584609.2020.1723752\r\nAbstract\r\nPolitical communication has become one of the central arenas of\r\ninnovation in the application of automated analysis approaches to\r\never-growing quantities of digitized texts. However, although\r\nresearchers routinely and conveniently resort to certain forms of human\r\ncoding to validate the results derived from automated procedures, in\r\npractice the actual “quality assurance” of such a “gold standard” often\r\ngoes unchecked. Contemporary practices of validation via manual\r\nannotations are far from being acknowledged as best practices in the\r\nliterature, and the reporting and interpretation of validation\r\nprocedures differ greatly. We systematically assess the connection\r\nbetween the quality of human judgment in manual annotations and the\r\nrelative performance evaluations of automated procedures against true\r\nstandards by relying on large-scale Monte Carlo simulations. The results\r\nfrom the simulations confirm that there is a substantially greater risk\r\nof a researcher reaching an incorrect conclusion regarding the\r\nperformance of automated procedures when the quality of manual\r\nannotations used for validation is not properly ensured. Our\r\ncontribution should therefore be regarded as a call for the systematic\r\napplication of high-quality manual validation materials in any political\r\ncommunication study, drawing on automated text analysis procedures.\r\nRudkowsky, et al. (2018)\r\nRudkowsky, E., Haselmayer, M., Wastian, M., Jenny, M., Emrich,\r\nŠ., & Sedlmair, M. (2018). More than Bags of Words: Sentiment\r\nAnalysis with Word Embeddings. Communication Methods and Measures,\r\n12(2–3), 140–157. https://doi.org/10.1080/19312458.2018.1455817\r\nAbstract\r\nMoving beyond the dominant bag-of-words approach to sentiment\r\nanalysis we introduce an alternative procedure based on distributed word\r\nembeddings. The strength of word embeddings is the ability to capture\r\nsimilarities in word meaning. We use word embeddings as part of a\r\nsupervised machine learning procedure which estimates levels of\r\nnegativity in parliamentary speeches. The procedure’s accuracy is\r\nevaluated with crowdcoded training sentences; its external validity\r\nthrough a study of patterns of negativity in Austrian parliamentary\r\nspeeches. The results show the potential of the word embeddings approach\r\nfor sentiment analysis in the social sciences.\r\nSilva (2017)\r\nSilva, D. M. D. (2017). The Othering of Muslims: Discourses of\r\nRadicalization in the New York Times, 1969–2014. Sociological Forum,\r\n32(1), 138–161. https://doi.org/10.1111/socf.12321\r\nAbstract\r\nIn this article, I engage with Edward Said’s Orientalism and various\r\nperspectives within the othering paradigm to analyze the emergence and\r\ntransformation of radicalization discourses in the news media. Employing\r\ndiscourse analysis of 607 New York Times articles from 1969 to 2014,\r\nthis article demonstrates that radicalization discourses are not new but\r\nare the result of complex sociolinguistic and historical developments\r\nthat cannot be reduced to dominant contemporary understandings of the\r\nconcept or to singular events or crises. The news articles were then\r\ncompared to 850 government documents, speeches, and other official\r\ncommunications. The analysis of the data indicates that media\r\nconceptualizations of radicalization, which once denoted political and\r\neconomic differences, have now shifted to overwhelmingly focus on Islam.\r\nAs such, radicalization discourse now evokes the construct\r\nradicalization as symbolic marker of conflict between the West and the\r\nEast. I also advanced the established notion that the news media employ\r\nstrategic discursive strategies that contribute to conceptual\r\ndistinctions that are used to construct Muslims as an “alien other” to\r\nthe West.\r\nGottlieb (2015)\r\nGottlieb, J. (2015). Protest News Framing Cycle: How The New York\r\nTimes Covered Occupy Wall Street. International Journal of\r\nCommunication, 9(0), 23.\r\nAbstract\r\nThis article introduces a protest news framing cycle and presents the\r\nresults of a longitudinal analysis of news attention and framing of\r\nprotest movements. To identify the frame-changing dynamic occurring over\r\ntime, a content analysis of the news coverage of Occupy Wall Street was\r\nconducted on 228 articles and 37 editorials in The New York Times from\r\nthe start of the protest in September 2011 until long after the protest\r\nhad subsided in July 2014. The article identifies longitudinal changes\r\nin news frames about the economic substance of the protest and the\r\nensuing conflict between protesters and city officials during the\r\noccupation. Findings suggest that conflict had a significant impact on\r\nthe number of news stories about the protest. Further, the results\r\ndemonstrate how news framing opportunities changed as the movement\r\nreached different stages of the news attention cycle. As the movement\r\ngrew, journalists focused on the movement’s economic grievances,\r\nincluding economic inequality, bank bailouts, and foreclosures. As the\r\nmovement peaked, news attention shifted to the intensifying conflict\r\nbetween city officials and protesters.\r\nDiakopoulos (2015)\r\nDiakopoulos, N. A. (2015). The Editor’s Eye: Curation and Comment\r\nRelevance on the New York Times. Proceedings of the 18th ACM Conference\r\non Computer Supported Cooperative Work & Social Computing,\r\n1153–1157. https://doi.org/10.1145/2675133.2675160\r\nAbstract\r\nThe journalistic curation of social media content from platforms like\r\nFacebook and YouTube or from commenting systems is underscored by an\r\nimperative for publishing accurate and quality content. This work\r\nexplores the manifestation of editorial quality criteria in comments\r\nthat have been curated and selected on the New York Times website as\r\n“NYT Picks.” The relationship between comment selection and comment\r\nrelevance is examined through the analysis of 331,785 comments,\r\nincluding 12,542 editor’s selections. A robust association between\r\neditorial selection and article relevance or conversational relevance\r\nwas found. The results are discussed in terms of their implications for\r\nreducing journalistic curatorial work load, or scaling the ability to\r\nexamine more comments for editorial selection , as well as how end-user\r\ncommenting experiences might be improved.\r\nGrimmer & Stewart (2013)\r\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The\r\nPromise and Pitfalls of Automatic Content Analysis Methods for Political\r\nTexts. Political Analysis, 21(3), 267–297.\r\nAbstract\r\nPolitics and political conflict often occur in the written and spoken\r\nword. Scholars have long recognized this, but the massive costs of\r\nanalyzing even moderately sized collections of texts have hindered their\r\nuse in political science research. Here lies the promise of automated\r\ntext analysis: it substantially reduces the costs of analyzing large\r\ncollections of text. We provide a guide to this exciting new area of\r\nresearch and show how, in many instances, the methods have already\r\nobtained part of their promise. But there are pitfalls to using\r\nautomated methods–they are no substitute for careful thought and close\r\nreading and require extensive and problem-specific validation. We survey\r\na wide range of new methods, provide guidance on how to validate the\r\noutput of the models, and clarify misconceptions and errors in the\r\nliterature. To conclude, we argue that for automated text methods to\r\nbecome a standard tool for political scientists, methodologists must\r\ncontribute new methods and new methods of validation.\r\nKothari (2010)\r\nKothari, A. (2010). The Framing of the Darfur Conflict in the New\r\nYork Times: 2003–2006. Journalism Studies, 11(2), 209–224. https://doi.org/10.1080/14616700903481978\r\nAbstract\r\nThis multi-method study examines how the New York Times reported the\r\nDarfur conflict in the Sudan, which has led to an estimated 300,000\r\ndeaths and over 2.3 million people displaced by the fighting. Drawing on\r\nnormative media theories and prior studies of Africa’s representation,\r\nthe role of sources in the frame-building process was analyzed, together\r\nwith the impact of news-making processes on journalists’ reporting about\r\nDarfur. The textual analysis largely supports results of prior studies\r\non news framing of Africa. However, interviews with four New York Times\r\njournalists reveal that the individual biases and motives of the\r\njournalists and their sources significantly influenced the coverage.\r\nWhile the journalists participated in news-making processes\r\ndistinguishable by journalist goal, source availability, and source\r\ncredibility, their sources also provided information that reinforced\r\ncertain media frames.\r\nKiousis (2004)\r\nKiousis, S. (2004). Explicating Media Salience: A Factor Analysis\r\nof New York Times Issue Coverage During the 2000 U.S. Presidential\r\nElection. Journal of Communication, 54(1), 71–87. https://doi.org/10.1111/j.1460-2466.2004.tb02614.x\r\nAbstract\r\nMedia salience—the key independent variable in agenda-setting\r\nresearch—has traditionally been explicated as a singular construct.\r\nNevertheless, scholars have defined and measured it using a number of\r\ndifferent conceptualizations and empirical indicators. To address this\r\nlimitation in research, this study introduced a conceptual model of\r\nmedia salience, suggesting it is a multidimensional construct consisting\r\nof 3 core elements: attention, prominence, and valence. Furthermore, the\r\nmodel was tested through an exploratory factor analysis of The New York\r\nTimes news coverage of 8 major political issues during the 2000\r\npresidential election as a case study. The data revealed that 2\r\ndimensions of media salience emerge: visibility and valence. Based on\r\nthe factor analysis, 2 indices are created to measure the construct,\r\nwhich are intended for use in future investigations.\r\nAlthaus & Tewksbury (2002)\r\nAlthaus, S. L., & Tewksbury, D. (2002). Agenda Setting and\r\nthe “New” News: Patterns of Issue Importance Among Readers of the Paper\r\nand Online Versions of the New York Times. Communication Research,\r\n29(2), 180–207. https://doi.org/10.1177/0093650202029002004\r\nAbstract\r\nThis study examines whether readers of the paper and online versions\r\nof a national newspaper acquire different perceptions of the importance\r\nof political issues. Using data from a weeklong experiment in which\r\nsubjects either readthe print version of the New York Times, the online\r\nversion of that paper, or received no special exposure, this study finds\r\nevidence that people exposed to the Times for 5 days adjusted their\r\nagendas in response to that exposure and that print readers modified\r\ntheir agendas differently than did online readers.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-23T13:53:41-05:00",
    "input_file": "lit-review.knit.md"
  },
  {
    "path": "posts/welcome/",
    "title": "Analysis of New York Times Headlines",
    "description": "This is the project page for the analysis of differences between main and print headlines for New York Times articles published surrounding the U.S. withdrawal of the military in Afghanistan.",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/Grateful-Network/"
      }
    ],
    "date": "2022-04-13",
    "categories": [],
    "contents": "\r\n\r\nFor this project, I am using some data gathered in the DACSS 602\r\ncourse “Research Design”.\r\nI continued down the same path but with new data and a new direction\r\nthrough the DACSS 697D course “Text as Data”.\r\nMore background data can be found in this series of posts from my academic blog.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-23T00:25:35-05:00",
    "input_file": {}
  }
]
