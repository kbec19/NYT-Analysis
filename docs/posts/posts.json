[
  {
    "path": "posts/headline-expanded/",
    "title": "Analysis of Main vs. Print Headlines: Phase 2",
    "description": "Text as Data Project Headline Comparison Research Using API Query \"Afghanistan\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/NYT-Analysis/"
      }
    ],
    "date": "2022-04-26",
    "categories": [
      "text as data",
      "NYT text analysis project"
    ],
    "contents": "\r\n\r\nContents\r\nComparing\r\nMain vs. Print Headlines in the New York Times\r\nMaking\r\nDifferent Choices on Inclusion of Observations\r\nGathering Data\r\nPrevious Process\r\nLoad Data\r\nCreate Corpus\r\nAssign Type to Docvars\r\nTokenization\r\nDocument Feature Matrix\r\n\r\nDictionary Analysis\r\nliwcalike()\r\nNRC\r\nLSD 2015\r\nGeneral Inquirer\r\nCorrelations\r\nVisualization\r\n\r\nUsing Lexicons Differently\r\nBing Lexicon\r\nNRC\r\nAFINN\r\nComparing Results\r\n\r\n\r\nOther Types of Analysis\r\nUtilizing the cleanNLP\r\npackage:\r\nUsing spaCyr\r\nFor Main Headlines\r\nFor Print Headlines\r\n\r\n\r\n\r\nComparing\r\nMain vs. Print Headlines in the New York Times\r\nMaking\r\nDifferent Choices on Inclusion of Observations\r\nIn my initial look at the headline data, it was clear that not all of\r\nthe articles had different headlines; some are the same entries, and\r\nsome have “N/A” in the “print” version only, indicating they were\r\nonline-only stories. Although I initially felt inclined to leave the\r\n“N/A” observations in the analysis, I removed those observations as they\r\nwould not be relevant to my new research questions comparing the framing\r\nfor different audiences.\r\nI also removed whole sections where the API returned an observation\r\nas there was apparently use of the term “Afghanistan” somewhere in the\r\narticle/entry, but the type of entry was clearly not being represented\r\nin the headline. For example, “Corrections” entries have headlines\r\nconsisting only of the term “Corrections” and the corresponding date.\r\nSimilar choices were made on the “Arts”, Books”, and “Podcasts” sections\r\nwhen entries are primarily the names of the things being reviewed that\r\nmay have a reference to the Afghanistan withdrawal somewhere in the\r\ntext, but it is not relevant specifically to the withdrawal time period\r\nbeing analyzed.\r\nWith few exceptions, this left the entirety of the “U.S.” and “World”\r\nnews sections, even if the content related to Afghanistan is not readily\r\nobservable. The count (~650) matched the number of articles pulled for\r\nthe hand coding research as well.\r\nGathering Data\r\nPrevious Process\r\nThe data was pulled via API using the same process as in my first\r\nphase of the comparison research, with the only change in the query term\r\n“Afghanistan” as opposed to “Afghanistan Withdrawal”. This led to a\r\nlarger dataset for comparison, though most of the increase in count was\r\nfiltered out due to their classification as not news-related.\r\nLoad Data\r\nNow to the active review of the data. Loading the data from my\r\ncollection phase:\r\n\r\n  doc_id       date\r\n1      1  7/17/2020\r\n2      2  8/30/2020\r\n3      3   6/2/2021\r\n4      4 12/20/2020\r\n5      5  9/11/2021\r\n6      6   9/1/2021\r\n                                                                           text\r\n1  $174 Million Afghan Drone Program Is Riddled With Problems, U.S. Report Says\r\n2          ‘A Hail Mary’: Psychedelic Therapy Draws Veterans to Jungle Retreats\r\n3   ‘Come On In, Boys’: A Wave of the Hand Sets Off Spain-Morocco Migrant Fight\r\n4 ‘Covid Can’t Compete.’ In a Place Mired in War, the Virus Is an Afterthought.\r\n5    ‘Everything Changed Overnight’: Afghan Reporters Face an Intolerant Regime\r\n6      ‘Finally, I Am Safe’: U.S. Air Base Becomes Temporary Refuge for Afghans\r\n  doc_id       date\r\n1      1  7/17/2020\r\n2      2  8/30/2020\r\n3      3   6/2/2021\r\n4      4 12/20/2020\r\n5      5  9/11/2021\r\n6      6   9/1/2021\r\n                                                                            text\r\n1 $174 Million Drone Program for Afghans Is Riddled With Problems, Pentagon Says\r\n2                Psychedelic Therapy In the Jungle Soothes The Pain for Veterans\r\n3                                 Morocco Sends Spanish Outpost a Migrant Influx\r\n4         ‘It’s a Lie’: Denial and Skepticism Permeate a Nation Embroiled in War\r\n5                                     ‘Everything Changed’: Media Face Crackdown\r\n6         ‘Finally, I Am Safe’: Thousands Find Temporary Refuge at U.S. Air Base\r\n\r\nCreate Corpus\r\n\r\n\r\nmain_corpus <- corpus(main_headlines, docid_field = \"doc_id\", text_field = \"text\")\r\nprint_corpus <- corpus(print_headlines, docid_field = \"doc_id\", text_field = \"text\")\r\n\r\n\r\n\r\nAssign Type to Docvars\r\n\r\n\r\nmain_corpus$type <- \"Main Headline\"\r\nprint_corpus$type <- \"Print Headline\"\r\ndocvars(main_corpus, field = \"type\") <- main_corpus$type\r\ndocvars(print_corpus, field = \"type\") <- print_corpus$type\r\n\r\n\r\n\r\nTokenization\r\nI want to optimize pre-processing by removing the “�” symbol that has\r\nplagued me since starting working with this API by using\r\n“remove_symbols=TRUE” in addition to removing the punctuation when\r\ntokenizing. I also want to remove stopwords. I do NOT want to use\r\nstemming at this point.\r\nMain Headlines\r\n\r\n[1] 936\r\nTokens consisting of 936 documents and 2 docvars.\r\n1 :\r\n [1] \"174\"      \"Million\"  \"Afghan\"   \"Drone\"    \"Program\"  \"Riddled\" \r\n [7] \"Problems\" \"U.S\"      \"Report\"   \"Says\"    \r\n\r\n2 :\r\n[1] \"Hail\"        \"Mary\"        \"Psychedelic\" \"Therapy\"    \r\n[5] \"Draws\"       \"Veterans\"    \"Jungle\"      \"Retreats\"   \r\n\r\n3 :\r\n[1] \"Come\"          \"Boys\"          \"Wave\"          \"Hand\"         \r\n[5] \"Sets\"          \"Spain-Morocco\" \"Migrant\"       \"Fight\"        \r\n\r\n4 :\r\n[1] \"Covid\"        \"Compete\"      \"Place\"        \"Mired\"       \r\n[5] \"War\"          \"Virus\"        \"Afterthought\"\r\n\r\n5 :\r\n[1] \"Everything\" \"Changed\"    \"Overnight\"  \"Afghan\"     \"Reporters\" \r\n[6] \"Face\"       \"Intolerant\" \"Regime\"    \r\n\r\n6 :\r\n[1] \"Finally\"   \"Safe\"      \"U.S\"       \"Air\"       \"Base\"     \r\n[6] \"Becomes\"   \"Temporary\" \"Refuge\"    \"Afghans\"  \r\n\r\n[ reached max_ndoc ... 930 more documents ]\r\n\r\nPrint Headlines\r\n\r\n[1] 936\r\nTokens consisting of 936 documents and 2 docvars.\r\n1 :\r\n[1] \"174\"      \"Million\"  \"Drone\"    \"Program\"  \"Afghans\"  \"Riddled\" \r\n[7] \"Problems\" \"Pentagon\" \"Says\"    \r\n\r\n2 :\r\n[1] \"Psychedelic\" \"Therapy\"     \"Jungle\"      \"Soothes\"    \r\n[5] \"Pain\"        \"Veterans\"   \r\n\r\n3 :\r\n[1] \"Morocco\" \"Sends\"   \"Spanish\" \"Outpost\" \"Migrant\" \"Influx\" \r\n\r\n4 :\r\n[1] \"Lie\"        \"Denial\"     \"Skepticism\" \"Permeate\"   \"Nation\"    \r\n[6] \"Embroiled\"  \"War\"       \r\n\r\n5 :\r\n[1] \"Everything\" \"Changed\"    \"Media\"      \"Face\"       \"Crackdown\" \r\n\r\n6 :\r\n[1] \"Finally\"   \"Safe\"      \"Thousands\" \"Find\"      \"Temporary\"\r\n[6] \"Refuge\"    \"U.S\"       \"Air\"       \"Base\"     \r\n\r\n[ reached max_ndoc ... 930 more documents ]\r\n\r\nDocument Feature Matrix\r\n\r\n\r\n\r\n\r\n\r\n#create a word frequency variable and the rankings\r\nmain_counts <- as.data.frame(sort(colSums(main_dfm),dec=T))\r\ncolnames(main_counts) <- c(\"Frequency\")\r\nmain_counts$Rank <- c(1:ncol(main_dfm))\r\nhead(main_counts)\r\n\r\n\r\n            Frequency Rank\r\nu.s               166    1\r\nafghan            151    2\r\nafghanistan       135    3\r\ntaliban           110    4\r\nbiden              95    5\r\nwar                64    6\r\n\r\nprint_counts <- as.data.frame(sort(colSums(print_dfm),dec=T))\r\ncolnames(print_counts) <- c(\"Frequency\")\r\nprint_counts$Rank <- c(1:ncol(print_dfm))\r\nhead(print_counts)\r\n\r\n\r\n            Frequency Rank\r\nu.s               171    1\r\nafghan            109    2\r\ntaliban           108    3\r\nafghanistan        84    4\r\nbiden              76    5\r\nwar                53    6\r\n\r\nNow I can take a look at this network of feature co-occurrences for\r\nthe main headlines:\r\n\r\n[1] 2804 2804\r\n[1] 20 20\r\n\r\n\r\nand for the print headlines:\r\n\r\n[1] 2717 2717\r\n[1] 20 20\r\n\r\n\r\nDictionary Analysis\r\nliwcalike()\r\n\r\n [1] \"docname\"      \"Segment\"      \"WPS\"          \"WC\"          \r\n [5] \"Sixltr\"       \"Dic\"          \"anger\"        \"anticipation\"\r\n [9] \"disgust\"      \"fear\"         \"joy\"          \"negative\"    \r\n[13] \"positive\"     \"sadness\"      \"surprise\"     \"trust\"       \r\n[17] \"AllPunc\"      \"Period\"       \"Comma\"        \"Colon\"       \r\n[21] \"SemiC\"        \"QMark\"        \"Exclam\"       \"Dash\"        \r\n[25] \"Quote\"        \"Apostro\"      \"Parenth\"      \"OtherP\"      \r\n\r\nNRC\r\n\r\n\r\n# convert tokens from each headline data set to DFM using the dictionary \"NRC\"\r\nmain_nrc <- dfm(main_tokens) %>%\r\n  dfm_lookup(data_dictionary_NRC)\r\nprint_nrc <- dfm(print_tokens) %>%\r\n  dfm_lookup(data_dictionary_NRC)\r\n\r\ndim(main_nrc)\r\n\r\n\r\n[1] 936  10\r\n\r\nmain_nrc\r\n\r\n\r\nDocument-feature matrix of: 936 documents, 10 features (67.61% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            0       0    0   0        2        0       0\r\n   2     0            0       0    1   0        1        1       0\r\n   3     1            0       0    1   0        1        0       0\r\n   4     0            0       0    1   0        2        0       0\r\n   5     1            0       1    1   0        1        0       1\r\n   6     0            1       1    0   2        0        2       0\r\n    features\r\ndocs surprise trust\r\n   1        0     0\r\n   2        0     1\r\n   3        0     0\r\n   4        0     0\r\n   5        0     0\r\n   6        1     3\r\n[ reached max_ndoc ... 930 more documents ]\r\n\r\ndim(print_nrc)\r\n\r\n\r\n[1] 936  10\r\n\r\nprint_nrc\r\n\r\n\r\nDocument-feature matrix of: 936 documents, 10 features (69.21% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            0       0    0   0        2        0       0\r\n   2     0            0       0    2   0        1        0       1\r\n   3     0            0       0    1   0        0        0       0\r\n   4     1            0       1    1   0        4        0       1\r\n   5     0            0       0    0   0        0        0       0\r\n   6     0            1       1    0   2        0        2       0\r\n    features\r\ndocs surprise trust\r\n   1        0     0\r\n   2        0     0\r\n   3        0     0\r\n   4        0     1\r\n   5        0     0\r\n   6        1     3\r\n[ reached max_ndoc ... 930 more documents ]\r\n\r\nAnd use the information in a data frame to plot the output:\r\n\r\n\r\n#for the main headlines\r\ndf_main_nrc <- convert(main_nrc, to = \"data.frame\")\r\ndf_main_nrc$polarity <- (df_main_nrc$positive - df_main_nrc$negative)/(df_main_nrc$positive + df_main_nrc$negative)\r\ndf_main_nrc$polarity[which((df_main_nrc$positive + df_main_nrc$negative) == 0)] <- 0\r\n\r\nggplot(df_main_nrc) + \r\n  geom_histogram(aes(x=polarity)) + \r\n  theme_bw()\r\n\r\n\r\n\r\n#and the print headlines\r\ndf_print_nrc <- convert(print_nrc, to = \"data.frame\")\r\ndf_print_nrc$polarity <- (df_print_nrc$positive - df_print_nrc$negative)/(df_print_nrc$positive + df_print_nrc$negative)\r\ndf_print_nrc$polarity[which((df_print_nrc$positive + df_print_nrc$negative) == 0)] <- 0\r\n\r\nggplot(df_print_nrc) + \r\n  geom_histogram(aes(x=polarity)) + \r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nLooking at the headlines that are indicated as “1”, or positive in\r\nsentiment, it’s clear that this dictionary is not capturing the\r\nsentiment accurately.\r\n\r\n\r\nhead(main_corpus[which(df_main_nrc$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n6 :\r\n\"‘Finally, I Am Safe’: U.S. Air Base Becomes Temporary Refuge...\"\r\n\r\n8 :\r\n\"‘Football Is Like Food’: Afghan Female Soccer Players Find a...\"\r\n\r\n18 :\r\n\"‘Is Austin on Your List?’: Biden’s Pentagon Pick Rose Despit...\"\r\n\r\n29 :\r\n\"‘We Have to Try’: Lawmakers Rush to Assist in Afghanistan Ev...\"\r\n\r\n40 :\r\n\"4 Takeaways From the U.S. Deal With the Taliban\"\r\n\r\n43 :\r\n\"98 Countries Pledge to Accept Afghans After U.S. Military De...\"\r\n\r\nhead(print_corpus[which(df_print_nrc$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n6 :\r\n\"‘Finally, I Am Safe’: Thousands Find Temporary Refuge at U.S...\"\r\n\r\n15 :\r\n\"Veterans Feel Urgency to Aid Afghan Allies\"\r\n\r\n18 :\r\n\"How Biden’s Defense Nominee Overcame Barriers to Diversity\"\r\n\r\n25 :\r\n\"How Biden, by Turns Genial and Blunt, Built Diplomatic Bridg...\"\r\n\r\n45 :\r\n\"Rescue Flight To Germany Inspires Name For Newborn\"\r\n\r\n50 :\r\n\"A Call for the Return of Civility, And Truth as a Guiding Li...\"\r\n\r\nLSD 2015\r\nI am going to want to look at multiple dictionaries to see if one can\r\nbest apply to this data. First, the LSD 2015 dictionary:\r\n\r\n\r\n# convert main corpus to DFM using the LSD2015 dictionary\r\nmain_lsd2015 <- dfm(tokens(main_corpus, remove_punct = TRUE),\r\n                              tolower = TRUE) %>%\r\n                          dfm_lookup(data_dictionary_LSD2015)\r\n# create main polarity measure for LSD2015\r\nmain_lsd2015 <- convert(main_lsd2015, to = \"data.frame\")\r\nmain_lsd2015$polarity <- (main_lsd2015$positive - main_lsd2015$negative)/(main_lsd2015$positive + main_lsd2015$negative)\r\nmain_lsd2015$polarity[which((main_lsd2015$positive + main_lsd2015$negative) == 0)] <- 0\r\n# convert print corpus to DFM using the LSD2015 dictionary\r\nprint_lsd2015 <- dfm(tokens(print_corpus, remove_punct = TRUE),\r\n                              tolower = TRUE) %>%\r\n                          dfm_lookup(data_dictionary_LSD2015)\r\n# create print polarity measure for LSD2015\r\nprint_lsd2015 <- convert(print_lsd2015, to = \"data.frame\")\r\nprint_lsd2015$polarity <- (print_lsd2015$positive - print_lsd2015$negative)/(print_lsd2015$positive + print_lsd2015$negative)\r\nprint_lsd2015$polarity[which((print_lsd2015$positive + print_lsd2015$negative) == 0)] <- 0\r\n\r\n\r\n\r\nGeneral Inquirer\r\nand the General Inquirer dictionary:\r\n\r\n\r\n# convert main corpus to DFM using the General Inquirer dictionary\r\nmain_geninq <- dfm(tokens(main_corpus, remove_punct = TRUE),\r\n                             tolower = TRUE) %>%\r\n                    dfm_lookup(data_dictionary_geninqposneg)\r\n# create main polarity measure for GenInq\r\nmain_geninq <- convert(main_geninq, to = \"data.frame\")\r\nmain_geninq$polarity <- (main_geninq$positive - main_geninq$negative)/(main_geninq$positive + main_geninq$negative)\r\nmain_geninq$polarity[which((main_geninq$positive + main_geninq$negative) == 0)] <- 0\r\n# convert print corpus to DFM using the General Inquirer dictionary\r\nprint_geninq <- dfm(tokens(print_corpus, remove_punct = TRUE),\r\n                             tolower = TRUE) %>%\r\n                    dfm_lookup(data_dictionary_geninqposneg)\r\n# create print polarity measure for GenInq\r\nprint_geninq <- convert(print_geninq, to = \"data.frame\")\r\nprint_geninq$polarity <- (print_geninq$positive - print_geninq$negative)/(print_geninq$positive + print_geninq $negative)\r\nprint_geninq$polarity[which((print_geninq$positive + print_geninq$negative) == 0)] <- 0\r\n\r\n\r\n\r\nNow I’m going to be able to compare the different dictionary scores\r\nin one data frame for each type of headline.\r\n\r\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy\r\n1          1         0                0           0        0       0\r\n2         10         0                0           0        2       0\r\n3        100         0                0           0        1       0\r\n4        101         0                2           0        0       1\r\n5        102         0                1           0        0       1\r\n6        103         1                1           1        1       1\r\n  nrc_negative nrc_positive nrc_sadness nrc_surprise nrc_trust\r\n1            2            0           0            0         0\r\n2            1            0           1            0         1\r\n3            1            0           0            0         0\r\n4            0            1           0            0         1\r\n5            0            1           0            0         1\r\n6            1            1           1            0         1\r\n  nrc_polarity lsd2015_negative lsd2015_positive lsd2015_neg_positive\r\n1           -1                1                0                    0\r\n2           -1                1                0                    0\r\n3           -1                2                0                    0\r\n4            1                1                1                    0\r\n5            1                0                1                    0\r\n6            0                1                1                    0\r\n  lsd2015_neg_negative lsd2015_polarity geninq_positive\r\n1                    0               -1               0\r\n2                    0               -1               0\r\n3                    0               -1               0\r\n4                    0                0               1\r\n5                    0                1               2\r\n6                    0                0               1\r\n  geninq_negative geninq_polarity\r\n1               0               0\r\n2               1              -1\r\n3               1              -1\r\n4               0               1\r\n5               0               1\r\n6               1               0\r\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy\r\n1          1         0                0           0        0       0\r\n2         10         0                0           0        2       0\r\n3        100         0                0           0        1       0\r\n4        101         0                2           0        0       2\r\n5        102         0                1           0        0       1\r\n6        103         1                0           1        1       0\r\n  nrc_negative nrc_positive nrc_sadness nrc_surprise nrc_trust\r\n1            2            0           0            0         0\r\n2            2            0           1            0         0\r\n3            1            0           0            0         0\r\n4            1            2           0            1         2\r\n5            0            1           0            0         1\r\n6            1            0           1            0         0\r\n  nrc_polarity lsd2015_negative lsd2015_positive lsd2015_neg_positive\r\n1   -1.0000000                1                0                    0\r\n2   -1.0000000                2                0                    0\r\n3   -1.0000000                3                0                    0\r\n4    0.3333333                2                1                    0\r\n5    1.0000000                0                1                    0\r\n6   -1.0000000                1                0                    0\r\n  lsd2015_neg_negative lsd2015_polarity geninq_positive\r\n1                    0       -1.0000000               0\r\n2                    0       -1.0000000               0\r\n3                    0       -1.0000000               0\r\n4                    0       -0.3333333               2\r\n5                    0        1.0000000               1\r\n6                    0       -1.0000000               0\r\n  geninq_negative geninq_polarity\r\n1               0       0.0000000\r\n2               1      -1.0000000\r\n3               1      -1.0000000\r\n4               1       0.3333333\r\n5               0       1.0000000\r\n6               1      -1.0000000\r\n\r\nCorrelations\r\nNow that we have them all in a single data frame, it’s\r\nstraightforward to figure out a bit about how well our different\r\nmeasures of polarity agree across the different approaches by looking at\r\ntheir correlation using the “cor()” function.\r\n\r\n\r\ncor(main_sent$nrc_polarity, main_sent$lsd2015_polarity)\r\n\r\n\r\n[1] 0.4547764\r\n\r\ncor(main_sent$nrc_polarity, main_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4633912\r\n\r\ncor(main_sent$lsd2015_polarity, main_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.5265193\r\n\r\n\r\n\r\ncor(print_sent$nrc_polarity, print_sent$lsd2015_polarity)\r\n\r\n\r\n[1] 0.4651277\r\n\r\ncor(print_sent$nrc_polarity, print_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4747134\r\n\r\ncor(print_sent$lsd2015_polarity, print_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4985192\r\n\r\nVisualization\r\n\r\n\r\nset.seed(11)\r\n# draw the wordcloud\r\nlibrary(wordcloud)\r\n\r\nsimplified_tokens <- read.csv(\"token.csv\")\r\n\r\nsimplified_corpus <- corpus(simplified_tokens, text_field = \"word\")\r\n\r\npar(mfrow=c(1,1)) # 1 panel plot\r\npar(mar=c(1, 3, 1, 3)) # Set the plot margin\r\npar(bg=\"black\") # set background color as black\r\npar(col.main=\"white\") # set title color as white\r\nwordcloud(simplified_corpus, scale=c(4,.5),min.freq=5, max.words=Inf, random.order=F, random.color=F, \r\n          colors = brewer.pal(8, \"Set3\"))   \r\ntitle(\"Main Website Headlines\")\r\n\r\n\r\n\r\n\r\nUsing Lexicons Differently\r\nBing Lexicon\r\n\r\n\r\n#create tokens without stop words for main headlines\r\ntkn_l_main <- apply(main_headlines, 1, function(x) { data.frame(text=x, stringsAsFactors = FALSE) %>% unnest_tokens(word, text)})\r\nmain_news_tokens <- lapply(tkn_l_main, function(x) {anti_join(x, stop_words)})\r\nstr(main_news_tokens, list.len = 5)\r\n\r\n\r\nList of 936\r\n $ :'data.frame':   12 obs. of  1 variable:\r\n  ..$ word: chr [1:12] \"1\" \"7\" \"17\" \"2020\" ...\r\n $ :'data.frame':   12 obs. of  1 variable:\r\n  ..$ word: chr [1:12] \"2\" \"8\" \"30\" \"2020\" ...\r\n $ :'data.frame':   12 obs. of  1 variable:\r\n  ..$ word: chr [1:12] \"3\" \"6\" \"2\" \"2021\" ...\r\n $ :'data.frame':   11 obs. of  1 variable:\r\n  ..$ word: chr [1:11] \"4\" \"12\" \"20\" \"2020\" ...\r\n $ :'data.frame':   10 obs. of  1 variable:\r\n  ..$ word: chr [1:10] \"5\" \"9\" \"11\" \"2021\" ...\r\n  [list output truncated]\r\n\r\nmain_news_tokens[[1]]\r\n\r\n\r\n             word\r\ndoc_id          1\r\ndate...2        7\r\ndate...3       17\r\ndate...4     2020\r\ntext...5      174\r\ntext...6  million\r\ntext...7   afghan\r\ntext...8    drone\r\ntext...9  program\r\ntext...10 riddled\r\ntext...11     u.s\r\ntext...12  report\r\n\r\n#create tokens without stop words for print headlines\r\ntkn_l_print <- apply(print_headlines, 1, function(x) { data.frame(text=x, stringsAsFactors = FALSE) %>% unnest_tokens(word, text)})\r\nprint_news_tokens <- lapply(tkn_l_print, function(x) {anti_join(x, stop_words)})\r\nstr(print_news_tokens, list.len = 5)\r\n\r\n\r\nList of 936\r\n $ :'data.frame':   11 obs. of  1 variable:\r\n  ..$ word: chr [1:11] \"1\" \"7\" \"17\" \"2020\" ...\r\n $ :'data.frame':   10 obs. of  1 variable:\r\n  ..$ word: chr [1:10] \"2\" \"8\" \"30\" \"2020\" ...\r\n $ :'data.frame':   10 obs. of  1 variable:\r\n  ..$ word: chr [1:10] \"3\" \"6\" \"2\" \"2021\" ...\r\n $ :'data.frame':   12 obs. of  1 variable:\r\n  ..$ word: chr [1:12] \"4\" \"12\" \"20\" \"2020\" ...\r\n $ :'data.frame':   7 obs. of  1 variable:\r\n  ..$ word: chr [1:7] \"5\" \"9\" \"11\" \"2021\" ...\r\n  [list output truncated]\r\n\r\nprint_news_tokens[[1]]\r\n\r\n\r\n              word\r\ndoc_id           1\r\ndate...2         7\r\ndate...3        17\r\ndate...4      2020\r\ntext...5       174\r\ntext...6   million\r\ntext...7     drone\r\ntext...8   program\r\ntext...9   afghans\r\ntext...10  riddled\r\ntext...11 pentagon\r\n\r\n\r\n\r\ncompute_sentiment <- function(d) {\r\n  if (nrow(d) == 0) {\r\n    return(NA)\r\n  }\r\n  neg_score <- d %>% filter(sentiment==\"negative\") %>% nrow()\r\n  pos_score <- d %>% filter(sentiment==\"positive\") %>% nrow()\r\n  pos_score - neg_score\r\n} \r\n\r\n\r\n\r\n\r\n\r\nsentiments_bing <- get_sentiments(\"bing\")\r\nstr(sentiments_bing)\r\n\r\n\r\ntibble [6,786 x 2] (S3: tbl_df/tbl/data.frame)\r\n $ word     : chr [1:6786] \"2-faces\" \"abnormal\" \"abolish\" \"abominable\" ...\r\n $ sentiment: chr [1:6786] \"negative\" \"negative\" \"negative\" \"negative\" ...\r\n\r\n#apply sentiment to main headlines\r\nmain_news_sentiment_bing <- sapply(main_news_tokens, function(x) { x %>% inner_join(sentiments_bing) %>% compute_sentiment()})\r\n#apply sentiment to print headlines\r\nprint_news_sentiment_bing <- sapply(print_news_tokens, function(x) { x %>% inner_join(sentiments_bing) %>% compute_sentiment()})\r\n\r\n\r\n\r\n\r\n\r\nstr(main_news_sentiment_bing)\r\n\r\n\r\n int [1:936] NA 1 NA -1 NA 1 -2 NA 1 NA ...\r\n\r\nstr(print_news_sentiment_bing)\r\n\r\n\r\n int [1:936] NA -1 NA -4 NA 1 -2 -1 NA -1 ...\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \r\n-4.0000 -1.0000 -1.0000 -0.5945  1.0000  2.0000     349 \r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \r\n-4.0000 -1.0000 -1.0000 -0.6209  0.0000  3.0000     353 \r\n\r\n\r\n                                                                       main_text\r\n1   $174 Million Afghan Drone Program Is Riddled With Problems, U.S. Report Says\r\n2           ‘A Hail Mary’: Psychedelic Therapy Draws Veterans to Jungle Retreats\r\n3    ‘Come On In, Boys’: A Wave of the Hand Sets Off Spain-Morocco Migrant Fight\r\n4  ‘Covid Can’t Compete.’ In a Place Mired in War, the Virus Is an Afterthought.\r\n5     ‘Everything Changed Overnight’: Afghan Reporters Face an Intolerant Regime\r\n6       ‘Finally, I Am Safe’: U.S. Air Base Becomes Temporary Refuge for Afghans\r\n7                    ‘Find Him and Kill Him’: An Afghan Pilot’s Desperate Escape\r\n8     ‘Football Is Like Food’: Afghan Female Soccer Players Find a Home in Italy\r\n9    ‘Go Big’ on Coronavirus Stimulus, Trump Says, Pitching Checks for Americans\r\n10            ‘Hospital Needs to Be Quarantined,’ but Works On in Country at War\r\n   score\r\n1     NA\r\n2      1\r\n3     NA\r\n4     -1\r\n5     NA\r\n6      1\r\n7     -2\r\n8     NA\r\n9      1\r\n10    NA\r\n                                                                       print_text\r\n1  $174 Million Drone Program for Afghans Is Riddled With Problems, Pentagon Says\r\n2                 Psychedelic Therapy In the Jungle Soothes The Pain for Veterans\r\n3                                  Morocco Sends Spanish Outpost a Migrant Influx\r\n4          ‘It’s a Lie’: Denial and Skepticism Permeate a Nation Embroiled in War\r\n5                                      ‘Everything Changed’: Media Face Crackdown\r\n6          ‘Finally, I Am Safe’: Thousands Find Temporary Refuge at U.S. Air Base\r\n7                  ‘Find Him and Kill Him’: A Pilot’s Desperate Escape From Kabul\r\n8                                     Soccer Players Under Threat Escape to Italy\r\n9                                    Plan Would Inject $1 Trillion Into Economy  \r\n10  As Pandemic Takes Toll on Afghan Doctors, Hospitals Still Tend to War Wounded\r\n   score\r\n1     NA\r\n2     -1\r\n3     NA\r\n4     -4\r\n5     NA\r\n6      1\r\n7     -2\r\n8     -1\r\n9     NA\r\n10    -1\r\n\r\nNRC\r\n\r\n\r\nsentiments_nrc <- get_sentiments(\"nrc\")\r\n(unique_sentiments_nrc <- unique(sentiments_nrc$sentiment))\r\n\r\n\r\n [1] \"trust\"        \"fear\"         \"negative\"     \"sadness\"     \r\n [5] \"anger\"        \"surprise\"     \"positive\"     \"disgust\"     \r\n [9] \"joy\"          \"anticipation\"\r\n\r\n\r\n\r\ncompute_pos_neg_sentiments_nrc <- function(the_sentiments_nrc) {\r\n  s <- unique(the_sentiments_nrc$sentiment)\r\n  df_sentiments <- data.frame(sentiment = s, \r\n                              mapped_sentiment = c(\"positive\", \"negative\", \"negative\", \"negative\",\r\n                                                    \"negative\", \"positive\", \"positive\", \"negative\", \r\n                                                    \"positive\", \"positive\"))\r\n  ss <- sentiments_nrc %>% inner_join(df_sentiments)\r\n  the_sentiments_nrc$sentiment <- ss$mapped_sentiment\r\n  the_sentiments_nrc\r\n}\r\n\r\nnrc_sentiments_pos_neg_scale <- compute_pos_neg_sentiments_nrc(sentiments_nrc)\r\n\r\n\r\n\r\n\r\n\r\n#calculating NRC sentiment for main headlines\r\nmain_news_sentiment_nrc <- sapply(main_news_tokens, function(x) { x %>% inner_join(nrc_sentiments_pos_neg_scale) %>% compute_sentiment()})\r\nstr(main_news_sentiment_nrc)\r\n\r\n\r\n int [1:936] -2 0 -3 -3 -5 8 -4 6 1 -3 ...\r\n\r\n#calculating NRC sentiment for print headlines\r\nprint_news_sentiment_nrc <- sapply(print_news_tokens, function(x) { x %>% inner_join(nrc_sentiments_pos_neg_scale) %>% compute_sentiment()})\r\nstr(print_news_sentiment_nrc)\r\n\r\n\r\n int [1:936] -2 -4 -1 -7 NA 8 -4 -3 2 -5 ...\r\n\r\n\r\n\r\n#data frame of main NRC sentiment\r\nsummary(main_news_sentiment_nrc)\r\n\r\n\r\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \r\n-12.0000  -3.0000  -1.0000  -0.7417   1.0000  13.0000      150 \r\n\r\nmain_news_sentiment_nrc_df <- data.frame(main_text=main_headlines$text, score = main_news_sentiment_nrc)\r\nhead(main_news_sentiment_nrc_df, 10)\r\n\r\n\r\n                                                                       main_text\r\n1   $174 Million Afghan Drone Program Is Riddled With Problems, U.S. Report Says\r\n2           ‘A Hail Mary’: Psychedelic Therapy Draws Veterans to Jungle Retreats\r\n3    ‘Come On In, Boys’: A Wave of the Hand Sets Off Spain-Morocco Migrant Fight\r\n4  ‘Covid Can’t Compete.’ In a Place Mired in War, the Virus Is an Afterthought.\r\n5     ‘Everything Changed Overnight’: Afghan Reporters Face an Intolerant Regime\r\n6       ‘Finally, I Am Safe’: U.S. Air Base Becomes Temporary Refuge for Afghans\r\n7                    ‘Find Him and Kill Him’: An Afghan Pilot’s Desperate Escape\r\n8     ‘Football Is Like Food’: Afghan Female Soccer Players Find a Home in Italy\r\n9    ‘Go Big’ on Coronavirus Stimulus, Trump Says, Pitching Checks for Americans\r\n10            ‘Hospital Needs to Be Quarantined,’ but Works On in Country at War\r\n   score\r\n1     -2\r\n2      0\r\n3     -3\r\n4     -3\r\n5     -5\r\n6      8\r\n7     -4\r\n8      6\r\n9      1\r\n10    -3\r\n\r\n#data frame of print NRC sentiment\r\nsummary(print_news_sentiment_nrc)\r\n\r\n\r\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \r\n-12.0000  -3.0000  -1.0000  -0.4994   2.0000  13.0000      151 \r\n\r\nprint_news_sentiment_nrc_df <- data.frame(print_text=print_headlines$text, score = print_news_sentiment_nrc)\r\nhead(print_news_sentiment_nrc_df, 10)\r\n\r\n\r\n                                                                       print_text\r\n1  $174 Million Drone Program for Afghans Is Riddled With Problems, Pentagon Says\r\n2                 Psychedelic Therapy In the Jungle Soothes The Pain for Veterans\r\n3                                  Morocco Sends Spanish Outpost a Migrant Influx\r\n4          ‘It’s a Lie’: Denial and Skepticism Permeate a Nation Embroiled in War\r\n5                                      ‘Everything Changed’: Media Face Crackdown\r\n6          ‘Finally, I Am Safe’: Thousands Find Temporary Refuge at U.S. Air Base\r\n7                  ‘Find Him and Kill Him’: A Pilot’s Desperate Escape From Kabul\r\n8                                     Soccer Players Under Threat Escape to Italy\r\n9                                    Plan Would Inject $1 Trillion Into Economy  \r\n10  As Pandemic Takes Toll on Afghan Doctors, Hospitals Still Tend to War Wounded\r\n   score\r\n1     -2\r\n2     -4\r\n3     -1\r\n4     -7\r\n5     NA\r\n6      8\r\n7     -4\r\n8     -3\r\n9      2\r\n10    -5\r\n\r\nAFINN\r\n\r\n\r\nsentiments_afinn <- get_sentiments(\"afinn\")\r\n\r\ncolnames(sentiments_afinn) <- c(\"word\", \"sentiment\")\r\nstr(sentiments_afinn)\r\n\r\n\r\nspec_tbl_df [2,477 x 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\r\n $ word     : chr [1:2477] \"abandon\" \"abandoned\" \"abandons\" \"abducted\" ...\r\n $ sentiment: num [1:2477] -2 -2 -2 -2 -2 -2 -3 -3 -3 -3 ...\r\n - attr(*, \"spec\")=\r\n  .. cols(\r\n  ..   word = col_character(),\r\n  ..   value = col_double()\r\n  .. )\r\n\r\n\r\n\r\n#applying AFINN sentiment to main headlines\r\nmain_news_sentiment_afinn_df <- lapply(main_news_tokens, function(x) { x %>% inner_join(sentiments_afinn)})\r\nmain_news_sentiment_afinn <- sapply(main_news_sentiment_afinn_df, function(x) { \r\n      ifelse(nrow(x) > 0, sum(x$sentiment), NA)\r\n  })\r\nstr(main_news_sentiment_afinn)\r\n\r\n\r\n num [1:936] NA 2 -1 -2 NA 1 -7 NA NA -2 ...\r\n\r\n#applying AFINN sentiment to print headlines\r\nprint_news_sentiment_afinn_df <- lapply(print_news_tokens, function(x) { x %>% inner_join(sentiments_afinn)})\r\nprint_news_sentiment_afinn <- sapply(print_news_sentiment_afinn_df, function(x) { \r\n      ifelse(nrow(x) > 0, sum(x$sentiment), NA)\r\n  })\r\nstr(print_news_sentiment_afinn)\r\n\r\n\r\n num [1:936] NA -2 NA -4 NA 1 -7 -3 NA -2 ...\r\n\r\n\r\n\r\n#data frame of AFINN main headlines\r\nsummary(main_news_sentiment_afinn)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \r\n-10.000  -3.000  -2.000  -1.769  -1.000   5.000     368 \r\n\r\nmain_news_sentiment_afinn_df <- data.frame(main_text=main_headlines$text, score = main_news_sentiment_afinn)\r\nhead(main_news_sentiment_afinn_df, 10)\r\n\r\n\r\n                                                                       main_text\r\n1   $174 Million Afghan Drone Program Is Riddled With Problems, U.S. Report Says\r\n2           ‘A Hail Mary’: Psychedelic Therapy Draws Veterans to Jungle Retreats\r\n3    ‘Come On In, Boys’: A Wave of the Hand Sets Off Spain-Morocco Migrant Fight\r\n4  ‘Covid Can’t Compete.’ In a Place Mired in War, the Virus Is an Afterthought.\r\n5     ‘Everything Changed Overnight’: Afghan Reporters Face an Intolerant Regime\r\n6       ‘Finally, I Am Safe’: U.S. Air Base Becomes Temporary Refuge for Afghans\r\n7                    ‘Find Him and Kill Him’: An Afghan Pilot’s Desperate Escape\r\n8     ‘Football Is Like Food’: Afghan Female Soccer Players Find a Home in Italy\r\n9    ‘Go Big’ on Coronavirus Stimulus, Trump Says, Pitching Checks for Americans\r\n10            ‘Hospital Needs to Be Quarantined,’ but Works On in Country at War\r\n   score\r\n1     NA\r\n2      2\r\n3     -1\r\n4     -2\r\n5     NA\r\n6      1\r\n7     -7\r\n8     NA\r\n9     NA\r\n10    -2\r\n\r\n#data frame of AFINN print headlines\r\nsummary(print_news_sentiment_afinn)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \r\n -10.00   -3.00   -2.00   -1.52   -1.00    6.00     359 \r\n\r\nprint_news_sentiment_afinn_df <- data.frame(print_text=print_headlines$text, score = print_news_sentiment_afinn)\r\nhead(print_news_sentiment_afinn_df, 10)\r\n\r\n\r\n                                                                       print_text\r\n1  $174 Million Drone Program for Afghans Is Riddled With Problems, Pentagon Says\r\n2                 Psychedelic Therapy In the Jungle Soothes The Pain for Veterans\r\n3                                  Morocco Sends Spanish Outpost a Migrant Influx\r\n4          ‘It’s a Lie’: Denial and Skepticism Permeate a Nation Embroiled in War\r\n5                                      ‘Everything Changed’: Media Face Crackdown\r\n6          ‘Finally, I Am Safe’: Thousands Find Temporary Refuge at U.S. Air Base\r\n7                  ‘Find Him and Kill Him’: A Pilot’s Desperate Escape From Kabul\r\n8                                     Soccer Players Under Threat Escape to Italy\r\n9                                    Plan Would Inject $1 Trillion Into Economy  \r\n10  As Pandemic Takes Toll on Afghan Doctors, Hospitals Still Tend to War Wounded\r\n   score\r\n1     NA\r\n2     -2\r\n3     NA\r\n4     -4\r\n5     NA\r\n6      1\r\n7     -7\r\n8     -3\r\n9     NA\r\n10    -2\r\n\r\nComparing Results\r\nHaving obtained for each news three potential results as sentiment\r\nevaluation, we would like to compare their congruency. As congruence we\r\nmean the fact that all three lexicons express the same positive or\r\nnegative result, in other words the same score sign indipendently from\r\nits magnitude. If NA values are present, the congruence shall be\r\ncomputed until at least two non NA values are available, otherwise is\r\nequal to NA.\r\nFurthermore we compute the final news sentiment as based upon the sum\r\nof each lexicon sentiment score.\r\n\r\n\r\ncompute_congruence <- function(x,y,z) {\r\n  v <- c(sign(x), sign(y), sign(z))\r\n  # if only one lexicon reports the score, we cannot check for congruence\r\n  if (sum(is.na(v)) >= 2) {\r\n    return (NA)\r\n  }\r\n  # removing NA and zero value\r\n  v <- na.omit(v)\r\n  v_sum <- sum(v)\r\n  abs(v_sum) == length(v)\r\n}\r\n\r\n\r\n\r\n\r\n\r\ncompute_final_sentiment <- function(x,y,z) {\r\n  if (is.na(x) && is.na(y) && is.na(z)) {\r\n    return (NA)\r\n  }\r\n\r\n  s <- sum(x, y, z, na.rm=TRUE)\r\n  # positive sentiments have score strictly greater than zero\r\n  # negative sentiments have score strictly less than zero\r\n  # neutral sentiments have score equal to zero \r\n  ifelse(s > 0, \"positive\", ifelse(s < 0, \"negative\", \"neutral\"))\r\n}\r\n\r\n\r\n\r\n\r\n\r\nall_sentiments_results <- data.frame(main_text = main_headlines$text, \r\n                                 main_bing = main_news_sentiment_bing, \r\n                                 main_nrc = main_news_sentiment_nrc, \r\n                                 main_afinn = main_news_sentiment_afinn,\r\n                                 print_text = print_headlines$text, \r\n                                 print_bing = print_news_sentiment_bing, \r\n                                 print_nrc = print_news_sentiment_nrc, \r\n                                 print_afinn = print_news_sentiment_afinn,\r\n                                 stringsAsFactors = FALSE)\r\n\r\nmain_sentiments_results <- data.frame(main_text = main_headlines$text, \r\n                                 bing_score = main_news_sentiment_bing, \r\n                                 nrc_score = main_news_sentiment_nrc, \r\n                                 afinn_score = main_news_sentiment_afinn,\r\n                                 stringsAsFactors = FALSE)\r\n\r\nprint_sentiments_results <- data.frame(print_text = print_headlines$text, \r\n                                 bing_score = print_news_sentiment_bing, \r\n                                 nrc_score = print_news_sentiment_nrc, \r\n                                 afinn_score = print_news_sentiment_afinn,\r\n                                 stringsAsFactors = FALSE)\r\n\r\n\r\nmain_sentiments_results <- main_sentiments_results %>% rowwise() %>% \r\n  mutate(final_sentiment = compute_final_sentiment(bing_score, nrc_score, afinn_score),\r\n         congruence = compute_congruence(bing_score, nrc_score, afinn_score))\r\n\r\nprint_sentiments_results <- print_sentiments_results %>% rowwise() %>% \r\n  mutate(final_sentiment = compute_final_sentiment(bing_score, nrc_score, afinn_score),\r\n         congruence = compute_congruence(bing_score, nrc_score, afinn_score))\r\n\r\nhead(main_sentiments_results, 10)\r\n\r\n\r\n# A tibble: 10 x 6\r\n# Rowwise: \r\n   main_text          bing_score nrc_score afinn_score final_sentiment\r\n   <chr>                   <int>     <int>       <dbl> <chr>          \r\n 1 $174 Million Afgh~         NA        -2          NA negative       \r\n 2 ‘A Hail Mary’: Ps~          1         0           2 positive       \r\n 3 ‘Come On In, Boys~         NA        -3          -1 negative       \r\n 4 ‘Covid Can’t Comp~         -1        -3          -2 negative       \r\n 5 ‘Everything Chang~         NA        -5          NA negative       \r\n 6 ‘Finally, I Am Sa~          1         8           1 positive       \r\n 7 ‘Find Him and Kil~         -2        -4          -7 negative       \r\n 8 ‘Football Is Like~         NA         6          NA positive       \r\n 9 ‘Go Big’ on Coron~          1         1          NA positive       \r\n10 ‘Hospital Needs t~         NA        -3          -2 negative       \r\n# ... with 1 more variable: congruence <lgl>\r\n\r\nhead(print_sentiments_results, 10)\r\n\r\n\r\n# A tibble: 10 x 6\r\n# Rowwise: \r\n   print_text         bing_score nrc_score afinn_score final_sentiment\r\n   <chr>                   <int>     <int>       <dbl> <chr>          \r\n 1 \"$174 Million Dro~         NA        -2          NA negative       \r\n 2 \"Psychedelic Ther~         -1        -4          -2 negative       \r\n 3 \"Morocco Sends Sp~         NA        -1          NA negative       \r\n 4 \"‘It’s a Lie’: De~         -4        -7          -4 negative       \r\n 5 \"‘Everything Chan~         NA        NA          NA <NA>           \r\n 6 \"‘Finally, I Am S~          1         8           1 positive       \r\n 7 \"‘Find Him and Ki~         -2        -4          -7 negative       \r\n 8 \"Soccer Players U~         -1        -3          -3 negative       \r\n 9 \"Plan Would Injec~         NA         2          NA positive       \r\n10 \"As Pandemic Take~         -1        -5          -2 negative       \r\n# ... with 1 more variable: congruence <lgl>\r\n\r\n\r\n\r\n#If it would be useful to replace the numeric score with same {negative, neutral, positive} scale.\r\nreplace_score_with_sentiment <- function(v_score) {\r\n  v_score[v_score > 0] <- \"positive\"\r\n  v_score[v_score < 0] <- \"negative\"\r\n  v_score[v_score == 0] <- \"neutral\"\r\n  v_score\r\n} \r\n#apply scale to main results\r\nmain_sentiments_results$bing_score <- replace_score_with_sentiment(main_sentiments_results$bing_score)\r\nmain_sentiments_results$nrc_score <- replace_score_with_sentiment(main_sentiments_results$nrc_score)\r\nmain_sentiments_results$afinn_score <- replace_score_with_sentiment(main_sentiments_results$afinn_score)\r\nmain_sentiments_results[,2:5] <- lapply(main_sentiments_results[,2:5], as.factor)\r\nhead(main_sentiments_results, 40)\r\n\r\n\r\n# A tibble: 40 x 6\r\n# Rowwise: \r\n   main_text          bing_score nrc_score afinn_score final_sentiment\r\n   <chr>              <fct>      <fct>     <fct>       <fct>          \r\n 1 $174 Million Afgh~ <NA>       negative  <NA>        negative       \r\n 2 ‘A Hail Mary’: Ps~ positive   neutral   positive    positive       \r\n 3 ‘Come On In, Boys~ <NA>       negative  negative    negative       \r\n 4 ‘Covid Can’t Comp~ negative   negative  negative    negative       \r\n 5 ‘Everything Chang~ <NA>       negative  <NA>        negative       \r\n 6 ‘Finally, I Am Sa~ positive   positive  positive    positive       \r\n 7 ‘Find Him and Kil~ negative   negative  negative    negative       \r\n 8 ‘Football Is Like~ <NA>       positive  <NA>        positive       \r\n 9 ‘Go Big’ on Coron~ positive   positive  <NA>        positive       \r\n10 ‘Hospital Needs t~ <NA>       negative  negative    negative       \r\n# ... with 30 more rows, and 1 more variable: congruence <lgl>\r\n\r\n#apply scale to print results\r\nprint_sentiments_results$bing_score <- replace_score_with_sentiment(print_sentiments_results$bing_score)\r\nprint_sentiments_results$nrc_score <- replace_score_with_sentiment(print_sentiments_results$nrc_score)\r\nprint_sentiments_results$afinn_score <- replace_score_with_sentiment(print_sentiments_results$afinn_score)\r\nprint_sentiments_results[,2:5] <- lapply(print_sentiments_results[,2:5], as.factor)\r\nhead(print_sentiments_results, 40)\r\n\r\n\r\n# A tibble: 40 x 6\r\n# Rowwise: \r\n   print_text         bing_score nrc_score afinn_score final_sentiment\r\n   <chr>              <fct>      <fct>     <fct>       <fct>          \r\n 1 \"$174 Million Dro~ <NA>       negative  <NA>        negative       \r\n 2 \"Psychedelic Ther~ negative   negative  negative    negative       \r\n 3 \"Morocco Sends Sp~ <NA>       negative  <NA>        negative       \r\n 4 \"‘It’s a Lie’: De~ negative   negative  negative    negative       \r\n 5 \"‘Everything Chan~ <NA>       <NA>      <NA>        <NA>           \r\n 6 \"‘Finally, I Am S~ positive   positive  positive    positive       \r\n 7 \"‘Find Him and Ki~ negative   negative  negative    negative       \r\n 8 \"Soccer Players U~ negative   negative  negative    negative       \r\n 9 \"Plan Would Injec~ <NA>       positive  <NA>        positive       \r\n10 \"As Pandemic Take~ negative   negative  negative    negative       \r\n# ... with 30 more rows, and 1 more variable: congruence <lgl>\r\n\r\nFinal Results\r\n\r\n\r\ntable(main_sentiments_results$congruence, main_sentiments_results$final_sentiment, dnn = c(\"congruence\", \"final\"))\r\n\r\n\r\n          final\r\ncongruence negative neutral positive\r\n     FALSE      114      24       71\r\n     TRUE       339       0      120\r\n\r\ntable(print_sentiments_results$congruence, print_sentiments_results$final_sentiment, dnn = c(\"congruence\", \"final\"))\r\n\r\n\r\n          final\r\ncongruence negative neutral positive\r\n     FALSE       93      35       97\r\n     TRUE       343       0      106\r\n\r\nOther Types of Analysis\r\nUtilizing the cleanNLP\r\npackage:\r\nAs usual, I am struggling to get this to work due to my inexperience\r\nwith the python backend. What worked for me last time I ran this in a\r\nprevious tutorial was no longer working, so I had to uninstall my\r\nminiconda installation and re-install it, but eventually it initialized\r\nso I could run the “cnlp_annotate” function.\r\n\r\n\r\ncnlp_init_udpipe()\r\n\r\n#first the main headlines\r\n\r\nmain_tibble <- as_tibble(main_headlines) %>%\r\n  select(c(\"doc_id\", \"text\"))\r\nmain_tibble <- utf8::as_utf8(main_tibble$text)\r\n\r\nannotated_main <- cnlp_annotate(main_tibble)\r\n\r\n#then the print headlines\r\n\r\nprint_tibble <- as_tibble(print_headlines) %>%\r\n  select(c(\"doc_id\", \"text\"))\r\nprint_tibble <- utf8::as_utf8(print_tibble$text)\r\n\r\nannotated_print <- cnlp_annotate(print_tibble)\r\n\r\n\r\n\r\nUsing spaCyr\r\nFor Main Headlines\r\n\r\n\r\nparsed_main <- spacy_parse(main_headlines,\r\n                           lemma = TRUE,\r\n                           entity = TRUE,\r\n                           nounphrase = TRUE,\r\n                           sentiment = TRUE,\r\n                           additional_attributes = c(\"is_punct\",\r\n                                                     \"is_stop\",\r\n                                                     \"is_currency\",\r\n                                                     \"is_digit\",\r\n                                                     \"is_quote\")) %>%\r\n  as_tibble %>%\r\n  select(-sentence_id)\r\n\r\nhead(parsed_main)\r\n\r\n\r\n# A tibble: 6 x 13\r\n  doc_id token_id token   lemma   pos   entity   nounphrase whitespace\r\n  <chr>     <int> <chr>   <chr>   <chr> <chr>    <chr>      <lgl>     \r\n1 1             1 $       $       SYM   \"MONEY_~ beg        FALSE     \r\n2 1             2 174     174     NUM   \"MONEY_~ mid        TRUE      \r\n3 1             3 Million million NUM   \"MONEY_~ mid        TRUE      \r\n4 1             4 Afghan  Afghan  PROPN \"\"       mid        TRUE      \r\n5 1             5 Drone   Drone   PROPN \"\"       mid        TRUE      \r\n6 1             6 Program Program PROPN \"\"       end_root   TRUE      \r\n# ... with 5 more variables: is_punct <lgl>, is_stop <lgl>,\r\n#   is_currency <lgl>, is_digit <lgl>, is_quote <lgl>\r\n\r\nsummary(parsed_main)\r\n\r\n\r\n    doc_id             token_id         token          \r\n Length:11027       Min.   : 1.000   Length:11027      \r\n Class :character   1st Qu.: 3.000   Class :character  \r\n Mode  :character   Median : 6.000   Mode  :character  \r\n                    Mean   : 6.241                     \r\n                    3rd Qu.: 9.000                     \r\n                    Max.   :22.000                     \r\n    lemma               pos               entity         \r\n Length:11027       Length:11027       Length:11027      \r\n Class :character   Class :character   Class :character  \r\n Mode  :character   Mode  :character   Mode  :character  \r\n                                                         \r\n                                                         \r\n                                                         \r\n  nounphrase        whitespace       is_punct        is_stop       \r\n Length:11027       Mode :logical   Mode :logical   Mode :logical  \r\n Class :character   FALSE:2127      FALSE:10095     FALSE:7566     \r\n Mode  :character   TRUE :8900      TRUE :932       TRUE :3461     \r\n                                                                   \r\n                                                                   \r\n                                                                   \r\n is_currency      is_digit        is_quote      \r\n Mode :logical   Mode :logical   Mode :logical  \r\n FALSE:11021     FALSE:10927     FALSE:10847    \r\n TRUE :6         TRUE :100       TRUE :180      \r\n                                                \r\n                                                \r\n                                                \r\n\r\nFor Print Headlines\r\n\r\n\r\nparsed_print <- spacy_parse(print_headlines,\r\n                           lemma = TRUE,\r\n                           entity = TRUE,\r\n                           nounphrase = TRUE,\r\n                           additional_attributes = c(\"is_punct\",\r\n                                                     \"is_stop\",\r\n                                                     \"is_currency\",\r\n                                                     \"is_digit\",\r\n                                                     \"is_quote\",\r\n                                                     \"sentiment\")) %>%\r\n  as_tibble %>%\r\n  select(-sentence_id)\r\n\r\nhead(parsed_print)\r\n\r\n\r\n# A tibble: 6 x 14\r\n  doc_id token_id token   lemma   pos   entity   nounphrase whitespace\r\n  <chr>     <int> <chr>   <chr>   <chr> <chr>    <chr>      <lgl>     \r\n1 1             1 $       $       SYM   \"MONEY_~ \"beg\"      FALSE     \r\n2 1             2 174     174     NUM   \"MONEY_~ \"mid\"      TRUE      \r\n3 1             3 Million million NUM   \"MONEY_~ \"mid\"      TRUE      \r\n4 1             4 Drone   Drone   PROPN \"\"       \"mid\"      TRUE      \r\n5 1             5 Program Program PROPN \"\"       \"end_root\" TRUE      \r\n6 1             6 for     for     ADP   \"\"       \"\"         TRUE      \r\n# ... with 6 more variables: is_punct <lgl>, is_stop <lgl>,\r\n#   is_currency <lgl>, is_digit <lgl>, is_quote <lgl>,\r\n#   sentiment <dbl>\r\n\r\nsummary(parsed_print)\r\n\r\n\r\n    doc_id             token_id         token          \r\n Length:9805        Min.   : 1.000   Length:9805       \r\n Class :character   1st Qu.: 3.000   Class :character  \r\n Mode  :character   Median : 5.000   Mode  :character  \r\n                    Mean   : 5.775                     \r\n                    3rd Qu.: 8.000                     \r\n                    Max.   :19.000                     \r\n    lemma               pos               entity         \r\n Length:9805        Length:9805        Length:9805       \r\n Class :character   Class :character   Class :character  \r\n Mode  :character   Mode  :character   Mode  :character  \r\n                                                         \r\n                                                         \r\n                                                         \r\n  nounphrase        whitespace       is_punct        is_stop       \r\n Length:9805        Mode :logical   Mode :logical   Mode :logical  \r\n Class :character   FALSE:2022      FALSE:9102      FALSE:6927     \r\n Mode  :character   TRUE :7783      TRUE :703       TRUE :2878     \r\n                                                                   \r\n                                                                   \r\n                                                                   \r\n is_currency      is_digit        is_quote         sentiment\r\n Mode :logical   Mode :logical   Mode :logical   Min.   :0  \r\n FALSE:9797      FALSE:9699      FALSE:9643      1st Qu.:0  \r\n TRUE :8         TRUE :106       TRUE :162       Median :0  \r\n                                                 Mean   :0  \r\n                                                 3rd Qu.:0  \r\n                                                 Max.   :0  \r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/headline-expanded/headlines.png",
    "last_modified": "2022-04-28T01:59:18-05:00",
    "input_file": "headline-expanded.knit.md"
  },
  {
    "path": "posts/headline-analysis/",
    "title": "Analysis of Main vs. Print Headlines: Phase 1",
    "description": "Text as Data Project Headline Comparison Research Using API Query \"Afghanistan Withdrawal\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/NYT-Analysis/"
      }
    ],
    "date": "2022-04-17",
    "categories": [
      "text as data",
      "NYT text analysis project"
    ],
    "contents": "\r\n\r\nContents\r\nComparing\r\nMain vs. Print Headlines in the New York Times\r\nResearch Background\r\nMaking\r\nChoices on Inclusion of Observations\r\nGathering Data\r\nPrevious Process\r\nLoad Data\r\nLoad Data\r\nCreate Corpus\r\nAssign Type to Docvars\r\nTokenization\r\nDocument Feature Matrix\r\n\r\nDictionary Analysis\r\nliwcalike()\r\nNRC\r\nLSD 2015\r\nGeneral Inquirer\r\nAnnotation\r\n\r\n\r\n\r\nComparing\r\nMain vs. Print Headlines in the New York Times\r\nResearch Background\r\nDuring the Fall 2021 semester, my research group hand coded PDF\r\ncopies of articles resulting from a simple search on the websites of the\r\nNew York Times and Wall Street Journal from Feburary 29, 2020 through\r\nSeptember 30, 2021 using the term “Afghanistan withdrawal”. One thing I\r\nnoticed was that when loading the PDF articles into NVivo for coding was\r\nthat it was difficult to match the New York Times articles to the\r\ncitation information in Zotero for many of the articles because the\r\narticle titles did not match. I realized that in the process of saving\r\nthe articles in Zotero, they were saved with the title viewable on the\r\nweb version of the article; however, once the article had been preserved\r\nby using the site’s “Print to PDF” function, the article title that it\r\nused as a default file name was different than the web version.\r\nThis semester, I began this project to be one expanding on last\r\nsemester’s research and looking to expand a machine analysis of articles\r\npulling articles beginning January 2020 through December 2021. For my\r\ninitial text collection, I collected articles using the New York Times\r\nAPI for the search query “Afghanistan”, and hoped to be able to analyze\r\nthe full text of a larger range of articles.\r\nHowever, I found that I am limited in that the article search API for\r\nthe New York Times does not pull the entire article; rather, I was able\r\nto pull the abstract/summary, lead paragraph, and snippet for each\r\narticle as well as the keywords, authors, sections, and url. In\r\naddition, I was able to get the article titles for both the print and\r\nonline versions of the article.\r\nThe API’s lack of full article text was not optimal for my purposes;\r\nto examine sentiment and co-occurence of various sources. Sources are\r\nnot necessarily detailed in the lead paragraph or abstract of an\r\narticle, so I moved to a different research path.\r\nRemembering the differences in headlines from our manual coding\r\nresearch and noting that the API provides both headlines in the article\r\nsearch API, I turned to analyzing the differences in the main vs. print\r\nheadlines for articles from the same research period as our first\r\nexamination. This way, I can potentially use a sample of the full\r\narticles collected in our previous research and take the additional step\r\nof analyzing the sentiment of full articles and how they may or may not\r\nrelate to the differing headlines.\r\nMaking Choices on\r\nInclusion of Observations\r\nIn my initial look at the headline data, it was clear that not all of\r\nthe articles had different headlines; some are the same entries, and\r\nsome have “N/A” in the “print” version only, indicating they were\r\nonline-only stories. Although I initially felt inclined to leave the\r\n“N/A” observations in the analysis, I removed those observations as they\r\nwould not be relevant to my new research questions comparing the framing\r\nfor different audiences.\r\nI also removed whole sections where the API returned an observation\r\nas there was apparently use of the term “Afghanistan withdrawal”\r\nsomewhere in the article/entry, but the type of entry was clearly not\r\nbeing represented in the headline. For example, “Corrections” entries\r\nhave headlines consisting only of the term “Corrections” and the\r\ncorresponding date. Similar choices were made on the “Arts”, Books”, and\r\n“Podcasts” sections when entries are primarily the names of the things\r\nbeing reviewed that may have a reference to the Afghanistan withdrawal\r\nsomewhere in the text, but it is not relevant specifically to the\r\nwithdrawal time period being analyzed.\r\nWith few exceptions, this left the entirety of the “U.S.” and “World”\r\nnews sections, even if the content related to Afghanistan is not readily\r\nobservable. The count (~650) matched the number of articles pulled for\r\nthe hand coding research as well.\r\nGathering Data\r\nPrevious Process\r\nTo pull the data, I had to reduce the queries into more workable\r\ngroups that would not time out, given the NYT API limits. I was able to\r\npull the ~700 articles by chunk, then assemble them into a dataframe\r\nafter cleaning. I will not run the code in this post, as it was already\r\nrun and is an exhaustive process.\r\n\r\n\r\n\r\nAfter compiling the data, I re-formatted the date column and saving\r\nthe formatted tibble for offline access.\r\n\r\n\r\n\r\nLoad Data\r\nNow to the active review of the data. Loading the data from my\r\ncollection phase:\r\nLoad Data\r\n\r\n  article_id      date\r\n1          1 2/29/2020\r\n2          2 2/29/2020\r\n3          3  3/1/2020\r\n4          4  3/2/2020\r\n5          5  3/2/2020\r\n6          6  3/3/2020\r\n                                                                 headline_main\r\n1                              4 Takeaways From the U.S. Deal With the Taliban\r\n2    Taliban and U.S. Strike Deal to Withdraw American Troops From Afghanistan\r\n3           Afghanistan War Enters New Stage as U.S. Military Prepares to Exit\r\n4                 At Center of Taliban Deal, a U.S. Envoy Who Made It Personal\r\n5 U.S. Announces Troop Withdrawal in Afghanistan as Respite From Violence Ends\r\n6                                           Trump Speaks With a Taliban Leader\r\n  article_id      date\r\n1          1 2/29/2020\r\n2          2 2/29/2020\r\n3          3  3/1/2020\r\n4          4  3/2/2020\r\n5          5  3/2/2020\r\n6          6  3/3/2020\r\n                                                        headline_print\r\n1                                 Table Is Set For a Pullout And Talks\r\n2                              U.S. and Taliban Sign Withdrawal Accord\r\n3                                      A Mission Shift for Afghanistan\r\n4 At the Center of the Taliban Deal, a U.S. Envoy Who Made It Personal\r\n5                           U.S. Troop Reduction Begins in Afghanistan\r\n6               Pursuing Exit, Trump Talks  To a Leader Of the Taliban\r\n\r\nCreate Corpus\r\n\r\n\r\nmain_corpus <- corpus(main_headlines, docid_field = \"article_id\", text_field = \"headline_main\")\r\nprint_corpus <- corpus(print_headlines, docid_field = \"article_id\", text_field = \"headline_print\")\r\n\r\n\r\n\r\nAssign Type to Docvars\r\n\r\n\r\nmain_corpus$type <- \"Main Headline\"\r\nprint_corpus$type <- \"Print Headline\"\r\ndocvars(main_corpus, field = \"type\") <- main_corpus$type\r\ndocvars(print_corpus, field = \"type\") <- print_corpus$type\r\n\r\n\r\n\r\nTokenization\r\nAfter many process posts, I finally realized how to remove the “�”\r\nsymbol that has plagued me since starting working with this API by using\r\n“remove_symbols=TRUE” in addition to removing the punctuation when\r\ntokenizing. I also want to remove stopwords.\r\nMain Headlines\r\n\r\n[1] 346\r\nTokens consisting of 346 documents and 2 docvars.\r\n1 :\r\n[1] \"4\"         \"Takeaways\" \"U.S\"       \"Deal\"      \"Taliban\"  \r\n\r\n2 :\r\n[1] \"Taliban\"     \"U.S\"         \"Strike\"      \"Deal\"       \r\n[5] \"Withdraw\"    \"American\"    \"Troops\"      \"Afghanistan\"\r\n\r\n3 :\r\n[1] \"Afghanistan\" \"War\"         \"Enters\"      \"New\"        \r\n[5] \"Stage\"       \"U.S\"         \"Military\"    \"Prepares\"   \r\n[9] \"Exit\"       \r\n\r\n4 :\r\n[1] \"Center\"   \"Taliban\"  \"Deal\"     \"U.S\"      \"Envoy\"    \"Made\"    \r\n[7] \"Personal\"\r\n\r\n5 :\r\n[1] \"U.S\"         \"Announces\"   \"Troop\"       \"Withdrawal\" \r\n[5] \"Afghanistan\" \"Respite\"     \"Violence\"    \"Ends\"       \r\n\r\n6 :\r\n[1] \"Trump\"   \"Speaks\"  \"Taliban\" \"Leader\" \r\n\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\nPrint Headlines\r\n\r\n[1] 346\r\nTokens consisting of 346 documents and 2 docvars.\r\n1 :\r\n[1] \"Table\"   \"Set\"     \"Pullout\" \"Talks\"  \r\n\r\n2 :\r\n[1] \"U.S\"        \"Taliban\"    \"Sign\"       \"Withdrawal\" \"Accord\"    \r\n\r\n3 :\r\n[1] \"Mission\"     \"Shift\"       \"Afghanistan\"\r\n\r\n4 :\r\n[1] \"Center\"   \"Taliban\"  \"Deal\"     \"U.S\"      \"Envoy\"    \"Made\"    \r\n[7] \"Personal\"\r\n\r\n5 :\r\n[1] \"U.S\"         \"Troop\"       \"Reduction\"   \"Begins\"     \r\n[5] \"Afghanistan\"\r\n\r\n6 :\r\n[1] \"Pursuing\" \"Exit\"     \"Trump\"    \"Talks\"    \"Leader\"   \"Taliban\" \r\n\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\nDocument Feature Matrix\r\n\r\n\r\n\r\n\r\n\r\n#create a word frequency variable and the rankings\r\nmain_counts <- as.data.frame(sort(colSums(main_dfm),dec=T))\r\ncolnames(main_counts) <- c(\"Frequency\")\r\nmain_counts$Rank <- c(1:ncol(main_dfm))\r\nhead(main_counts)\r\n\r\n\r\n            Frequency Rank\r\nu.s               100    1\r\nafghanistan        87    2\r\nafghan             85    3\r\ntaliban            65    4\r\nbiden              53    5\r\nwar                30    6\r\n\r\nprint_counts <- as.data.frame(sort(colSums(print_dfm),dec=T))\r\ncolnames(print_counts) <- c(\"Frequency\")\r\nprint_counts$Rank <- c(1:ncol(print_dfm))\r\nhead(print_counts)\r\n\r\n\r\n            Frequency Rank\r\nu.s               100    1\r\ntaliban            66    2\r\nafghan             64    3\r\nafghanistan        56    4\r\nbiden              41    5\r\nexit               27    6\r\n\r\nNow I can take a look at this network of feature co-occurrences for\r\nthe main headlines:\r\n\r\n[1] 1232 1232\r\n[1] 20 20\r\n\r\n\r\nand for the print headlines:\r\n\r\n[1] 1178 1178\r\n[1] 20 20\r\n\r\n\r\nThis brings me to where I had previously stopped in my comparison and\r\nanalysis, and now that I’m able to produce a cleaner result, I’ll move\r\non to further analysis using the quanteda dictionary.\r\nDictionary Analysis\r\nliwcalike()\r\n\r\n [1] \"docname\"      \"Segment\"      \"WPS\"          \"WC\"          \r\n [5] \"Sixltr\"       \"Dic\"          \"anger\"        \"anticipation\"\r\n [9] \"disgust\"      \"fear\"         \"joy\"          \"negative\"    \r\n[13] \"positive\"     \"sadness\"      \"surprise\"     \"trust\"       \r\n[17] \"AllPunc\"      \"Period\"       \"Comma\"        \"Colon\"       \r\n[21] \"SemiC\"        \"QMark\"        \"Exclam\"       \"Dash\"        \r\n[25] \"Quote\"        \"Apostro\"      \"Parenth\"      \"OtherP\"      \r\n\r\nNRC\r\n\r\n\r\n# convert tokens from each headline data set to DFM using the dictionary \"NRC\"\r\nmain_nrc <- dfm(main_tokens) %>%\r\n  dfm_lookup(data_dictionary_NRC)\r\nprint_nrc <- dfm(print_tokens) %>%\r\n  dfm_lookup(data_dictionary_NRC)\r\n\r\ndim(main_nrc)\r\n\r\n\r\n[1] 346  10\r\n\r\nmain_nrc\r\n\r\n\r\nDocument-feature matrix of: 346 documents, 10 features (69.36% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            1       0    0   1        0        1       0\r\n   2     1            1       0    0   1        2        1       1\r\n   3     0            0       0    2   0        1        0       0\r\n   4     0            1       0    0   1        0        2       0\r\n   5     1            0       0    1   1        1        1       1\r\n   6     0            0       0    0   0        0        1       0\r\n    features\r\ndocs surprise trust\r\n   1        1     1\r\n   2        1     1\r\n   3        0     0\r\n   4        1     3\r\n   5        0     1\r\n   6        1     1\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\ndim(print_nrc)\r\n\r\n\r\n[1] 346  10\r\n\r\nprint_nrc\r\n\r\n\r\nDocument-feature matrix of: 346 documents, 10 features (71.47% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            0       0    0   0        0        0       0\r\n   2     0            0       0    0   0        0        1       0\r\n   3     0            0       0    0   0        0        0       0\r\n   4     0            1       0    0   1        0        2       0\r\n   5     0            0       0    0   0        0        0       0\r\n   6     0            0       0    0   0        0        1       0\r\n    features\r\ndocs surprise trust\r\n   1        0     0\r\n   2        0     1\r\n   3        0     0\r\n   4        1     3\r\n   5        0     0\r\n   6        1     1\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\nAnd use the information in a data frame to plot the output:\r\n\r\n\r\n#for the main headlines\r\ndf_main_nrc <- convert(main_nrc, to = \"data.frame\")\r\ndf_main_nrc$polarity <- (df_main_nrc$positive - df_main_nrc$negative)/(df_main_nrc$positive + df_main_nrc$negative)\r\ndf_main_nrc$polarity[which((df_main_nrc$positive + df_main_nrc$negative) == 0)] <- 0\r\n\r\nggplot(df_main_nrc) + \r\n  geom_histogram(aes(x=polarity)) + \r\n  theme_bw()\r\n\r\n\r\n\r\n#and the print headlines\r\ndf_print_nrc <- convert(print_nrc, to = \"data.frame\")\r\ndf_print_nrc$polarity <- (df_print_nrc$positive - df_print_nrc$negative)/(df_print_nrc$positive + df_print_nrc$negative)\r\ndf_print_nrc$polarity[which((df_print_nrc$positive + df_print_nrc$negative) == 0)] <- 0\r\n\r\nggplot(df_print_nrc) + \r\n  geom_histogram(aes(x=polarity)) + \r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nLooking at the headlines that are indicated as “1”, or positive in\r\nsentiment, it’s clear that this dictionary is not capturing the\r\nsentiment accurately.\r\n\r\n\r\nhead(main_corpus[which(df_main_nrc$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n1 :\r\n\"4 Takeaways From the U.S. Deal With the Taliban\"\r\n\r\n4 :\r\n\"At Center of Taliban Deal, a U.S. Envoy Who Made It Personal\"\r\n\r\n6 :\r\n\"Trump Speaks With a Taliban Leader\"\r\n\r\n8 :\r\n\"After Tours in Afghanistan, U.S. Veterans Weigh Peace With t...\"\r\n\r\n9 :\r\n\"Javier Pérez de Cuéllar Dies at 100; U.N. Chief Brokered Pea...\"\r\n\r\n10 :\r\n\"From the Afghan Peace Deal, a Weak and Pliable Neighbor for ...\"\r\n\r\nhead(print_corpus[which(df_print_nrc$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n2 :\r\n\"U.S. and Taliban Sign Withdrawal Accord\"\r\n\r\n4 :\r\n\"At the Center of the Taliban Deal, a U.S. Envoy Who Made It ...\"\r\n\r\n6 :\r\n\"Pursuing Exit, Trump Talks  To a Leader Of the Taliban\"\r\n\r\n7 :\r\n\"Attacks on Afghans by Taliban Rise After Signing of Peace De...\"\r\n\r\n8 :\r\n\"After Afghanistan Tours,  U.S. Veterans Appraise  Peace Deal...\"\r\n\r\n9 :\r\n\"Javier Pérez de Cuéllar, U.N. Chief  Behind Vital Peace Pact...\"\r\n\r\nLSD 2015\r\nI am going to want to look at multiple dictionaries to see if one can\r\nbest apply to this data. First, the LSD 2015 dictionary:\r\n\r\n\r\n# convert main corpus to DFM using the LSD2015 dictionary\r\nmain_lsd2015 <- dfm(tokens(main_corpus, remove_punct = TRUE),\r\n                              tolower = TRUE) %>%\r\n                          dfm_lookup(data_dictionary_LSD2015)\r\n# create main polarity measure for LSD2015\r\nmain_lsd2015 <- convert(main_lsd2015, to = \"data.frame\")\r\nmain_lsd2015$polarity <- (main_lsd2015$positive - main_lsd2015$negative)/(main_lsd2015$positive + main_lsd2015$negative)\r\nmain_lsd2015$polarity[which((main_lsd2015$positive + main_lsd2015$negative) == 0)] <- 0\r\n# convert print corpus to DFM using the LSD2015 dictionary\r\nprint_lsd2015 <- dfm(tokens(print_corpus, remove_punct = TRUE),\r\n                              tolower = TRUE) %>%\r\n                          dfm_lookup(data_dictionary_LSD2015)\r\n# create print polarity measure for LSD2015\r\nprint_lsd2015 <- convert(print_lsd2015, to = \"data.frame\")\r\nprint_lsd2015$polarity <- (print_lsd2015$positive - print_lsd2015$negative)/(print_lsd2015$positive + print_lsd2015$negative)\r\nprint_lsd2015$polarity[which((print_lsd2015$positive + print_lsd2015$negative) == 0)] <- 0\r\n\r\n\r\n\r\nGeneral Inquirer\r\nand the General Inquirer dictionary:\r\n\r\n\r\n# convert main corpus to DFM using the General Inquirer dictionary\r\nmain_geninq <- dfm(tokens(main_corpus, remove_punct = TRUE),\r\n                             tolower = TRUE) %>%\r\n                    dfm_lookup(data_dictionary_geninqposneg)\r\n# create main polarity measure for GenInq\r\nmain_geninq <- convert(main_geninq, to = \"data.frame\")\r\nmain_geninq$polarity <- (main_geninq$positive - main_geninq$negative)/(main_geninq$positive + main_geninq$negative)\r\nmain_geninq$polarity[which((main_geninq$positive + main_geninq$negative) == 0)] <- 0\r\n# convert print corpus to DFM using the General Inquirer dictionary\r\nprint_geninq <- dfm(tokens(print_corpus, remove_punct = TRUE),\r\n                             tolower = TRUE) %>%\r\n                    dfm_lookup(data_dictionary_geninqposneg)\r\n# create print polarity measure for GenInq\r\nprint_geninq <- convert(print_geninq, to = \"data.frame\")\r\nprint_geninq$polarity <- (print_geninq$positive - print_geninq$negative)/(print_geninq$positive + print_geninq $negative)\r\nprint_geninq$polarity[which((print_geninq$positive + print_geninq$negative) == 0)] <- 0\r\n\r\n\r\n\r\nNow I’m going to be able to compare the different dictionary scores\r\nin one data frame for each type of headline.\r\n\r\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy\r\n1          1         0                1           0        0       1\r\n2         10         0                3           0        0       2\r\n3        100         0                0           0        0       0\r\n4        101         0                2           0        0       1\r\n5        102         1                0           0        2       0\r\n6        103         1                1           0        1       0\r\n  nrc_negative nrc_positive nrc_sadness nrc_surprise nrc_trust\r\n1            0            1           0            1         1\r\n2            0            3           0            1         3\r\n3            1            0           1            0         0\r\n4            2            1           0            0         2\r\n5            2            0           2            1         0\r\n6            0            2           0            0         1\r\n  nrc_polarity lsd2015_negative lsd2015_positive lsd2015_neg_positive\r\n1    1.0000000                0                0                    0\r\n2    1.0000000                1                1                    0\r\n3   -1.0000000                1                0                    0\r\n4   -0.3333333                1                0                    0\r\n5   -1.0000000                2                0                    0\r\n6    1.0000000                0                0                    0\r\n  lsd2015_neg_negative lsd2015_polarity geninq_positive\r\n1                    0                0               1\r\n2                    0                0               2\r\n3                    0               -1               0\r\n4                    0               -1               0\r\n5                    0               -1               1\r\n6                    0                0               1\r\n  geninq_negative geninq_polarity\r\n1               1       0.0000000\r\n2               1       0.3333333\r\n3               0       0.0000000\r\n4               2      -1.0000000\r\n5               1       0.0000000\r\n6               1       0.0000000\r\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy\r\n1          1         0                0           0        0       0\r\n2         10         0                2           0        0       1\r\n3        100         0                0           0        0       0\r\n4        101         0                2           0        0       1\r\n5        102         1                0           0        4       0\r\n6        103         1                1           0        1       0\r\n  nrc_negative nrc_positive nrc_sadness nrc_surprise nrc_trust\r\n1            0            0           0            0         0\r\n2            0            2           0            1         2\r\n3            1            0           1            0         0\r\n4            2            1           0            0         2\r\n5            3            0           3            1         0\r\n6            0            1           0            0         0\r\n  nrc_polarity lsd2015_negative lsd2015_positive lsd2015_neg_positive\r\n1    0.0000000                0                0                    0\r\n2    1.0000000                1                0                    0\r\n3   -1.0000000                1                0                    0\r\n4   -0.3333333                1                0                    0\r\n5   -1.0000000                2                0                    0\r\n6    1.0000000                0                0                    0\r\n  lsd2015_neg_negative lsd2015_polarity geninq_positive\r\n1                    0                0               0\r\n2                    0               -1               2\r\n3                    0               -1               0\r\n4                    0               -1               0\r\n5                    0               -1               1\r\n6                    0                0               1\r\n  geninq_negative geninq_polarity\r\n1               0       0.0000000\r\n2               1       0.3333333\r\n3               1      -1.0000000\r\n4               2      -1.0000000\r\n5               0       1.0000000\r\n6               0       1.0000000\r\n\r\nNow that we have them all in a single data frame, it’s\r\nstraightforward to figure out a bit about how well our different\r\nmeasures of polarity agree across the different approaches by looking at\r\ntheir correlation using the “cor()” function.\r\n\r\n\r\ncor(main_sent$nrc_polarity, main_sent$lsd2015_polarity)\r\n\r\n\r\n[1] 0.4968335\r\n\r\ncor(main_sent$nrc_polarity, main_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4813359\r\n\r\ncor(main_sent$lsd2015_polarity, main_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.5327856\r\n\r\n\r\n\r\ncor(print_sent$nrc_polarity, print_sent$lsd2015_polarity)\r\n\r\n\r\n[1] 0.4197161\r\n\r\ncor(print_sent$nrc_polarity, print_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4917256\r\n\r\ncor(print_sent$lsd2015_polarity, print_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4921922\r\n\r\nAnnotation\r\n\r\n\r\nlibrary(cleanNLP)\r\ncnlp_init_udpipe()\r\n\r\n\r\n\r\n\r\n\r\nlibrary(tidyr)\r\n#amain <- as_tibble(headlines_main)\r\n#annotated_main <- cnlp_annotate(main_corpus)\r\n#annotated_main\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/headline-analysis/unnamed-chunk-12-1.png",
    "last_modified": "2022-04-26T14:34:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/pdf-analysis/",
    "title": "Analysis of PDF Articles",
    "description": "Text as Data Project-Article Sentiment Research",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/NYT-Analysis/"
      }
    ],
    "date": "2022-04-17",
    "categories": [
      "text as data",
      "NYT text analysis project"
    ],
    "contents": "\r\n\r\nContents\r\nGetting Started\r\nPulling in the PDF docs\r\nExtracting\r\nPDF Files being examined (random at this time - exploratory)\r\nInspect the first\r\narticle\r\nInspecting Individual\r\nArticles\r\nUnlist\r\n\r\nConclusion\r\n\r\nGetting Started\r\nThe primary goal of this aspect of research is to refine the process\r\nfor examining the content of the full articles for which the main\r\nvs. print headlines are the most different from each other in the\r\nprimary project analysis.\r\nPulling in the PDF docs\r\nI have the PDF files in my working directory. Using the\r\n“list.files()” function from the “pdftools” package, I can create a\r\nvector of PDF file names, specifying only files that end in “.pdf”.\r\n\r\n\r\nShow code\r\n\r\n#load libraries\r\nlibrary(pdftools)\r\nlibrary(readtext)\r\nlibrary(readr)\r\nlibrary(tm)\r\nlibrary(tidytext)\r\nlibrary(stringr)\r\nlibrary(MASS)\r\nlibrary(tidyverse)\r\nlibrary(plyr); library(dplyr)\r\nlibrary(quanteda)\r\nlibrary(purrr)\r\nlibrary(here)\r\n\r\n\r\n\r\nExtracting\r\nPDF Files being examined (random at this time - exploratory)\r\n\r\n\r\n#create file names\r\nfiles <- list.files(pattern = \"pdf$\")\r\n\r\n#extract the pdf file data\r\nnyt_articles <- lapply(files, pdf_text)\r\n\r\n#apply length functions\r\nlapply(nyt_articles, length)\r\n\r\n\r\n[[1]]\r\n[1] 4\r\n\r\n[[2]]\r\n[1] 2\r\n\r\n[[3]]\r\n[1] 3\r\n\r\n[[4]]\r\n[1] 4\r\n\r\n[[5]]\r\n[1] 10\r\n\r\n[[6]]\r\n[1] 6\r\n\r\n[[7]]\r\n[1] 2\r\n\r\n[[8]]\r\n[1] 5\r\n\r\n[[9]]\r\n[1] 5\r\n\r\n[[10]]\r\n[1] 2\r\n\r\n#view the structure of the list\r\nstr(nyt_articles)\r\n\r\n\r\nList of 10\r\n $ : chr [1:4] \"                                 https://www.nytimes.com/2021/04/13/us/politics/samantha-power-biden.html\\n\\n\\n\"| __truncated__ \"                                Secretary of State Antony J. Blinken at the opening session of talks with China\"| __truncated__ \"The mostly benign prodding by Democrats and Republicans during the hearing signaled how countering China has be\"| __truncated__ \"“There is so much that can be done between bombing and nothing,” Mr. Prendergast said, paraphrasing Luis Moreno\"| __truncated__\r\n $ : chr [1:2] \"                            https://www.nytimes.com/2021/04/29/world/asia/central-asia-border-\\n               \"| __truncated__ \"In announcing the cease-fire, the Kyrgyz Ministry of Interior said that it “does not have\\ndesigns on foreign t\"| __truncated__\r\n $ : chr [1:3] \"                                https://www.nytimes.com/2021/08/05/us/politics/taliban-afghanistan-peace-deal.h\"| __truncated__ \"The statement came as Taliban representatives met with Afghan government officials, including Mr. Abdullah, for\"| __truncated__ \"“The Taliban is not interested in negotiating seriously right now because of what’s happening on the battlefiel\"| __truncated__\r\n $ : chr [1:4] \"                                https://www.nytimes.com/2021/08/08/us/politics/taliban-afghanistan-united-state\"| __truncated__ \"Over the past week, Taliban fighters have moved swiftly to retake cities around Afghanistan, assassinated gover\"| __truncated__ \"                                 Ms. Psaki speaking to reporters at the White House, on Friday. Tom Brenner for\"| __truncated__ \"Mr. Biden, declaring that the United States had long ago accomplished its mission of denying terrorists a haven\"| __truncated__\r\n $ : chr [1:10] \"                                https://www.nytimes.com/2021/08/30/world/asia/us-withdrawal-afghanistan-kabul.h\"| __truncated__ \"Old Soviet tanks litter the grounds of Bala Hissar, outside Kunduz. Jim Huylebroek for The New York Times\\n\" \"  Khalil Haqqani, a Taliban leader, appeared at Friday prayers in Kabul this month with an American-made M-4 ri\"| __truncated__ \"The Taliban’s leverage, earned after years of fighting the world’s most advanced military, multiplied as they c\"| __truncated__ ...\r\n $ : chr [1:6] \"                                 https://www.nytimes.com/2021/09/01/world/asia/afghanistan-taliban-government-l\"| __truncated__ \"  Internally displaced Afghans fleeing the fighting in the north still live at a camp in the Sarawi Shomali par\"| __truncated__ \"  A vendor selling Taliban flags in Kabul on Friday near posters of the senior Taliban officials Amir Khan Mutt\"| __truncated__ \"The Taliban are also fighting stubborn opposition forces led by National Resistance Front leaders in Panjshir P\"| __truncated__ ...\r\n $ : chr [1:2] \"                               https://www.nytimes.com/2021/09/02/us/politics/congress-pentagon-budget-biden.ht\"| __truncated__ \"The lopsided vote underscored another reality: Even as the hard-charging liberal bloc of lawmakers pledging to \"| __truncated__\r\n $ : chr [1:5] \"                                 https://www.nytimes.com/2021/09/07/us/politics/afghan-war-iraq-veterans.html\\n\"| __truncated__ \"                                Jen Burch said the doctors who examined her in 2014 found ground glass nodules \"| __truncated__ \"                                 Melissa Gauntner has dealt with dual traumas and has at times been gripped wit\"| __truncated__ \"In military families, scholars find what they call secondary traumatic distress, symptoms of anxiety stemming f\"| __truncated__ ...\r\n $ : chr [1:5] \"                                 https://www.nytimes.com/2020/10/05/world/asia/afghan-peace-talks-children.html\"| __truncated__ \"                                   Fatima Gailani, whose father was one of the leaders of the mujahedeen resist\"| __truncated__ \"                                Anas Haqqani, the youngest son of the insurgent chief Jalaluddin Haqqani, is pa\"| __truncated__ \"                                 Jalaluddin Haqqani in an undated photo from a video released by the Taliban on\"| __truncated__ ...\r\n $ : chr [1:2] \"                                https://www.nytimes.com/2020/03/04/world/asia/afghanistan-taliban-violence.html\"| __truncated__ \"     Understand the Taliban Takeover in Afghanistan\\n\\n     Who are the Taliban? The Taliban arose in 1994 amid\"| __truncated__\r\n\r\nInspect the first article\r\n\r\n\r\nhead(nyt_articles[1])\r\n\r\n\r\n[[1]]\r\n[1] \"                                 https://www.nytimes.com/2021/04/13/us/politics/samantha-power-biden.html\\n\\n\\n\\nAfter Backing Military Force in Past, U.S.A.I.D. Nominee Focuses on Deploying Soft\\nPower\\nIf confirmed to oversee the U.S. Agency for International Development, Samantha Power will confront adversaries by bolstering\\ndemocracy and human rights. China is an early focus.\\n\\n\\n          By Lara Jakes\\n\\nPublished April 13, 2021   Updated April 14, 2021\\n\\n\\nWASHINGTON — Near the end of the 2014 documentary “Watchers of the Sky,” which chronicles the origins of the legal definition\\nof genocide, Samantha Power grows emotional. At the time, Ms. Power was President Barack Obama’s ambassador to the United\\nNations, and, she said, had “great visibility into a lot of the pain” in the world.\\n\\nFrom that perch, preventing mass atrocities abroad required “thinking through what we can do about it, to exhaust the tools at your\\ndisposal,” Ms. Power said in the film. “And I always think about the privilege of, you know, of getting to try — just to try.”\\n\\nFew doubt Ms. Power’s zeal — given her career as a war correspondent, human rights activist, academic expert and foreign policy\\nadviser — even if it has meant advocating military force to stop widespread killings.\\n\\nNow, as President Biden’s nominee to lead the United States Agency for International Development, she is preparing to rejoin the\\ngovernment as an administrator of soft power, and resist using weapons as a means of deterrence and punishment that she has\\npushed for in the past.\\n\\nA Senate committee is expected to vote Thursday on her nomination to lead one of the world’s largest distributors of humanitarian\\naid.\\n\\nIf she is confirmed, Mr. Biden will also seat her on the National Security Council, where during the Obama administration she\\npressed for military intervention to protect civilians from state-sponsored attacks in Libya in 2011 and Syria in 2013. (However, she\\nalso opposed the 2003 invasion of Iraq.)\\n\\nThat she will be back at the table at the council — and again almost certain to be debating whether to entangle American forces in\\nenduring conflicts — has concerned some officials, analysts and think tank experts who demand military restraint from the Biden\\nadministration. Mr. Biden appears to be leaning that way: He has embraced economic sanctions as a tool of hard power and is\\nexpected to announce a full withdrawal of American troops from Afghanistan by Sept. 11, ending the United States’ longest war.\\n\\n“If you’re talking about humanitarianism, famine, the wars — really, other than natural causes, war is the No. 1 cause of famine\\naround the world,” Senator Rand Paul, Republican of Kentucky, told Ms. Power last month during her Senate confirmation hearing.\\n“Are you willing to admit that the Libyan and Syrian interventions that you advocated for were a mistake?”\\n\\nMs. Power did not. “When these situations arise, it’s a question almost of lesser evils — that the choices are very challenging,” she\\nsaid.\\n\\nBy its very nature, the U.S. aid agency takes a long-term view of the world compared with the immediacy of military action. Beyond\\nthe roughly $6 billion in humanitarian aid it is delivering this year to disaster-ridden nations, the agency seeks to prevent conflict at\\nits roots, largely bolstering economies, countering state corruption and fostering democracy and human rights.\\n\\nThat mission is central to Mr. Biden’s foreign policy, and will perhaps prove nowhere more pivotal than in his global competition\\nwith China.\\n\\nLast month, Secretary of State Antony J. Blinken assured allies that they would not be backed into an “‘us-or-them’ choice with\\nChina” as the two superpowers vie for economic, diplomatic and military advantage.\\n\"\r\n[2] \"                                Secretary of State Antony J. Blinken at the opening session of talks with China at the\\n                                Captain Cook hotel in Anchorage. Pool photo by Frederic J. Brown\\n\\n\\n\\nInstead, the United States is highlighting what officials call China’s malign ideology and self-interests as it expands an influence\\ncampaign across Africa, Europe and South America with financial loans, infrastructure funds, coronavirus vaccines and advanced\\ntechnology.\\n\\nThe Trump administration also seized on China’s human rights abuses — particularly against ethnic Uyghurs in the country’s\\nwestern region of Xinjiang — to persuade allies to turn against Beijing. On the Trump administration’s final day in office, Mike\\nPompeo, the secretary of state, declared China’s oppression against Uyghurs as an act of genocide, and he criticized Beijing’s\\nviolent suppression of dissidents in Hong Kong and military harassment of Taiwan.\\n\\n\\n                                Sign Up for On Politics A guide to the political news cycle, cutting\\n                                through the spin and delivering clarity from the chaos. Get it sent to your\\n                                inbox.\\n\\n\\nOfficials said China’s much-debated Belt and Road Initiative was a prime battleground for U.S.A.I.D. to challenge Beijing.\\n\\nRepresentative Tom Malinowski, Democrat of New Jersey and a former assistant secretary of state for democracy and human\\nrights for Mr. Obama, described a “perception that China is exporting corruption” with its loans and development projects.\\n\\nFor example, a study in February by the International Republican Institute, a private nonprofit group that receives government\\nfunding and promotes democracy, concluded that Panama’s decision in 2017 to sever diplomatic ties with Taiwan “appears to have\\nbeen driven by payoffs” from China. It also noted that Nepal regularly revoked the legal status of Tibetan refugees after becoming\\neconomically reliant on Beijing.\\n\\nThe American aid agency alone cannot match the funds that China has seeded in developing countries. But Mr. Malinowski said its\\nsupport to journalists, legal advisers and legitimate opposition groups could “expose and combat” corrosive foreign leaders who\\nhad benefited from Beijing’s financial backing and playbook for how to remain in power.\\n\\n“There is one issue that has risen to the top in this administration that I know she is very focused on, and that’s fighting corruption,”\\nMr. Malinowski said of Ms. Power. “And U.S.A.I.D. has a very important role to play there, potentially.”\\n\\nAt her confirmation hearing in March, Ms. Power told senators she was moved to pursue a career in foreign policy after the 1989\\nmassacre of protesters in Tiananmen Square in Beijing. She described China’s “coercive and predatory approach, which is so\\ntransactional” in its dealings with developing countries that ultimately become dependent on Beijing through what she called “debt-\\ntrap diplomacy.”\\n\\n“I think it’s not going over that well, and that creates an opening for the United States,” Ms. Power told Senator Todd Young,\\nRepublican of Indiana.\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \r\n[3] \"The mostly benign prodding by Democrats and Republicans during the hearing signaled how countering China has become a rare,\\nif reliable, issue of bipartisanship in Congress. “It’s absolutely essential that our development dollars, I think, be used to advance\\nour geostrategic priorities,” Mr. Young said.\\n\\nThe aid agency and the State Department have budgeted about $2 billion on programs to foster democracy, human rights and open\\ngovernance abroad in the 2021 fiscal year — one-third as much as funding for humanitarian assistance.\\n\\nIt is an area that Ms. Power is expected to expand. The Biden administration’s first budget blueprint, released on Friday, asserted it\\nwould commit an unspecified but “significant increase in resources” to advance human rights and democracy while thwarting\\ncorruption and authoritarianism.\\n\\n\\n\\n\\n                               Asylum seekers from Central America crossing the Paso del Norte International Bridge,\\n                               in Ciudad Juarez, Mexico. One of Ms. Power’s priorities will be to target corruption,\\n                               violence and poverty in the region. Jose Luis Gonzalez/Reuters\\n\\n\\n\\nThe spending plan also will support another of Ms. Power’s priorities: targeting corruption, violence and poverty in Central\\nAmerica as a means to curb the flow of thousands of migrants who head to the southwestern border each year. The Biden\\nadministration is banking on a $4 billion strategy through 2025 — including an initial tranche of $861 million proposed this year — to\\nhelp stabilize the region.\\n\\nIn El Salvador, for example, homicides dropped 61 percent after a U.S.A.I.D. effort to reduce violence from 2015 to 2017, Ms. Power\\ntold the senators, and the agency’s programs in Honduras have yielded similar results. The programs not only supported local\\nprosecutors but also brought together government officials, businesses and church and community leaders to divert young people\\nfrom gangs through job training, tutoring and artistic activities.\\n\\nShe was met with some skepticism.\\n\\nSenator Rob Portman, Republican of Ohio, noted that the number of children from Central America at the border had steadily\\nincreased since January, even though the United States spent $3.6 billion over the past five years on similar efforts.\\n\\n“The results are not impressive,” Mr. Portman said. “It’s an economic issue, primarily,” and “people will still be looking to come to\\nthe United States.”\\n\\nExplaining foreign policy decisions to the American people, and making it relevant to their lives, is a driving theme of the State\\nDepartment under Mr. Biden. Ms. Power can reach back to her own experiences as both an immigrant from Ireland and a\\nstoryteller to make the case for easing the border crisis by attacking its root causes.\\n\\n“That’s part of the job, too — you’ve got to be a salesperson, you’ve got to go out there and explain to people, ‘Here’s why we need\\nmore resources to do this work, and here’s where U.S.A.I.D. can be an incredibly important partner,’” said John Prendergast, a\\nlongtime human rights and anticorruption activist and close friend to Ms. Power.\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \r\n[4] \"“There is so much that can be done between bombing and nothing,” Mr. Prendergast said, paraphrasing Luis Moreno Ocampo, the\\nformer prosector of the International Criminal Court who was featured in the same documentary about genocide as Ms. Power.\\n“And Samantha’s whole work and life has been between those two extremes.”\\n\\nGayle Smith, who ran the aid agency for Mr. Obama and is now the State Department’s coronavirus vaccine envoy, put it more\\nbluntly.\\n\\n“It’s not like U.S.A.I.D. is going to invade somebody,” she said.\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \r\n\r\nInspecting Individual\r\nArticles\r\nNow I’m going to use “purrr” to “pluck()” each of the articles as\r\nits’ own vector and create a corpus of each article to examine.\r\n\r\n\r\narticle_111 <- nyt_articles %>% \r\n  pluck(1)\r\narticle_111 <- as_vector(article_111)\r\n\r\narticle_111_corpus <- corpus(article_111)\r\narticle_111_summary <- summary(article_111_corpus)\r\narticle_111_summary\r\n\r\n\r\nCorpus consisting of 4 documents, showing 4 documents:\r\n\r\n  Text Types Tokens Sentences\r\n text1   357    688        24\r\n text2   289    523        19\r\n text3   304    562        18\r\n text4    76    105         4\r\n\r\nI also found a very interesting way to pul the text and save them as\r\nindividual .txt files, but for now I’m just going to note that as an\r\nalternative process. I’ve struggled quite a bit to get the PDF text read\r\ncompared to the headlines.\r\n\r\n\r\nShow code\r\n\r\nconvertpdf2txt <- function(dirpath){\r\n  files <- list.files(dirpath, full.names = T)\r\n  x <- sapply(files, function(x){\r\n  x <- pdftools::pdf_text(x) %>%\r\n  paste(sep = \" \") %>%\r\n  stringr::str_replace_all(fixed(\"\\n\"), \" \") %>%\r\n  stringr::str_replace_all(fixed(\"\\r\"), \" \") %>%\r\n  stringr::str_replace_all(fixed(\"\\t\"), \" \") %>%\r\n  stringr::str_replace_all(fixed(\"\\\"\"), \" \") %>%\r\n  paste(sep = \" \", collapse = \" \") %>%\r\n  stringr::str_squish() %>%\r\n  stringr::str_replace_all(\"- \", \"\") \r\n  return(x)\r\n    })\r\n}\r\n# apply function\r\ntxts <- convertpdf2txt(\"./files\")\r\n# inspect the structure of the txts element\r\nstr(txts)\r\n\r\n\r\n Named chr [1:10] \"https://www.nytimes.com/2021/04/13/us/politics/samantha-power-biden.html After Backing Military Force in Past, \"| __truncated__ ...\r\n - attr(*, \"names\")= chr [1:10] \"./files/article_111.pdf\" \"./files/article_132.pdf\" \"./files/article_193.pdf\" \"./files/article_196.pdf\" ...\r\n\r\n\r\n\r\nShow code\r\n\r\n#apply length functions\r\nlapply(txts, length)\r\n\r\n\r\n$`./files/article_111.pdf`\r\n[1] 1\r\n\r\n$`./files/article_132.pdf`\r\n[1] 1\r\n\r\n$`./files/article_193.pdf`\r\n[1] 1\r\n\r\n$`./files/article_196.pdf`\r\n[1] 1\r\n\r\n$`./files/article_278.pdf`\r\n[1] 1\r\n\r\n$`./files/article_288.pdf`\r\n[1] 1\r\n\r\n$`./files/article_293.pdf`\r\n[1] 1\r\n\r\n$`./files/article_300.pdf`\r\n[1] 1\r\n\r\n$`./files/article_56.pdf`\r\n[1] 1\r\n\r\n$`./files/article_7.pdf`\r\n[1] 1\r\n\r\nShow code\r\n\r\n#view the structure of the list\r\nstr(txts)\r\n\r\n\r\n Named chr [1:10] \"https://www.nytimes.com/2021/04/13/us/politics/samantha-power-biden.html After Backing Military Force in Past, \"| __truncated__ ...\r\n - attr(*, \"names\")= chr [1:10] \"./files/article_111.pdf\" \"./files/article_132.pdf\" \"./files/article_193.pdf\" \"./files/article_196.pdf\" ...\r\n\r\nShow code\r\n\r\n# add names to txt files\r\nnames(txts) <- paste(\"nyt\", 1:length(txts), sep = \"\")\r\n# save result to disc\r\nlapply(seq_along(txts), function(i)writeLines(text = unlist(txts[i]),\r\n    con = paste(\"./txts\", names(txts)[i],\".txt\", sep = \"\")))\r\n\r\n\r\n[[1]]\r\nNULL\r\n\r\n[[2]]\r\nNULL\r\n\r\n[[3]]\r\nNULL\r\n\r\n[[4]]\r\nNULL\r\n\r\n[[5]]\r\nNULL\r\n\r\n[[6]]\r\nNULL\r\n\r\n[[7]]\r\nNULL\r\n\r\n[[8]]\r\nNULL\r\n\r\n[[9]]\r\nNULL\r\n\r\n[[10]]\r\nNULL\r\n\r\nUnlist\r\nDocumenting, for now, the ways I’m struggling with so I can find out\r\nwhy.\r\n\r\n\r\n#convert list to vector\r\n#nyt_vector <- unlist(nyt_articles, recursive = TRUE)\r\n#put articles into data frame\r\n#nyt_df <- as.data.frame(nyt_vector, row.names = NULL, stringsAsFactors = FALSE)\r\n\r\n\r\n\r\n\r\n\r\n#create corpus\r\n#nyt_corpus <- corpus(txts)\r\n#confirming class of corpus\r\n#class(nyt_corpus)\r\n#confirm length of corpus\r\n#length(nyt_corpus)\r\n\r\n\r\n\r\nConclusion\r\nI will not be able to fit this type of analysis into the scope of my\r\ncurrent project. I will use this in further studies.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-23T13:23:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/lit-review/",
    "title": "Literature Review",
    "description": "Text as Data Project-Literature Review",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/NYT-Analysis/"
      }
    ],
    "date": "2022-04-15",
    "categories": [
      "text as data",
      "NYT text analysis project",
      "literature review"
    ],
    "contents": "\r\n\r\nContents\r\nAnnotated\r\nBibliography (Starting with Abstracts Only - In Development)\r\nLiu & Huang (2022)\r\nChan, et al. (2021)\r\nVan Atteveldt, et\r\nal. (2021)\r\nBurggraff & Trilling\r\n(2020)\r\nBoukes, et al. (2020)\r\nSong, et al. (2020)\r\nRudkowsky, et al. (2018)\r\nSilva (2017)\r\nGottlieb (2015)\r\nDiakopoulos (2015)\r\nGrimmer & Stewart\r\n(2013)\r\nKothari (2010)\r\nKiousis (2004)\r\nAlthaus & Tewksbury\r\n(2002)\r\n\r\n\r\nAnnotated\r\nBibliography (Starting with Abstracts Only - In Development)\r\nLiu & Huang (2022)\r\nLiu, M., & Huang, J. (2022). “Climate change” versus “global\r\nwarming”: A corpus-assisted discourse analysis of two popular terms in\r\nthe New York Times. Journal of World Languages. https://doi.org/10.1515/jwl-2022-0004\r\nAbstract\r\n“Climate change” and “global warming” are two popular terms that may\r\nbe often used interchangeably in news media. This study proposes to give\r\na corpusassisted discourse study of the representations of climate\r\nchange and global warming in the New York Times (2000–2019) in order to\r\nexamine how they are actually used in the newspaper. The findings show\r\nboth similarities and differences in their representations in terms of\r\nthe associated topics/themes, the particular ways of framing, and the\r\nperspectivization strategy employed. It is argued that a corpus-assisted\r\ndiscourse study of a large sample of news articles presents a more\r\naccurate picture of the actual use of the two terms in news media.\r\nChan, et al. (2021)\r\nChan, C., Bajjalieh, J., Auvil, L., Wessler, H., Althaus, S.,\r\nWelbers, K., Atteveldt, W. van, & Jungblut, M. (2021). Four best\r\npractices for measuring news sentiment using ‘off-the-shelf’\r\ndictionaries: A large-scale p-hacking experiment. Computational\r\nCommunication Research, 3(1), 1–27.\r\nAbstract\r\nWe examined the validity of 37 sentiment scores based on\r\ndictionary-based methods using a large news corpus and demonstrated the\r\nrisk of generating a spectrum of results with different levels of\r\nstatistical significance by presenting an analysis of relationships\r\nbetween news sentiment and U.S. presidential approval. We summarize our\r\nfindings into four best practices: 1) use a suitable sentiment\r\ndictionary; 2) do not assume that the validity and reliability of the\r\ndictionary is ‘built-in’; 3) check for the influence of content length\r\nand 4) do not use multiple dictionaries to test the same statistical\r\nhypothesis.\r\nVan Atteveldt, et al. (2021)\r\nvan Atteveldt, W., van der Velden, M. A. C. G., & Boukes, M.\r\n(2021). The Validity of Sentiment Analysis: Comparing Manual Annotation,\r\nCrowd-Coding, Dictionary Approaches, and Machine Learning Algorithms.\r\nCommunication Methods and Measures, 15(2), 121–140. https://doi.org/10.1080/19312458.2020.1869198\r\nAbstract\r\nSentiment is central to many studies of communication science, from\r\nnegativity and polarization in political communication to analyzing\r\nproduct reviews and social media comments in other sub-fields. This\r\nstudy provides an exhaustive comparison of sentiment analysis methods,\r\nusing a validation set of Dutch economic headlines to compare the\r\nperformance of manual annotation, crowd coding, numerous dictionaries\r\nand machine learning using both traditional and deep learning\r\nalgorithms. The three main conclusions of this article are that: (1) The\r\nbest performance is still attained with trained human or crowd coding;\r\n(2) None of the used dictionaries come close to acceptable levels of\r\nvalidity; and (3) machine learning, especially deep learning,\r\nsubstantially outperforms dictionary-based methods but falls short of\r\nhuman performance. From these findings, we stress the importance of\r\nalways validating automatic text analysis methods before usage.\r\nMoreover, we provide a recommended step-bystep approach for (automated)\r\ntext analysis projects to ensure both efficiency and validity.\r\nBurggraff & Trilling (2020)\r\nBurggraaff, C., & Trilling, D. (2020). Through a different\r\ngate: An automated content analysis of how online news and print news\r\ndiffer. Journalism, 21(1), 112–129. https://doi.org/10.1177/1464884917716699\r\nAbstract\r\nWe investigate how news values differ between online and print news\r\narticles. We hypothesize that print and online articles differ in terms\r\nof news values because of differences in the routines used to produce\r\nthem. Based on a quantitative automated content analysis of N = 762,095\r\nDutch news items, we show that online news items are more likely to be\r\nfollow-up items than print items, and that there are further differences\r\nregarding news values like references to persons, the power elite,\r\nnegativity, and positivity. In order to conduct this large-scale\r\nanalysis, we developed innovative methods to automatically code a wide\r\nrange of news values. In particular, this article demonstrates how\r\ntechniques such as sentiment analysis, named entity recognition,\r\nsupervised machine learning, and automated queries of external databases\r\ncan be combined and used to study journalistic content. Possible\r\nexplanations for the difference found between online and offline news\r\nare discussed.\r\nBoukes, et al. (2020)\r\nBoukes, M., van de Velde, B., Araujo, T., & Vliegenthart, R.\r\n(2020). What’s the Tone? Easy Doesn’t Do It: Analyzing Performance and\r\nAgreement Between Off-the-Shelf Sentiment Analysis Tools. Communication\r\nMethods and Measures, 14(2), 83–104. https://doi.org/10.1080/19312458.2019.1671966\r\nAbstract\r\nThis article scrutinizes the method of automated content analysis to\r\nmeasure the tone of news coverage. We compare a range of off-the-shelf\r\nsentiment analysis tools to manually coded economic news as well as\r\nexamine the agreement between these dictionary approaches themselves. We\r\nassess the performance of five off-the-shelf sentiment analysis tools\r\nand two tailor-made dictionary-based approaches. The analyses result in\r\nfive conclusions. First, there is little overlap between the\r\noff-the-shelf tools; causing wide divergence in terms of tone\r\nmeasurement. Second, there is no stronger overlap with manual coding for\r\nshort texts (i.e., headlines) than for long texts (i.e., full articles).\r\nThird, an approach that combines individual dictionaries achieves a\r\ncomparably good performance. Fourth, precision may increase to\r\nacceptable levels at higher levels of granularity. Fifth, performance of\r\ndictionary approaches depends more on the number of relevant keywords in\r\nthe dictionary than on the number of valenced words as such; a small\r\ntailor-made lexicon was not inferior to large established dictionaries.\r\nAltogether, we conclude that off-the-shelf sentiment analysis tools are\r\nmostly unreliable and unsuitable for research purposes – at least in the\r\ncontext of Dutch economic news – and manual validation for the specific\r\nlanguage, domain, and genre of the research project at hand is always\r\nwarranted.\r\nSong, et al. (2020)\r\nSong, H., Tolochko, P., Eberl, J.-M., Eisele, O., Greussing, E.,\r\nHeidenreich, T., Lind, F., Galyga, S., & Boomgaarden, H. G. (2020).\r\nIn Validations We Trust? The Impact of Imperfect Human Annotations as a\r\nGold Standard on the Quality of Validation of Automated Content\r\nAnalysis. Political Communication, 37(4), 550–572. https://doi.org/10.1080/10584609.2020.1723752\r\nAbstract\r\nPolitical communication has become one of the central arenas of\r\ninnovation in the application of automated analysis approaches to\r\never-growing quantities of digitized texts. However, although\r\nresearchers routinely and conveniently resort to certain forms of human\r\ncoding to validate the results derived from automated procedures, in\r\npractice the actual “quality assurance” of such a “gold standard” often\r\ngoes unchecked. Contemporary practices of validation via manual\r\nannotations are far from being acknowledged as best practices in the\r\nliterature, and the reporting and interpretation of validation\r\nprocedures differ greatly. We systematically assess the connection\r\nbetween the quality of human judgment in manual annotations and the\r\nrelative performance evaluations of automated procedures against true\r\nstandards by relying on large-scale Monte Carlo simulations. The results\r\nfrom the simulations confirm that there is a substantially greater risk\r\nof a researcher reaching an incorrect conclusion regarding the\r\nperformance of automated procedures when the quality of manual\r\nannotations used for validation is not properly ensured. Our\r\ncontribution should therefore be regarded as a call for the systematic\r\napplication of high-quality manual validation materials in any political\r\ncommunication study, drawing on automated text analysis procedures.\r\nRudkowsky, et al. (2018)\r\nRudkowsky, E., Haselmayer, M., Wastian, M., Jenny, M., Emrich,\r\nŠ., & Sedlmair, M. (2018). More than Bags of Words: Sentiment\r\nAnalysis with Word Embeddings. Communication Methods and Measures,\r\n12(2–3), 140–157. https://doi.org/10.1080/19312458.2018.1455817\r\nAbstract\r\nMoving beyond the dominant bag-of-words approach to sentiment\r\nanalysis we introduce an alternative procedure based on distributed word\r\nembeddings. The strength of word embeddings is the ability to capture\r\nsimilarities in word meaning. We use word embeddings as part of a\r\nsupervised machine learning procedure which estimates levels of\r\nnegativity in parliamentary speeches. The procedure’s accuracy is\r\nevaluated with crowdcoded training sentences; its external validity\r\nthrough a study of patterns of negativity in Austrian parliamentary\r\nspeeches. The results show the potential of the word embeddings approach\r\nfor sentiment analysis in the social sciences.\r\nSilva (2017)\r\nSilva, D. M. D. (2017). The Othering of Muslims: Discourses of\r\nRadicalization in the New York Times, 1969–2014. Sociological Forum,\r\n32(1), 138–161. https://doi.org/10.1111/socf.12321\r\nAbstract\r\nIn this article, I engage with Edward Said’s Orientalism and various\r\nperspectives within the othering paradigm to analyze the emergence and\r\ntransformation of radicalization discourses in the news media. Employing\r\ndiscourse analysis of 607 New York Times articles from 1969 to 2014,\r\nthis article demonstrates that radicalization discourses are not new but\r\nare the result of complex sociolinguistic and historical developments\r\nthat cannot be reduced to dominant contemporary understandings of the\r\nconcept or to singular events or crises. The news articles were then\r\ncompared to 850 government documents, speeches, and other official\r\ncommunications. The analysis of the data indicates that media\r\nconceptualizations of radicalization, which once denoted political and\r\neconomic differences, have now shifted to overwhelmingly focus on Islam.\r\nAs such, radicalization discourse now evokes the construct\r\nradicalization as symbolic marker of conflict between the West and the\r\nEast. I also advanced the established notion that the news media employ\r\nstrategic discursive strategies that contribute to conceptual\r\ndistinctions that are used to construct Muslims as an “alien other” to\r\nthe West.\r\nGottlieb (2015)\r\nGottlieb, J. (2015). Protest News Framing Cycle: How The New York\r\nTimes Covered Occupy Wall Street. International Journal of\r\nCommunication, 9(0), 23.\r\nAbstract\r\nThis article introduces a protest news framing cycle and presents the\r\nresults of a longitudinal analysis of news attention and framing of\r\nprotest movements. To identify the frame-changing dynamic occurring over\r\ntime, a content analysis of the news coverage of Occupy Wall Street was\r\nconducted on 228 articles and 37 editorials in The New York Times from\r\nthe start of the protest in September 2011 until long after the protest\r\nhad subsided in July 2014. The article identifies longitudinal changes\r\nin news frames about the economic substance of the protest and the\r\nensuing conflict between protesters and city officials during the\r\noccupation. Findings suggest that conflict had a significant impact on\r\nthe number of news stories about the protest. Further, the results\r\ndemonstrate how news framing opportunities changed as the movement\r\nreached different stages of the news attention cycle. As the movement\r\ngrew, journalists focused on the movement’s economic grievances,\r\nincluding economic inequality, bank bailouts, and foreclosures. As the\r\nmovement peaked, news attention shifted to the intensifying conflict\r\nbetween city officials and protesters.\r\nDiakopoulos (2015)\r\nDiakopoulos, N. A. (2015). The Editor’s Eye: Curation and Comment\r\nRelevance on the New York Times. Proceedings of the 18th ACM Conference\r\non Computer Supported Cooperative Work & Social Computing,\r\n1153–1157. https://doi.org/10.1145/2675133.2675160\r\nAbstract\r\nThe journalistic curation of social media content from platforms like\r\nFacebook and YouTube or from commenting systems is underscored by an\r\nimperative for publishing accurate and quality content. This work\r\nexplores the manifestation of editorial quality criteria in comments\r\nthat have been curated and selected on the New York Times website as\r\n“NYT Picks.” The relationship between comment selection and comment\r\nrelevance is examined through the analysis of 331,785 comments,\r\nincluding 12,542 editor’s selections. A robust association between\r\neditorial selection and article relevance or conversational relevance\r\nwas found. The results are discussed in terms of their implications for\r\nreducing journalistic curatorial work load, or scaling the ability to\r\nexamine more comments for editorial selection , as well as how end-user\r\ncommenting experiences might be improved.\r\nGrimmer & Stewart (2013)\r\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The\r\nPromise and Pitfalls of Automatic Content Analysis Methods for Political\r\nTexts. Political Analysis, 21(3), 267–297.\r\nAbstract\r\nPolitics and political conflict often occur in the written and spoken\r\nword. Scholars have long recognized this, but the massive costs of\r\nanalyzing even moderately sized collections of texts have hindered their\r\nuse in political science research. Here lies the promise of automated\r\ntext analysis: it substantially reduces the costs of analyzing large\r\ncollections of text. We provide a guide to this exciting new area of\r\nresearch and show how, in many instances, the methods have already\r\nobtained part of their promise. But there are pitfalls to using\r\nautomated methods–they are no substitute for careful thought and close\r\nreading and require extensive and problem-specific validation. We survey\r\na wide range of new methods, provide guidance on how to validate the\r\noutput of the models, and clarify misconceptions and errors in the\r\nliterature. To conclude, we argue that for automated text methods to\r\nbecome a standard tool for political scientists, methodologists must\r\ncontribute new methods and new methods of validation.\r\nKothari (2010)\r\nKothari, A. (2010). The Framing of the Darfur Conflict in the New\r\nYork Times: 2003–2006. Journalism Studies, 11(2), 209–224. https://doi.org/10.1080/14616700903481978\r\nAbstract\r\nThis multi-method study examines how the New York Times reported the\r\nDarfur conflict in the Sudan, which has led to an estimated 300,000\r\ndeaths and over 2.3 million people displaced by the fighting. Drawing on\r\nnormative media theories and prior studies of Africa’s representation,\r\nthe role of sources in the frame-building process was analyzed, together\r\nwith the impact of news-making processes on journalists’ reporting about\r\nDarfur. The textual analysis largely supports results of prior studies\r\non news framing of Africa. However, interviews with four New York Times\r\njournalists reveal that the individual biases and motives of the\r\njournalists and their sources significantly influenced the coverage.\r\nWhile the journalists participated in news-making processes\r\ndistinguishable by journalist goal, source availability, and source\r\ncredibility, their sources also provided information that reinforced\r\ncertain media frames.\r\nKiousis (2004)\r\nKiousis, S. (2004). Explicating Media Salience: A Factor Analysis\r\nof New York Times Issue Coverage During the 2000 U.S. Presidential\r\nElection. Journal of Communication, 54(1), 71–87. https://doi.org/10.1111/j.1460-2466.2004.tb02614.x\r\nAbstract\r\nMedia salience—the key independent variable in agenda-setting\r\nresearch—has traditionally been explicated as a singular construct.\r\nNevertheless, scholars have defined and measured it using a number of\r\ndifferent conceptualizations and empirical indicators. To address this\r\nlimitation in research, this study introduced a conceptual model of\r\nmedia salience, suggesting it is a multidimensional construct consisting\r\nof 3 core elements: attention, prominence, and valence. Furthermore, the\r\nmodel was tested through an exploratory factor analysis of The New York\r\nTimes news coverage of 8 major political issues during the 2000\r\npresidential election as a case study. The data revealed that 2\r\ndimensions of media salience emerge: visibility and valence. Based on\r\nthe factor analysis, 2 indices are created to measure the construct,\r\nwhich are intended for use in future investigations.\r\nAlthaus & Tewksbury (2002)\r\nAlthaus, S. L., & Tewksbury, D. (2002). Agenda Setting and\r\nthe “New” News: Patterns of Issue Importance Among Readers of the Paper\r\nand Online Versions of the New York Times. Communication Research,\r\n29(2), 180–207. https://doi.org/10.1177/0093650202029002004\r\nAbstract\r\nThis study examines whether readers of the paper and online versions\r\nof a national newspaper acquire different perceptions of the importance\r\nof political issues. Using data from a weeklong experiment in which\r\nsubjects either readthe print version of the New York Times, the online\r\nversion of that paper, or received no special exposure, this study finds\r\nevidence that people exposed to the Times for 5 days adjusted their\r\nagendas in response to that exposure and that print readers modified\r\ntheir agendas differently than did online readers.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-23T13:53:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Analysis of New York Times Headlines",
    "description": "This is the project page for the analysis of differences between main and print headlines for New York Times articles published surrounding the U.S. withdrawal of the military in Afghanistan.",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/Grateful-Network/"
      }
    ],
    "date": "2022-04-13",
    "categories": [],
    "contents": "\r\n\r\nFor this project, I am using some data gathered in the DACSS 602\r\ncourse “Research Design”.\r\nI continued down the same path but with new data and a new direction\r\nthrough the DACSS 697D course “Text as Data”.\r\nMore background data can be found in this series of posts from my academic blog.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-23T00:25:35-05:00",
    "input_file": {}
  }
]
