[
  {
    "path": "posts/welcome/",
    "title": "Analysis of New York Times Headlines",
    "description": "This is the project page for the analysis of differences between main and print headlines for New York Times articles published surrounding the U.S. withdrawal of the military in Afghanistan.",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/Grateful-Network/"
      }
    ],
    "date": "2022-04-22",
    "categories": [],
    "contents": "\r\nFor this project, I am using some data gathered in the DACSS 602\r\ncourse “Research Design”.\r\nI continued down the same path but with new data and a new direction\r\nthrough the DACSS 697D course “Text as Data”.\r\nMore background data can be found in this series of posts from my academic blog.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-22T23:38:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/headline-analysis/",
    "title": "Analysis of Main vs. Print Headlines",
    "description": "Text as Data Project Headline Comparison Research",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://kbec19.github.io/NYT-Analysis/"
      }
    ],
    "date": "2022-04-17",
    "categories": [
      "text as data",
      "NYT text analysis project"
    ],
    "contents": "\r\n\r\nContents\r\nComparing\r\nMain vs. Print Headlines in the New York Times\r\nResearch Background\r\nMaking\r\nChoices on Inclusion of Observations\r\nGathering Data\r\nPrevious Process\r\nLoad Data\r\nLoad Data\r\nCreate Corpus\r\nAssign Type to Docvars\r\nTokenization\r\nDocument Feature Matrix\r\n\r\nDictionary Analysis\r\nliwcalike()\r\nNRC\r\nLSD 2015\r\nGeneral Inquirer\r\nAnnotation\r\n\r\n\r\n\r\nComparing\r\nMain vs. Print Headlines in the New York Times\r\nResearch Background\r\nDuring the Fall 2021 semester, my research group hand coded PDF\r\ncopies of articles resulting from a simple search on the websites of the\r\nNew York Times and Wall Street Journal from Feburary 29, 2020 through\r\nSeptember 30, 2021 using the term “Afghanistan withdrawal”. One thing I\r\nnoticed was that when loading the PDF articles into NVivo for coding was\r\nthat it was difficult to match the New York Times articles to the\r\ncitation information in Zotero for many of the articles because the\r\narticle titles did not match. I realized that in the process of saving\r\nthe articles in Zotero, they were saved with the title viewable on the\r\nweb version of the article; however, once the article had been preserved\r\nby using the site’s “Print to PDF” function, the article title that it\r\nused as a default file name was different than the web version.\r\nThis semester, I began this project to be one expanding on last\r\nsemester’s research and looking to expand a machine analysis of articles\r\npulling articles beginning January 2020 through December 2021. For my\r\ninitial text collection, I collected articles using the New York Times\r\nAPI for the search query “Afghanistan”, and hoped to be able to analyze\r\nthe full text of a larger range of articles.\r\nHowever, I found that I am limited in that the article search API for\r\nthe New York Times does not pull the entire article; rather, I was able\r\nto pull the abstract/summary, lead paragraph, and snippet for each\r\narticle as well as the keywords, authors, sections, and url. In\r\naddition, I was able to get the article titles for both the print and\r\nonline versions of the article.\r\nThe API’s lack of full article text was not optimal for my purposes;\r\nto examine sentiment and co-occurence of various sources. Sources are\r\nnot necessarily detailed in the lead paragraph or abstract of an\r\narticle, so I moved to a different research path.\r\nRemembering the differences in headlines from our manual coding\r\nresearch and noting that the API provides both headlines in the article\r\nsearch API, I turned to analyzing the differences in the main vs. print\r\nheadlines for articles from the same research period as our first\r\nexamination. This way, I can potentially use a sample of the full\r\narticles collected in our previous research and take the additional step\r\nof analyzing the sentiment of full articles and how they may or may not\r\nrelate to the differing headlines.\r\nMaking Choices on\r\nInclusion of Observations\r\nIn my initial look at the headline data, it was clear that not all of\r\nthe articles had different headlines; some are the same entries, and\r\nsome have “N/A” in the “print” version only, indicating they were\r\nonline-only stories. Although I initially felt inclined to leave the\r\n“N/A” observations in the analysis, I removed those observations as they\r\nwould not be relevant to my new research questions comparing the framing\r\nfor different audiences.\r\nI also removed whole sections where the API returned an observation\r\nas there was apparently use of the term “Afghanistan withdrawal”\r\nsomewhere in the article/entry, but the type of entry was clearly not\r\nbeing represented in the headline. For example, “Corrections” entries\r\nhave headlines consisting only of the term “Corrections” and the\r\ncorresponding date. Similar choices were made on the “Arts”, Books”, and\r\n“Podcasts” sections when entries are primarily the names of the things\r\nbeing reviewed that may have a reference to the Afghanistan withdrawal\r\nsomewhere in the text, but it is not relevant specifically to the\r\nwithdrawal time period being analyzed.\r\nWith few exceptions, this left the entirety of the “U.S.” and “World”\r\nnews sections, even if the content related to Afghanistan is not readily\r\nobservable. The count (~650) matched the number of articles pulled for\r\nthe hand coding research as well.\r\nGathering Data\r\nPrevious Process\r\nTo pull the data, I had to reduce the queries into more workable\r\ngroups that would not time out, given the NYT API limits. I was able to\r\npull the ~700 articles by chunk, then assemble them into a dataframe\r\nafter cleaning. I will not run the code in this post, as it was already\r\nrun and is an exhaustive process.\r\n\r\n\r\n\r\nAfter compiling the data, I re-formatted the date column and saving\r\nthe formatted tibble for offline access.\r\n\r\n\r\n\r\nLoad Data\r\nNow to the active review of the data. Loading the data from my\r\ncollection phase:\r\nLoad Data\r\n\r\n  article_id      date\r\n1          1 2/29/2020\r\n2          2 2/29/2020\r\n3          3  3/1/2020\r\n4          4  3/2/2020\r\n5          5  3/2/2020\r\n6          6  3/3/2020\r\n                                                                 headline_main\r\n1                              4 Takeaways From the U.S. Deal With the Taliban\r\n2    Taliban and U.S. Strike Deal to Withdraw American Troops From Afghanistan\r\n3           Afghanistan War Enters New Stage as U.S. Military Prepares to Exit\r\n4                 At Center of Taliban Deal, a U.S. Envoy Who Made It Personal\r\n5 U.S. Announces Troop Withdrawal in Afghanistan as Respite From Violence Ends\r\n6                                           Trump Speaks With a Taliban Leader\r\n  article_id      date\r\n1          1 2/29/2020\r\n2          2 2/29/2020\r\n3          3  3/1/2020\r\n4          4  3/2/2020\r\n5          5  3/2/2020\r\n6          6  3/3/2020\r\n                                                        headline_print\r\n1                                 Table Is Set For a Pullout And Talks\r\n2                              U.S. and Taliban Sign Withdrawal Accord\r\n3                                      A Mission Shift for Afghanistan\r\n4 At the Center of the Taliban Deal, a U.S. Envoy Who Made It Personal\r\n5                           U.S. Troop Reduction Begins in Afghanistan\r\n6               Pursuing Exit, Trump Talks  To a Leader Of the Taliban\r\n\r\nCreate Corpus\r\n\r\n\r\nmain_corpus <- corpus(main_headlines, docid_field = \"article_id\", text_field = \"headline_main\")\r\nprint_corpus <- corpus(print_headlines, docid_field = \"article_id\", text_field = \"headline_print\")\r\n\r\n\r\n\r\nAssign Type to Docvars\r\n\r\n\r\nmain_corpus$type <- \"Main Headline\"\r\nprint_corpus$type <- \"Print Headline\"\r\ndocvars(main_corpus, field = \"type\") <- main_corpus$type\r\ndocvars(print_corpus, field = \"type\") <- print_corpus$type\r\n\r\n\r\n\r\nTokenization\r\nAfter many process posts, I finally realized how to remove the “�”\r\nsymbol that has plagued me since starting working with this API by using\r\n“remove_symbols=TRUE” in addition to removing the punctuation when\r\ntokenizing. I also want to remove stopwords.\r\nMain Headlines\r\n\r\n[1] 346\r\nTokens consisting of 346 documents and 2 docvars.\r\n1 :\r\n[1] \"4\"         \"Takeaways\" \"U.S\"       \"Deal\"      \"Taliban\"  \r\n\r\n2 :\r\n[1] \"Taliban\"     \"U.S\"         \"Strike\"      \"Deal\"       \r\n[5] \"Withdraw\"    \"American\"    \"Troops\"      \"Afghanistan\"\r\n\r\n3 :\r\n[1] \"Afghanistan\" \"War\"         \"Enters\"      \"New\"        \r\n[5] \"Stage\"       \"U.S\"         \"Military\"    \"Prepares\"   \r\n[9] \"Exit\"       \r\n\r\n4 :\r\n[1] \"Center\"   \"Taliban\"  \"Deal\"     \"U.S\"      \"Envoy\"    \"Made\"    \r\n[7] \"Personal\"\r\n\r\n5 :\r\n[1] \"U.S\"         \"Announces\"   \"Troop\"       \"Withdrawal\" \r\n[5] \"Afghanistan\" \"Respite\"     \"Violence\"    \"Ends\"       \r\n\r\n6 :\r\n[1] \"Trump\"   \"Speaks\"  \"Taliban\" \"Leader\" \r\n\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\nPrint Headlines\r\n\r\n[1] 346\r\nTokens consisting of 346 documents and 2 docvars.\r\n1 :\r\n[1] \"Table\"   \"Set\"     \"Pullout\" \"Talks\"  \r\n\r\n2 :\r\n[1] \"U.S\"        \"Taliban\"    \"Sign\"       \"Withdrawal\" \"Accord\"    \r\n\r\n3 :\r\n[1] \"Mission\"     \"Shift\"       \"Afghanistan\"\r\n\r\n4 :\r\n[1] \"Center\"   \"Taliban\"  \"Deal\"     \"U.S\"      \"Envoy\"    \"Made\"    \r\n[7] \"Personal\"\r\n\r\n5 :\r\n[1] \"U.S\"         \"Troop\"       \"Reduction\"   \"Begins\"     \r\n[5] \"Afghanistan\"\r\n\r\n6 :\r\n[1] \"Pursuing\" \"Exit\"     \"Trump\"    \"Talks\"    \"Leader\"   \"Taliban\" \r\n\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\nDocument Feature Matrix\r\n\r\n\r\n\r\n\r\n\r\n#create a word frequency variable and the rankings\r\nmain_counts <- as.data.frame(sort(colSums(main_dfm),dec=T))\r\ncolnames(main_counts) <- c(\"Frequency\")\r\nmain_counts$Rank <- c(1:ncol(main_dfm))\r\nhead(main_counts)\r\n\r\n\r\n            Frequency Rank\r\nu.s               100    1\r\nafghanistan        87    2\r\nafghan             85    3\r\ntaliban            65    4\r\nbiden              53    5\r\nwar                30    6\r\n\r\nprint_counts <- as.data.frame(sort(colSums(print_dfm),dec=T))\r\ncolnames(print_counts) <- c(\"Frequency\")\r\nprint_counts$Rank <- c(1:ncol(print_dfm))\r\nhead(print_counts)\r\n\r\n\r\n            Frequency Rank\r\nu.s               100    1\r\ntaliban            66    2\r\nafghan             64    3\r\nafghanistan        56    4\r\nbiden              41    5\r\nexit               27    6\r\n\r\nNow I can take a look at this network of feature co-occurrences for\r\nthe main headlines:\r\n\r\n[1] 1232 1232\r\n[1] 20 20\r\n\r\n\r\nand for the print headlines:\r\n\r\n[1] 1178 1178\r\n[1] 20 20\r\n\r\n\r\nThis brings me to where I had previously stopped in my comparison and\r\nanalysis, and now that I’m able to produce a cleaner result, I’ll move\r\non to further analysis using the quanteda dictionary.\r\nDictionary Analysis\r\nliwcalike()\r\n\r\n [1] \"docname\"      \"Segment\"      \"WPS\"          \"WC\"          \r\n [5] \"Sixltr\"       \"Dic\"          \"anger\"        \"anticipation\"\r\n [9] \"disgust\"      \"fear\"         \"joy\"          \"negative\"    \r\n[13] \"positive\"     \"sadness\"      \"surprise\"     \"trust\"       \r\n[17] \"AllPunc\"      \"Period\"       \"Comma\"        \"Colon\"       \r\n[21] \"SemiC\"        \"QMark\"        \"Exclam\"       \"Dash\"        \r\n[25] \"Quote\"        \"Apostro\"      \"Parenth\"      \"OtherP\"      \r\n\r\nNRC\r\n\r\n\r\n# convert tokens from each headline data set to DFM using the dictionary \"NRC\"\r\nmain_nrc <- dfm(main_tokens) %>%\r\n  dfm_lookup(data_dictionary_NRC)\r\nprint_nrc <- dfm(print_tokens) %>%\r\n  dfm_lookup(data_dictionary_NRC)\r\n\r\ndim(main_nrc)\r\n\r\n\r\n[1] 346  10\r\n\r\nmain_nrc\r\n\r\n\r\nDocument-feature matrix of: 346 documents, 10 features (69.36% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            1       0    0   1        0        1       0\r\n   2     1            1       0    0   1        2        1       1\r\n   3     0            0       0    2   0        1        0       0\r\n   4     0            1       0    0   1        0        2       0\r\n   5     1            0       0    1   1        1        1       1\r\n   6     0            0       0    0   0        0        1       0\r\n    features\r\ndocs surprise trust\r\n   1        1     1\r\n   2        1     1\r\n   3        0     0\r\n   4        1     3\r\n   5        0     1\r\n   6        1     1\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\ndim(print_nrc)\r\n\r\n\r\n[1] 346  10\r\n\r\nprint_nrc\r\n\r\n\r\nDocument-feature matrix of: 346 documents, 10 features (71.47% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            0       0    0   0        0        0       0\r\n   2     0            0       0    0   0        0        1       0\r\n   3     0            0       0    0   0        0        0       0\r\n   4     0            1       0    0   1        0        2       0\r\n   5     0            0       0    0   0        0        0       0\r\n   6     0            0       0    0   0        0        1       0\r\n    features\r\ndocs surprise trust\r\n   1        0     0\r\n   2        0     1\r\n   3        0     0\r\n   4        1     3\r\n   5        0     0\r\n   6        1     1\r\n[ reached max_ndoc ... 340 more documents ]\r\n\r\nAnd use the information in a data frame to plot the output:\r\n\r\n\r\n#for the main headlines\r\ndf_main_nrc <- convert(main_nrc, to = \"data.frame\")\r\ndf_main_nrc$polarity <- (df_main_nrc$positive - df_main_nrc$negative)/(df_main_nrc$positive + df_main_nrc$negative)\r\ndf_main_nrc$polarity[which((df_main_nrc$positive + df_main_nrc$negative) == 0)] <- 0\r\n\r\nggplot(df_main_nrc) + \r\n  geom_histogram(aes(x=polarity)) + \r\n  theme_bw()\r\n\r\n\r\n\r\n#and the print headlines\r\ndf_print_nrc <- convert(print_nrc, to = \"data.frame\")\r\ndf_print_nrc$polarity <- (df_print_nrc$positive - df_print_nrc$negative)/(df_print_nrc$positive + df_print_nrc$negative)\r\ndf_print_nrc$polarity[which((df_print_nrc$positive + df_print_nrc$negative) == 0)] <- 0\r\n\r\nggplot(df_print_nrc) + \r\n  geom_histogram(aes(x=polarity)) + \r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nLooking at the headlines that are indicated as “1”, or positive in\r\nsentiment, it’s clear that this dictionary is not capturing the\r\nsentiment accurately.\r\n\r\n\r\nhead(main_corpus[which(df_main_nrc$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n1 :\r\n\"4 Takeaways From the U.S. Deal With the Taliban\"\r\n\r\n4 :\r\n\"At Center of Taliban Deal, a U.S. Envoy Who Made It Personal\"\r\n\r\n6 :\r\n\"Trump Speaks With a Taliban Leader\"\r\n\r\n8 :\r\n\"After Tours in Afghanistan, U.S. Veterans Weigh Peace With t...\"\r\n\r\n9 :\r\n\"Javier Pérez de Cuéllar Dies at 100; U.N. Chief Brokered Pea...\"\r\n\r\n10 :\r\n\"From the Afghan Peace Deal, a Weak and Pliable Neighbor for ...\"\r\n\r\nhead(print_corpus[which(df_print_nrc$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n2 :\r\n\"U.S. and Taliban Sign Withdrawal Accord\"\r\n\r\n4 :\r\n\"At the Center of the Taliban Deal, a U.S. Envoy Who Made It ...\"\r\n\r\n6 :\r\n\"Pursuing Exit, Trump Talks  To a Leader Of the Taliban\"\r\n\r\n7 :\r\n\"Attacks on Afghans by Taliban Rise After Signing of Peace De...\"\r\n\r\n8 :\r\n\"After Afghanistan Tours,  U.S. Veterans Appraise  Peace Deal...\"\r\n\r\n9 :\r\n\"Javier Pérez de Cuéllar, U.N. Chief  Behind Vital Peace Pact...\"\r\n\r\nLSD 2015\r\nI am going to want to look at multiple dictionaries to see if one can\r\nbest apply to this data. First, the LSD 2015 dictionary:\r\n\r\n\r\n# convert main corpus to DFM using the LSD2015 dictionary\r\nmain_lsd2015 <- dfm(tokens(main_corpus, remove_punct = TRUE),\r\n                              tolower = TRUE) %>%\r\n                          dfm_lookup(data_dictionary_LSD2015)\r\n# create main polarity measure for LSD2015\r\nmain_lsd2015 <- convert(main_lsd2015, to = \"data.frame\")\r\nmain_lsd2015$polarity <- (main_lsd2015$positive - main_lsd2015$negative)/(main_lsd2015$positive + main_lsd2015$negative)\r\nmain_lsd2015$polarity[which((main_lsd2015$positive + main_lsd2015$negative) == 0)] <- 0\r\n# convert print corpus to DFM using the LSD2015 dictionary\r\nprint_lsd2015 <- dfm(tokens(print_corpus, remove_punct = TRUE),\r\n                              tolower = TRUE) %>%\r\n                          dfm_lookup(data_dictionary_LSD2015)\r\n# create print polarity measure for LSD2015\r\nprint_lsd2015 <- convert(print_lsd2015, to = \"data.frame\")\r\nprint_lsd2015$polarity <- (print_lsd2015$positive - print_lsd2015$negative)/(print_lsd2015$positive + print_lsd2015$negative)\r\nprint_lsd2015$polarity[which((print_lsd2015$positive + print_lsd2015$negative) == 0)] <- 0\r\n\r\n\r\n\r\nGeneral Inquirer\r\nand the General Inquirer dictionary:\r\n\r\n\r\n# convert main corpus to DFM using the General Inquirer dictionary\r\nmain_geninq <- dfm(tokens(main_corpus, remove_punct = TRUE),\r\n                             tolower = TRUE) %>%\r\n                    dfm_lookup(data_dictionary_geninqposneg)\r\n# create main polarity measure for GenInq\r\nmain_geninq <- convert(main_geninq, to = \"data.frame\")\r\nmain_geninq$polarity <- (main_geninq$positive - main_geninq$negative)/(main_geninq$positive + main_geninq$negative)\r\nmain_geninq$polarity[which((main_geninq$positive + main_geninq$negative) == 0)] <- 0\r\n# convert print corpus to DFM using the General Inquirer dictionary\r\nprint_geninq <- dfm(tokens(print_corpus, remove_punct = TRUE),\r\n                             tolower = TRUE) %>%\r\n                    dfm_lookup(data_dictionary_geninqposneg)\r\n# create print polarity measure for GenInq\r\nprint_geninq <- convert(print_geninq, to = \"data.frame\")\r\nprint_geninq$polarity <- (print_geninq$positive - print_geninq$negative)/(print_geninq$positive + print_geninq $negative)\r\nprint_geninq$polarity[which((print_geninq$positive + print_geninq$negative) == 0)] <- 0\r\n\r\n\r\n\r\nNow I’m going to be able to compare the different dictionary scores\r\nin one data frame for each type of headline.\r\n\r\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy\r\n1          1         0                1           0        0       1\r\n2         10         0                3           0        0       2\r\n3        100         0                0           0        0       0\r\n4        101         0                2           0        0       1\r\n5        102         1                0           0        2       0\r\n6        103         1                1           0        1       0\r\n  nrc_negative nrc_positive nrc_sadness nrc_surprise nrc_trust\r\n1            0            1           0            1         1\r\n2            0            3           0            1         3\r\n3            1            0           1            0         0\r\n4            2            1           0            0         2\r\n5            2            0           2            1         0\r\n6            0            2           0            0         1\r\n  nrc_polarity lsd2015_negative lsd2015_positive lsd2015_neg_positive\r\n1    1.0000000                0                0                    0\r\n2    1.0000000                1                1                    0\r\n3   -1.0000000                1                0                    0\r\n4   -0.3333333                1                0                    0\r\n5   -1.0000000                2                0                    0\r\n6    1.0000000                0                0                    0\r\n  lsd2015_neg_negative lsd2015_polarity geninq_positive\r\n1                    0                0               1\r\n2                    0                0               2\r\n3                    0               -1               0\r\n4                    0               -1               0\r\n5                    0               -1               1\r\n6                    0                0               1\r\n  geninq_negative geninq_polarity\r\n1               1       0.0000000\r\n2               1       0.3333333\r\n3               0       0.0000000\r\n4               2      -1.0000000\r\n5               1       0.0000000\r\n6               1       0.0000000\r\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy\r\n1          1         0                0           0        0       0\r\n2         10         0                2           0        0       1\r\n3        100         0                0           0        0       0\r\n4        101         0                2           0        0       1\r\n5        102         1                0           0        4       0\r\n6        103         1                1           0        1       0\r\n  nrc_negative nrc_positive nrc_sadness nrc_surprise nrc_trust\r\n1            0            0           0            0         0\r\n2            0            2           0            1         2\r\n3            1            0           1            0         0\r\n4            2            1           0            0         2\r\n5            3            0           3            1         0\r\n6            0            1           0            0         0\r\n  nrc_polarity lsd2015_negative lsd2015_positive lsd2015_neg_positive\r\n1    0.0000000                0                0                    0\r\n2    1.0000000                1                0                    0\r\n3   -1.0000000                1                0                    0\r\n4   -0.3333333                1                0                    0\r\n5   -1.0000000                2                0                    0\r\n6    1.0000000                0                0                    0\r\n  lsd2015_neg_negative lsd2015_polarity geninq_positive\r\n1                    0                0               0\r\n2                    0               -1               2\r\n3                    0               -1               0\r\n4                    0               -1               0\r\n5                    0               -1               1\r\n6                    0                0               1\r\n  geninq_negative geninq_polarity\r\n1               0       0.0000000\r\n2               1       0.3333333\r\n3               1      -1.0000000\r\n4               2      -1.0000000\r\n5               0       1.0000000\r\n6               0       1.0000000\r\n\r\nNow that we have them all in a single data frame, it’s\r\nstraightforward to figure out a bit about how well our different\r\nmeasures of polarity agree across the different approaches by looking at\r\ntheir correlation using the “cor()” function.\r\n\r\n\r\ncor(main_sent$nrc_polarity, main_sent$lsd2015_polarity)\r\n\r\n\r\n[1] 0.4968335\r\n\r\ncor(main_sent$nrc_polarity, main_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4813359\r\n\r\ncor(main_sent$lsd2015_polarity, main_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.5327856\r\n\r\n\r\n\r\ncor(print_sent$nrc_polarity, print_sent$lsd2015_polarity)\r\n\r\n\r\n[1] 0.4197161\r\n\r\ncor(print_sent$nrc_polarity, print_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4917256\r\n\r\ncor(print_sent$lsd2015_polarity, print_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4921922\r\n\r\nAnnotation\r\n\r\n\r\nlibrary(cleanNLP)\r\ncnlp_init_udpipe()\r\n\r\n\r\n\r\n\r\n\r\nlibrary(tidyr)\r\n#amain <- as_tibble(headlines_main)\r\n#annotated_main <- cnlp_annotate(main_corpus)\r\n#annotated_main\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/headline-analysis/unnamed-chunk-12-1.png",
    "last_modified": "2022-04-23T00:12:27-05:00",
    "input_file": "headline-analysis.knit.md"
  }
]
